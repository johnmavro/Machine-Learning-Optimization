{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0A17RyO6oElc",
    "outputId": "6b125283-556a-441c-efea-0b1c3a5aaa7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/Othercomputers/Il mio laptop/Machine-Learning-Optimization_new\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/Othercomputers/Il mio laptop/Machine-Learning-Optimization_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "mC7BWlWnoFBa",
    "outputId": "8b243316-7df8-433b-82be-f78ef575888f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: barbar in c:\\users\\federico betti\\anaconda3\\envs\\cs439\\lib\\site-packages (0.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Federico Betti\\anaconda3\\envs\\CS439\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#@title Import and utilities \n",
    "\n",
    "from Frank_Wolfe.utils.utils import *\n",
    "from Frank_Wolfe.DFW import *\n",
    "from Frank_Wolfe.architectures import *\n",
    "from Frank_Wolfe.MultiClassHingeLoss import *\n",
    "!pip install barbar\n",
    "from barbar import Bar\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gMj3YVkWoM9x"
   },
   "outputs": [],
   "source": [
    "save_stats = True\n",
    "save_figs = True\n",
    "load = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "Jf6SJcqjoUSA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to CIFAR10-dataset\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2141c835bd147428cb9a01089f8caca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting CIFAR10-dataset\\cifar-10-python.tar.gz to CIFAR10-dataset\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m trainData \u001b[38;5;241m=\u001b[39m datasetDict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasetDict\u001b[39m\u001b[38;5;124m'\u001b[39m](root\u001b[38;5;241m=\u001b[39mroot, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     22\u001b[0m                                             transform\u001b[38;5;241m=\u001b[39mtrainTransformDict[dataset_name])\n\u001b[0;32m     23\u001b[0m testData \u001b[38;5;241m=\u001b[39m datasetDict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasetDict\u001b[39m\u001b[38;5;124m'\u001b[39m](root\u001b[38;5;241m=\u001b[39mroot, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     24\u001b[0m                                         transform\u001b[38;5;241m=\u001b[39mtestTransformDict[dataset_name])\n\u001b[1;32m---> 26\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# define the loss object\u001b[39;00m\n\u001b[0;32m     29\u001b[0m loss_criterion \u001b[38;5;241m=\u001b[39m MultiClassHingeLoss()\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torch\\nn\\modules\\module.py:907\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    903\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    904\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> 907\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torch\\nn\\modules\\module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torch\\nn\\modules\\module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torch\\nn\\modules\\module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 601\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torch\\nn\\modules\\module.py:905\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    904\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> 905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torch\\cuda\\__init__.py:210\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "#@title Choose dataset name and architecture of the network \n",
    "\n",
    "dataset_name = 'CIFAR10' #@param ['CIFAR10', 'CIFAR100']\n",
    "model_type = 'DenseNet' #@param ['DenseNet', 'WideResNet', 'GoogLeNet', 'ResNeXt']\n",
    "if model_type == 'GoogLeNet':\n",
    "    model = GoogleNet(num_class=10 if dataset_name == 'CIFAR10' else 100)\n",
    "elif model_type == 'DenseNet':\n",
    "    model = torchvision.models.densenet121(pretrained=False)\n",
    "elif model_type == 'ResNeXt':\n",
    "    model = torchvision.models.resnet101(pretrained=False)\n",
    "elif model_type == 'WideResNet':\n",
    "    model =  WideResNet(num_classes=10 if dataset_name == 'CIFAR10' else 100)\n",
    "else:\n",
    "    raise ValueError(\"Please, select an available architecture\")\n",
    "\n",
    "datasetDict = setDatasetAttributes(dataset_name)\n",
    "trainTransformDict, testTransformDict = setTrainAndTest(dataset_name)\n",
    "\n",
    "root = f\"{dataset_name}-dataset\"\n",
    "\n",
    "trainData = datasetDict['datasetDict'](root=root, train=True, download=True,\n",
    "                                            transform=trainTransformDict[dataset_name])\n",
    "testData = datasetDict['datasetDict'](root=root, train=False,\n",
    "                                        transform=testTransformDict[dataset_name])\n",
    "\n",
    "model = model.to(device=\"cuda:0\")\n",
    "\n",
    "# define the loss object\n",
    "loss_criterion = MultiClassHingeLoss().to(device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "C6IgJdqGoWkV"
   },
   "outputs": [],
   "source": [
    "#@title Choose optimizer and parameters \n",
    "\n",
    "optimizer_name = \"DFW\" #@param  ['DFW', 'Adam', 'AdaGrad', 'SGD with momentum', 'SGD']\n",
    "momentum = 0.9 #@param {type:\"number\"}\n",
    "lr = 0.001 #@param {type:\"number\"}\n",
    "eta = 0.1 #@param {type:\"number\"}\n",
    "beta_1 = 0.9 #@param {type:\"number\"}\n",
    "beta_2 = 0.99 #@param {type:\"number\"}\n",
    "weight_decay = 0.01 #@param {type:\"number\"}\n",
    "\n",
    "if optimizer_name == \"DFW\":\n",
    "    optimizer = DFW(params=model.parameters(), eta=eta, momentum=momentum, \n",
    "                    prox_steps=1)\n",
    "    assert eta > 0\n",
    "elif optimizer_name == \"SGD\" or optimizer_name == \"SGD with momentum\":\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(), lr=lr,\n",
    "                              momentum=momentum, weight_decay=weight_decay)\n",
    "    assert lr > 0\n",
    "    assert 0 <= momentum <= 1\n",
    "elif optimizer_name == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, \n",
    "                               betas=(beta_1, beta_2), weight_decay=weight_decay)\n",
    "elif optimizer_name == \"AdaGrad\":\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cq-yV7BYplnM",
    "outputId": "6f914260-14c6-437b-c1d0-acd595e73014"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 0/10\n",
      "Evaluation of train data:\n",
      "50000/50000: [===============================>] - ETA 0.6s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.1s\n",
      "\n",
      " Finished epoch 0/10: Train Loss 1.0318777755355835 | Test Loss 1.0318778211593629 | Train Acc 0.1 | Test Acc 0.1\n",
      "Time elapsed for the current epoch 60.96457648277283\n",
      "\n",
      "Epoch 1/10\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.5s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.1s\n",
      "\n",
      " Finished epoch 1/10: Train Loss 1.183716427268982 | Test Loss 1.4749269319534302 | Train Acc 0.31122 | Test Acc 0.4166\n",
      "Time elapsed for the current epoch 240.85318732261658\n",
      "\n",
      "Epoch 2/10\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.4s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.1s\n",
      "\n",
      " Finished epoch 2/10: Train Loss 0.8551070518112183 | Test Loss 0.8091922302246094 | Train Acc 0.5443 | Test Acc 0.5908\n",
      "Time elapsed for the current epoch 241.20625829696655\n",
      "\n",
      "Epoch 3/10\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.4s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.1s\n",
      "\n",
      " Finished epoch 3/10: Train Loss 0.621567981185913 | Test Loss 0.5698805132865906 | Train Acc 0.69824 | Test Acc 0.7381\n",
      "Time elapsed for the current epoch 238.17877006530762\n",
      "\n",
      "Epoch 4/10\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.4s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.1s\n",
      "\n",
      " Finished epoch 4/10: Train Loss 0.4562594448661804 | Test Loss 0.44950591735839845 | Train Acc 0.7917 | Test Acc 0.7938\n",
      "Time elapsed for the current epoch 238.04972314834595\n",
      "\n",
      "Epoch 5/10\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.4s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.1s\n",
      "\n",
      " Finished epoch 5/10: Train Loss 0.37915001037597656 | Test Loss 0.4320404792785644 | Train Acc 0.83066 | Test Acc 0.8156\n",
      "Time elapsed for the current epoch 237.74381947517395\n",
      "\n",
      "Epoch 6/10\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.4s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.1s\n",
      "\n",
      " Finished epoch 6/10: Train Loss 0.3279238820266724 | Test Loss 0.36220944242477415 | Train Acc 0.8563 | Test Acc 0.8398\n",
      "Time elapsed for the current epoch 237.59572982788086\n",
      "\n",
      "Epoch 7/10\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.4s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.1s\n",
      "\n",
      " Finished epoch 7/10: Train Loss 0.2927869176197052 | Test Loss 0.3641465504646301 | Train Acc 0.87202 | Test Acc 0.8424\n",
      "Time elapsed for the current epoch 238.46229028701782\n",
      "\n",
      "Epoch 8/10\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.4s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.1s\n",
      "\n",
      " Finished epoch 8/10: Train Loss 0.2661537017440796 | Test Loss 0.3551136448860168 | Train Acc 0.88428 | Test Acc 0.8528\n",
      "Time elapsed for the current epoch 237.93136358261108\n",
      "\n",
      "Epoch 9/10\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.4s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.1s\n",
      "\n",
      " Finished epoch 9/10: Train Loss 0.23798730578422547 | Test Loss 0.33324440326690674 | Train Acc 0.8969 | Test Acc 0.8604\n",
      "Time elapsed for the current epoch 237.36038970947266\n",
      "\n",
      "Epoch 10/10\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.4s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.1s\n",
      "\n",
      " Finished epoch 10/10: Train Loss 0.21528921226024628 | Test Loss 0.32412698917388916 | Train Acc 0.9094 | Test Acc 0.8622\n",
      "Time elapsed for the current epoch 238.2840874195099"
     ]
    }
   ],
   "source": [
    "#@title Train the network  \n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "epochs_times = []\n",
    "\n",
    "nepochs = 10 #@param {type:\"integer\"}\n",
    "batch_size = 64  #@param {type:\"integer\"}\n",
    "verbose = 0 #@param [0, 1]\n",
    "\n",
    "# Loaders\n",
    "trainLoader = torch.utils.data.DataLoader(trainData, batch_size=batch_size, shuffle=True,\n",
    "                                      pin_memory=torch.cuda.is_available(), num_workers=2)\n",
    "testLoader = torch.utils.data.DataLoader(testData, batch_size=batch_size, shuffle=False,\n",
    "                                      pin_memory=torch.cuda.is_available(), num_workers=2)\n",
    "\n",
    "# initialize some necessary metrics objects\n",
    "train_loss, train_accuracy = AverageMeter(), AverageMeter()\n",
    "test_loss, test_accuracy = AverageMeter(), AverageMeter()\n",
    "\n",
    "# function to reset metrics\n",
    "def reset_metrics():\n",
    "    train_loss.reset()\n",
    "    train_accuracy.reset()\n",
    "    test_loss.reset()\n",
    "    test_accuracy.reset()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(data=\"train\"):\n",
    "    if data == \"train\":\n",
    "        loader = trainLoader\n",
    "        mean_loss, mean_accuracy = train_loss, train_accuracy\n",
    "    elif data == \"test\":\n",
    "        loader = testLoader\n",
    "        mean_loss, mean_accuracy = test_loss, test_accuracy\n",
    "    \n",
    "    sys.stdout.write(f\"Evaluation of {data} data:\\n\")\n",
    "    for x_input, y_target in Bar(loader):\n",
    "        x_input, y_target = x_input.to(device=\"cuda:0\"), y_target.to(device=\"cuda:0\")\n",
    "        output = model.eval()(x_input)\n",
    "        loss = loss_criterion(output, y_target)\n",
    "        mean_loss(loss.item(), len(y_target))\n",
    "        mean_accuracy(categorical_accuracy(y_true=y_target, output=output), len(y_target))\n",
    "\n",
    "for epoch in range(nepochs + 1):\n",
    "    start = time.time()\n",
    "    reset_metrics()\n",
    "    sys.stdout.write(f\"\\n\\nEpoch {epoch}/{nepochs}\\n\")\n",
    "    if epoch == 0:\n",
    "        # Just evaluate the model once to get the metrics\n",
    "        evaluate_model(data='train')\n",
    "    else:\n",
    "        # Train\n",
    "        sys.stdout.write(f\"Training:\\n\")\n",
    "        for x_input, y_target in Bar(trainLoader):\n",
    "            x_input, y_target = x_input.to(device=\"cuda:0\"), y_target.to(device=\"cuda:0\")\n",
    "            optimizer.zero_grad()  # Zero the gradient buffers\n",
    "            output = model.train()(x_input)\n",
    "            loss = loss_criterion(output, y_target)\n",
    "            loss.backward()  # Backpropagation\n",
    "            if optimizer_name == \"DFW\":\n",
    "              optimizer.step(lambda: float(loss), model, loss_criterion, x_input, y_target)\n",
    "            else:\n",
    "              optimizer.step() \n",
    "            train_loss(loss.item(), len(y_target))\n",
    "            train_accuracy(categorical_accuracy(y_true=y_target, output=output), len(y_target))\n",
    "\n",
    "    evaluate_model(data='test')\n",
    "    sys.stdout.write(f\"\\n Finished epoch {epoch}/{nepochs}: Train Loss {train_loss.result()} | Test Loss {test_loss.result()} | Train Acc {train_accuracy.result()} | Test Acc {test_accuracy.result()}\\n\")\n",
    "\n",
    "    train_losses.append(train_loss.result())\n",
    "    train_accuracies.append(train_accuracy.result())\n",
    "    test_losses.append(test_loss.result())\n",
    "    test_accuracies.append(test_accuracy.result())\n",
    "\n",
    "\n",
    "    elapsed_time = time.time()-start\n",
    "    sys.stdout.write(f\"Time elapsed for the current epoch {elapsed_time}\")\n",
    "    epochs_times.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "9w2DgFHAExBE"
   },
   "outputs": [],
   "source": [
    "#@title Save training results and plot\n",
    "\n",
    "if load:\n",
    "    output_folder = os.path.join(os.getcwd(), 'results')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    fname = output_folder + '/stats_dict_' + model_type + '.pkl'\n",
    "    with open(fname, 'rb') as handle:\n",
    "        stats_dict = pickle.load(handle)\n",
    "\n",
    "results = {'epochs': nepochs, 'train_losses': train_losses, \n",
    "           'train_acc': train_accuracies, 'test_losses': test_losses, \n",
    "           'test_acc': test_accuracies, 'elapsed_time': elapsed_time}\n",
    "stats_dict = {}\n",
    "stats_dict.update({optimizer_name: results})\n",
    "\n",
    "# save everything onto file\n",
    "if save_stats: \n",
    "    output_folder = os.path.join(os.getcwd(), 'results')  # set the folder\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    fname = output_folder + '/stats_dict_' + model_type + '.pkl'\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pickle.dump(stats_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldLaA2by6VYV"
   },
   "source": [
    "# Parameters used in the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7F6Ai3G6bFd"
   },
   "source": [
    "In order to reproduce our results, the following set of parameters should be used.\\\n",
    "If not specified, the remaining parameters (e.g. $\\epsilon$ for Adam and Adagrad) are set to their default values.\n",
    "\n",
    "Deep Frank Wolfe:\\\n",
    "$η = 0.1$, $μ = 0.9$, $w_d = 0$\n",
    "\n",
    "Stochastic Gradient Descent:\\\n",
    "$\\gamma = 0.001$, $\\mu = 0.9$, $w_d = 0$\n",
    "\n",
    "Adam:\\\n",
    "$\\gamma = 0.001$, $\\mu = 0.9$, $\\beta_1 = 0.9$, $\\beta_2 = 0.99$\n",
    "\n",
    "AdaGrad:\\\n",
    "$\\gamma = 0.001$, $w_d = 0$\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DFW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
