{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup instructions\n",
    "\n",
    "We provide a user-ready interface to reproduce our results concerning the Block Coordinate Descent algorithm. Therefore, we strongly recommend using Google Colab to perform training. \n",
    "To run this notebook on Google Colab, please import from the repository folder the **Block_Coordinate_Descent** directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-KDbRlCKv4U8",
    "outputId": "235c1b37-d6e1-4685-ce66-24c977b5f153"
   },
   "outputs": [],
   "source": [
    "#@title Import and Utilities\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from Block_Coordinate_Descent.utilities import *\n",
    "from Block_Coordinate_Descent.Torch_architectures import *\n",
    "from Block_Coordinate_Descent.Train_functions import *\n",
    "from Block_Coordinate_Descent.CD_utilities import *\n",
    "from Block_Coordinate_Descent.layers import *\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"Torchvision Version:\", torchvision.__version__)\n",
    "print(\"GPU is available?\", torch.cuda.is_available())\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqvTMJAbw74Q"
   },
   "source": [
    "# Imported datasets\n",
    "\n",
    "We carried out a comparison of our algorithms using the following datasets:\n",
    "\n",
    "1. MNIST\n",
    "2. FMNIST\n",
    "3. CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PEt-flsyEKM"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVkAgTGRo1is"
   },
   "outputs": [],
   "source": [
    "#@title Choose the dataset and the optimizer to perform training\n",
    "\n",
    "ts = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0,), (1,))])\n",
    "\n",
    "# change the flag to choose the dataset to work with\n",
    "dataset_flag = \"MNIST\" #@param ['MNIST','FMNIST','CIFAR10']\n",
    "batch_size = 256 #@param {type:\"integer\"}\n",
    "if dataset_flag == 'MNIST':\n",
    "    trainset = datasets.MNIST('../data', train=True, download=True, transform=ts)\n",
    "    testset = datasets.MNIST(root='../data', train=False, download=True, transform=ts)\n",
    "    dataset_train = torch.utils.data.DataLoader(testset,batch_size = 128, shuffle = True)\n",
    "    dataset_test = torch.utils.data.DataLoader(trainset,batch_size = batch_size,shuffle = True)\n",
    "elif dataset_flag == 'FMNIST':\n",
    "    trainset = datasets.FashionMNIST('../data', train=True, download=True, transform=ts)\n",
    "    testset = datasets.FashionMNIST(root='../data', train=False, download=True, transform=ts)\n",
    "    dataset_train = torch.utils.data.DataLoader(testset,batch_size = 128, shuffle = True)\n",
    "    dataset_test = torch.utils.data.DataLoader(trainset,batch_size = batch_size,shuffle = True)\n",
    "elif dataset_flag== 'CIFAR10':\n",
    "    trainset = datasets.CIFAR10('../data', train=True, download=True, transform=ts)\n",
    "    testset = datasets.CIFAR10(root='../data', train=False, download=True, transform=ts)\n",
    "    dataset_train = torch.utils.data.DataLoader(testset,batch_size = 128, shuffle = True)\n",
    "    dataset_test = torch.utils.data.DataLoader(trainset,batch_size = batch_size,shuffle = True)\n",
    "\n",
    "x_train, y_train, x_test, y_test,y_train_one_hot, y_test_one_hot, I1, I2 = load_dataset(trainset, testset,10)\n",
    "\n",
    "# move to GPU\n",
    "x_train = x_train.to(device = device)\n",
    "x_test = x_test.to(device = device)\n",
    "y_train = y_train.to(device = device)\n",
    "y_test = y_test.to(device = device)\n",
    "y_train_one_hot = y_train_one_hot.to(device)\n",
    "y_test_one_hot = y_test_one_hot.to(device)\n",
    "input_size = x_train.shape[0]\n",
    "hidden_size = int(1.5*input_size)\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lX28NWho-N4"
   },
   "outputs": [],
   "source": [
    "#@title Model Selection\n",
    "\n",
    "model_name = 'Multilayer-Perceptron' #@param ['Multilayer-Perceptron']\n",
    "optimizer_name = \"Coordinate-Descent+Adam\" #@param ['SGD','Adam','Coordinate-Descent','Coordinate-Descent+SGD','Coordinate-Descent+Adam']\n",
    "momentum = 0.9 #@param {type:\"number\"}\n",
    "lr = 0.01 #@param {type:\"number\"}\n",
    "weight_decay = 0.00 #@param {type:\"number\"}\n",
    "beta_1 = 0.9 #@param {type:\"number\"}\n",
    "beta_2 = 0.999 #@param {type:\"number\"}\n",
    "gamma = 0.1 #@param {type:\"number\"}\n",
    "alpha = 4 #@param {type:\"number\"}\n",
    "epochs = 50 #@param {type:\"integer\"}\n",
    "# the ratio of the epochs for coordinate descent for mixed classifiers\n",
    "ratio =  0.6#@param {type:\"number\"}\n",
    "GD_Update = False #@param {type:\"boolean\"}\n",
    "linear_extension = False #@param {type:\"boolean\"}\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "if(model_name =='Multilayer-Perceptron'):\n",
    "    model = MultiLayerPerceptron(input_size,hidden_size,output_size) \n",
    "\n",
    "# optimizer\n",
    "if (optimizer_name == \"SGD\" or optimizer_name == \"Coordinate-Descent+SGD\"):\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(), lr=lr,\n",
    "                              momentum=momentum, weight_decay=weight_decay)\n",
    "    assert lr > 0\n",
    "    assert 0 <= momentum <= 1\n",
    "    \n",
    "elif (optimizer_name == \"Adam\" or optimizer_name == \"Coordinate-Descent+Adam\"):\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, \n",
    "                               betas=(beta_1, beta_2), weight_decay=weight_decay)\n",
    "\n",
    "if(optimizer_name != 'Coordinate-Descent'):\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrFu1jgT3kLz"
   },
   "source": [
    "# Perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_O44lBu4TC_",
    "outputId": "6434b00e-e9f2-4434-c96b-a7943242d00f"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "accuracy_train = []\n",
    "accuracy_test = []\n",
    "epochs_times = []\n",
    "start = time.time()\n",
    "if(optimizer_name == 'Coordinate-Descent' or optimizer_name == 'Coordinate-Descent+SGD' \n",
    "   or optimizer_name== 'Coordinate-Descent+Adam'):\n",
    "   if(optimizer_name != 'Coordinate-Descent'):\n",
    "    total_epochs = epochs\n",
    "    epochs = int(total_epochs * ratio)\n",
    "  train_losses, test_losses , accuracy_train, accuracy_test,epochs_times,Ws,bs = execute_training([[\"Perceptron\",hidden_size,1],[\"Perceptron\",hidden_size,1]], input_size, hidden_size, output_size, x_train, x_test, y_train, y_test, y_train_one_hot, y_test_one_hot,\n",
    "                                         GD_Update, linear_extension, I1 = hidden_size,I2=1, niter = epochs, gamma = gamma, alpha = alpha)\n",
    "  #Train using BCD\n",
    "  train_losses = list(train_losses)\n",
    "  test_losses = list(test_losses)\n",
    "  accuracy_train = list(accuracy_train)\n",
    "  accuracy_test = list(accuracy_test)\n",
    "  if(optimizer_name != 'Coordinate-Descent'):\n",
    "    epochs = total_epochs-epochs\n",
    "if(optimizer_name != 'Coordinate-Descent'):\n",
    "  model = model.to(device)\n",
    "  #train using sgd or adam\n",
    "  if(optimizer_name == 'Coordinate-Descent+SGD' or optimizer_name == 'Coordinate-Descent+Adam'):\n",
    "    i=0\n",
    "    for param in model.parameters():\n",
    "      if i%2 == 0:\n",
    "        param.data = Ws[int(i/2)]\n",
    "        #temp_W.pop()\n",
    "      else:\n",
    "        param.data = torch.flatten(bs[int(i/2)])\n",
    "        #temp_b.pop()\n",
    "      i+=1\n",
    "  train_loss, test_loss, acc_train, acc_test, times = train_model(model, dataset_train, dataset_test, optimizer, cross_entropy, epochs,scheduler,optimizer_name)\n",
    "  train_losses = list(train_losses) + train_loss\n",
    "  test_losses = list(test_losses) + test_loss\n",
    "  accuracy_train = list(accuracy_train) + acc_train\n",
    "  accuracy_test = list(accuracy_test) + acc_test\n",
    "  epochs_times = list(epochs_times) + times\n",
    "elapsed_time = time.time() - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "op31_mG1kcON",
    "outputId": "1b2ccb8b-98cc-4637-de12-00f57099816e"
   },
   "outputs": [],
   "source": [
    "results = {'epochs': epochs_times, 'train_losses': train_losses, \n",
    "           'train_acc': accuracy_train, 'test_losses': test_losses, \n",
    "           'test_acc': accuracy_test, 'elapsed_time': elapsed_time}\n",
    "stats_dict = {}\n",
    "stats_dict.update({optimizer_name: results})\n",
    "save_stats = True\n",
    "if(GD_Update):\n",
    "    suffix = '-Entropy.pkl'\n",
    "elif(linear_extension):\n",
    "    suffix = '-linear_prox.pkl'\n",
    "else:\n",
    "    suffix = '.pkl'\n",
    "\n",
    "# save everything onto file\n",
    "output_folder = os.path.join(os.getcwd(), dataset_flag)  # set the folder\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "fname = output_folder + '/stats_dict_' + model_name + '_' + optimizer_name + suffix\n",
    "with open(fname, 'wb') as handle:\n",
    "    pickle.dump(stats_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFdulP3nz6ES"
   },
   "outputs": [],
   "source": [
    "#@title Plot latest training trends obtained in the previous cells\n",
    "\n",
    "# Test accuracy and training loss plots\n",
    "full_name = optimizer_name + suffix\n",
    "full_name.removesuffix('.pkl')\n",
    "plot_stats(dataset_flag, full_name, optimizer_name, \"MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FhdhvgvWeCx"
   },
   "source": [
    "# Produce complete training plots\n",
    "\n",
    "### NOTE: following the initial instructions, you loaded also the dictionaries which contain our training results. Saving the training statistics will overwite (in the local folder on Colab) this results, thus showing a different plot than the one presented in the report. If our results are only partially overwritten, we strongly recommend performing 50 training epochs for compatibility with our results, otherwise an error message will be raised by the plotting function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "W9Px3TKWWZu-",
    "outputId": "c7ab31b9-0365-492d-9dfe-93339ac82180"
   },
   "outputs": [],
   "source": [
    "plot_MNIST = True #@param {type:\"boolean\"}\n",
    "plot_FMNIST = True #@param {type:\"boolean\"}\n",
    "plot_CIFAR10 = True #@param {type:\"boolean\"}\n",
    "plot_prox_linear = True #@param {type:\"boolean\"}\n",
    "plot_GD_update = True #@param {type:\"boolean\"}\n",
    "\n",
    "if(plot_MNIST):\n",
    "    plot_stats(\"MNIST\", [\"Adam\",\"SGD\",\"Coordinate-Descent\",\"Coordinate-Descent+Adam\"], \n",
    "               [\"Adam\",\"SGD\",\"Coordinate-Descent\",\"Coordinate-Descent+Adam\"], \"MLP\")\n",
    "if(plot_FMNIST):\n",
    "    plot_stats(\"FMNIST\", [\"Adam\",\"SGD\",\"Coordinate-Descent\",\"Coordinate-Descent+Adam\"], \n",
    "               [\"Adam\",\"SGD\",\"Coordinate-Descent\",\"Coordinate-Descent+Adam\"], \"MLP\")\n",
    "if(plot_CIFAR10):\n",
    "    plot_stats(\"CIFAR10\", [\"Adam\",\"SGD\",\"Coordinate-Descent\",\"Coordinate-Descent+SGD\"], \n",
    "               [\"Adam\",\"SGD\",\"Coordinate-Descent\",\"Coordinate-Descent+SGD\"], \"MLP\")\n",
    "if(plot_prox_linear):\n",
    "    plot_stats(dataset_flag, [\"Coordinate-Descent\",\"Coordinate-Descent-linear_prox\"], \n",
    "               [\"Coordinate-Descent\",\"Coordinate-Descent\"], \"MLP\")\n",
    "if(plot_GD_update):\n",
    "    plot_stats(dataset_flag, [\"Coordinate-Descent-Entropy\"], [\"Coordinate-Descent\"], \"MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reproduce our results (i.e. the training trends shown in the report), the following set of parameters should be used.\\ If not specified otherwise, other parameters (e.g. for numerical stability) are set to their default values.\n",
    "\n",
    "$\\text{Block Coordinate Descent}$:\\\n",
    "$\\gamma = 0.1$, $\\alpha = 4$\n",
    "\n",
    "$\\text{Stochastic Gradient Descent (with scheduler):$\\\n",
    "$\\gamma = 0.01$, $\\mu = 0.9$\n",
    "\n",
    "Adam:\\\n",
    "$\\gamma = 0.001$, $\\mu = 0.9$, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$\n",
    "\n",
    "For hybrid variants, we took ratio = 0.6\n",
    "\n",
    "For the mixed optimizer of SGD decrease the learning rate to $\\gamma = 0.001$"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "baseline_SGD.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
