{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Block_coordinate_descent_Convolution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjmJ5IsZInkj",
        "outputId": "3f85a7e1-fbff-4099-9fa1-fde778de7ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 1.11.0+cu113\n",
            "Torchvision Version: 0.12.0+cu113\n",
            "GPU is available? True\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "import copy\n",
        "import math\n",
        "\n",
        "from utilities import *\n",
        "\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "print(\"Torchvision Version:\", torchvision.__version__)\n",
        "print(\"GPU is available?\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "DT-roPj1Isd7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imported datasets\n",
        "For the testing and comparison of our algorithms we will use the following datasets:\n",
        "\n",
        "1. MNIST\n",
        "2. FashionMNIST\n",
        "3. CIFAR10"
      ],
      "metadata": {
        "id": "N_1aD5CdJCfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0,), (1,))])\n",
        "\n",
        "# change the flag to choose the dataset to work with\n",
        "dataset_flag = 0\n",
        "\n",
        "if dataset_flag ==0:\n",
        "  trainset = datasets.MNIST('../data', train=True, download=True, transform=ts)\n",
        "  testset = datasets.MNIST(root='../data', train=False, download=True, transform=ts)\n",
        "elif dataset_flag ==1:\n",
        "  trainset = datasets.FashionMNIST('../data', train=True, download=True, transform=ts)\n",
        "  testset = datasets.FashionMNIST(root='../data', train=False, download=True, transform=ts)\n",
        "else:\n",
        "  trainset = datasets.CIFAR10('../data', train=True, download=True, transform=ts)\n",
        "  testset = datasets.CIFAR10(root='../data', train=False, download=True, transform=ts)"
      ],
      "metadata": {
        "id": "FCpRsZlPIuew"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "boston = datasets.load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "x_train = torch.from_numpy(x_train).to(device = device)\n",
        "x_test = torch.from_numpy(x_test).to(device = device)\n",
        "y_train = torch.from_numpy(y_train).to(device = device)\n",
        "y_test = torch.from_numpy(y_test).to(device = device)\n",
        "\n",
        "x_train = x_train.T.float()\n",
        "x_test = x_test.T.float()\n",
        "\n",
        "N = x_train.shape[1]\n",
        "N_test = x_test.shape[1]\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-oRYcSE2I27L",
        "outputId": "ec49768f-cc2e-409f-eb42-5557550b0c2c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom sklearn import datasets\\nfrom sklearn.model_selection import train_test_split\\n\\nboston = datasets.load_boston()\\nX = boston.data\\ny = boston.target\\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\\n\\nx_train = torch.from_numpy(x_train).to(device = device)\\nx_test = torch.from_numpy(x_test).to(device = device)\\ny_train = torch.from_numpy(y_train).to(device = device)\\ny_test = torch.from_numpy(y_test).to(device = device)\\n\\nx_train = x_train.T.float()\\nx_test = x_test.T.float()\\n\\nN = x_train.shape[1]\\nN_test = x_test.shape[1]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset preprocessing"
      ],
      "metadata": {
        "id": "dtekIhnJJJhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_test, y_test, y_train_one_hot, y_test_one_hot, I1, I2 = load_dataset(trainset, testset, 10)\n",
        "\n",
        "# We move to GPU\n",
        "x_train = x_train.to(device = device)\n",
        "x_test = x_test.to(device = device)\n",
        "y_train = y_train.to(device = device)\n",
        "y_test = y_test.to(device = device)\n",
        "y_train_one_hot = y_train_one_hot.to(device = device)\n",
        "y_test_one_hot = y_test_one_hot.to(device = device)"
      ],
      "metadata": {
        "id": "GJ9kefhFI6vI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture initialization\n",
        "\n",
        "For the MultiLayerPerceptron we have the parameters **input_size** , **hidden_size**,**output_size** corresponding to the size of the input layer, the hidden layer and the output layer, respectively.\n",
        "\n",
        "The MLP only has 3 layers like https://github.com/timlautk/BCD-for-DNNs-PyTorch/blob/master/bcd_dnn_mlp_mnist.ipynb as a starting point.\n",
        "\n",
        "Also we use ReLU currently for the same reason."
      ],
      "metadata": {
        "id": "w7M3uSc8JRdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = x_train.shape[0]\n",
        "hidden_size = 1600\n",
        "output_size = 10"
      ],
      "metadata": {
        "id": "QiyJP8y2I81R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "Note: Fix it so that it moves everything to device in the following function and that it does the label sample split here"
      ],
      "metadata": {
        "id": "QAm_re0mJbOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_v_js(U1, U2, W, b, rho, gamma):\n",
        "    \"\"\"\n",
        "    The function updates the V_js parameters during the training phase\n",
        "    \n",
        "    :param U1: The U parameter on the same level of V that we are updating\n",
        "    :param U2: The U parameter which is in the next level of the V that we are updating\n",
        "    :param W: The W parameter which is in the next level of the V that we are updating\n",
        "    :param b: The b parameter which is in the next level of the V that we are updating\n",
        "    :param rho: The constant rho parameter which is in the next level of the V that we are updating\n",
        "    :param gamma: The constant gamma parameter which is in the next level of the V that we are updating\n",
        "    :return: The updated V\n",
        "    \"\"\"\n",
        "    _, d = W.size()\n",
        "    I = torch.eye(d, device=device)\n",
        "    U1 = nn.ReLU()(U1)\n",
        "    _, col_U2 = U2.size()\n",
        "    Vstar = torch.mm(torch.inverse(rho * (torch.mm(torch.t(W), W)) + gamma * I),\n",
        "                     rho * torch.mm(torch.t(W), U2 - b.repeat(1, col_U2)) + gamma * U1)\n",
        "    return Vstar"
      ],
      "metadata": {
        "id": "78BFLCPNJbss"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_wb_js(U, V, W, b, alpha, rho):\n",
        "    \"\"\"\n",
        "    The function updates the W and b parameters during the training phase\n",
        "    \n",
        "    :param U: The U in the current level of W and b\n",
        "    :param V: The V in the previous level with respect to the W that we are updating\n",
        "    :param W: The current W that we have to update\n",
        "    :param b: The current b that we have to update\n",
        "    :param alpha: The alpha constant of the updates\n",
        "    :param rho: The rho constant of the updates\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    d, N = V.size()\n",
        "    I = torch.eye(d, device=device)\n",
        "    _, col_U = U.size()\n",
        "    Wstar = torch.mm(alpha * W + rho * torch.mm(U - b.repeat(1, col_U), torch.t(V)),\n",
        "                     torch.inverse(alpha * I + rho * (torch.mm(V, torch.t(V)))))\n",
        "    bstar = (alpha * b + rho * torch.sum(U - torch.mm(W, V), dim=1).reshape(b.size())) / (rho * N + alpha)\n",
        "    return Wstar, bstar"
      ],
      "metadata": {
        "id": "rPdLFBQ_Jf0R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_prox(a, b, gamma, d, N):\n",
        "    \"\"\"\n",
        "    The function compute the solution to the relu proximal update problem\n",
        "    \n",
        "    :param a: the a in the closed formula of the linearized update\n",
        "    :param b: the b in the closed formula of the linearized update\n",
        "    :param gamma: The constant used in the update\n",
        "    :param d: the dimension of the current layer\n",
        "    :param N: The number of samples\n",
        "    :return: The obtained solution of the prox update\n",
        "    \"\"\"\n",
        "    val = torch.empty(d, N, device=device)\n",
        "    x = (a + gamma * b) / (1 + gamma)\n",
        "    y = torch.min(b, torch.zeros(d, N, device=device))\n",
        "    val = torch.where(a + gamma * b < 0, y, torch.zeros(d, N, device=device))\n",
        "    val = torch.where(\n",
        "        ((a + gamma * b >= 0) & (b >= 0)) | ((a * (gamma - np.sqrt(gamma * (gamma + 1))) <= gamma * b) & (b < 0)), x,\n",
        "        val)\n",
        "    val = torch.where((-a <= gamma * b) & (gamma * b <= a * (gamma - np.sqrt(gamma * (gamma + 1)))), b, val)\n",
        "    return val\n"
      ],
      "metadata": {
        "id": "xD0dhzjdJiKg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_pred(Ws,bs,input,N):\n",
        "  \"\"\"\n",
        "  The function is used to make the predictions based on the best found parameters\n",
        "  :param Ws: The weight matrices\n",
        "  :param bs: the bias vectors\n",
        "  :return pred, prob\n",
        "  \"\"\"\n",
        "  a1_train = input\n",
        "  for i in range(0,len(Ws)-1):\n",
        "    a1_train = nn.ReLU()(torch.addmm(bs[i].repeat(1, N), Ws[i], a1_train))\n",
        "  pred = torch.argmax(torch.addmm(bs[len(Ws)-1].repeat(1, N), Ws[len(Ws)-1], a1_train), dim=0)\n",
        "  output_last = torch.addmm(bs[len(Ws)-1].repeat(1, N), Ws[len(Ws)-1], a1_train)\n",
        "  prob = torch.exp(output_last)/torch.sum(torch.exp(output_last),dim=0)\n",
        "  return pred, prob"
      ],
      "metadata": {
        "id": "5C2ka5vzJlCm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import cross_entropy\n",
        "cross_entropy = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "-Lq1gjZ4Zgfo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shift_right(l):\n",
        "  return l[-1:]+l[:-1]\n",
        "\n",
        "def filter_conv(W,I1,I2,size = 2):\n",
        "  mask_list = []\n",
        "  for i in range(size):\n",
        "    mask_list += [1]*size+[0]*(I2-size)\n",
        "  mask_list +=[0]*(I1-size)*I2\n",
        "  full_mask = [mask_list]\n",
        "  counter = I2-size\n",
        "  for i in range((I2-size+1)*(I1-size+1)-1):\n",
        "    next_mask=shift_right(full_mask[-1])\n",
        "    #print(counter)\n",
        "    if(counter==0):\n",
        "      counter = I2-size\n",
        "      for j in range(size-1):\n",
        "        next_mask=shift_right(next_mask)\n",
        "    else:\n",
        "      counter -=1\n",
        "    full_mask.append(next_mask)\n",
        "  return torch.mul(torch.tensor(full_mask).to(device),W)\n",
        "\n",
        "def avg_pool(W,I1,I2,size = 2):\n",
        "  mask_list = []\n",
        "  for i in range(size):\n",
        "    mask_list += [1/size**2]*size+[0]*(I2-size)\n",
        "  mask_list +=[0]*(I1-size)*I2\n",
        "  full_mask = [mask_list]\n",
        "  counter = I2-size\n",
        "  for i in range((I2-size+1)*(I1-size+1)-1):\n",
        "    next_mask=shift_right(full_mask[-1])\n",
        "    if(counter==0):\n",
        "      counter = I2-size\n",
        "      for j in range(size-1):\n",
        "        next_mask=shift_right(next_mask)\n",
        "    else:\n",
        "      counter -=1\n",
        "    full_mask.append(next_mask)\n",
        "  return torch.mul(torch.tensor(full_mask).to(device),W)"
      ],
      "metadata": {
        "id": "E69pT7pV7A_8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([[   1,    1,    1,    1,   10,   10,   10,   10,  100,  100,  100,  100,\n",
        "        1000, 1000, 1000, 1000]]*4).to(device)\n",
        "filter_conv(X,4,4,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMlnYcC78RLc",
        "outputId": "b08741d0-ef2e-4cbd-d181-f44b8737db15"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   1,    1,    1,    0,   10,   10,   10,    0,  100,  100,  100,    0,\n",
              "            0,    0,    0,    0],\n",
              "        [   0,    1,    1,    1,    0,   10,   10,   10,    0,  100,  100,  100,\n",
              "            0,    0,    0,    0],\n",
              "        [   0,    0,    0,    0,   10,   10,   10,    0,  100,  100,  100,    0,\n",
              "         1000, 1000, 1000,    0],\n",
              "        [   0,    0,    0,    0,    0,   10,   10,   10,    0,  100,  100,  100,\n",
              "            0, 1000, 1000, 1000]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The function requires at least 1 hidden layer otherwise it need some rewrriting\n",
        "def execute_training(layers, input_size, hidden_size, output_size, train_set, val_set, \n",
        "                     train_labels, val_labels, use_gradient, I1 = 40, I2 = 40,\n",
        "                     niter = 100, gamma = 1, alpha = 5):\n",
        "  \"\"\"\n",
        "  The function takes the following arguements and produces a list of weights and biases with which \n",
        "  you can use the make_pred function to get a list of predictions\n",
        "  :param layers: The total number of layers of the network\n",
        "  :param input_size: The total size of the input layer\n",
        "  :param hidden_size: The size of the hidden layer\n",
        "  :param output_size: The size of the output layer (usefull for multiclass classification)\n",
        "  :param train_set: The training set\n",
        "  :param val_set: The validation set\n",
        "  :param train_labels: The training labels\n",
        "  :param val labels: The validation labels\n",
        "  :param use_gradient: True if the first update of V is carried out without linearization but using the gradient\n",
        "  :param niter: The default number of epochs to train the network\n",
        "  :param gamma: The gamma parameter of the algorithm\n",
        "  :param alpha: The alpha parameter of the algorithm\n",
        "  :return Ws,bs: Returns two lists that go in order from the input to the output layer of the weights and the biases of each layer\n",
        "  \"\"\"\n",
        "\n",
        "  N = len(train_labels)\n",
        "  N_test = len(val_labels)\n",
        "\n",
        "  # weight initialization (we replicate pytorch weight initialization)\n",
        "\n",
        "  std = math.sqrt(1/input_size)\n",
        "  W = torch.FloatTensor(hidden_size, input_size).uniform_(-std, std)\n",
        "  b = torch.FloatTensor(hidden_size, 1).uniform_(-std, std)\n",
        "\n",
        "  b = b.to(device = device)\n",
        "  W = W.to(device = device)\n",
        "\n",
        "  U = torch.addmm(b.repeat(1, N), W, x_train) # equivalent to W1@x_train+b1.repeat(1,N)\n",
        "  V = nn.ReLU()(U)\n",
        "\n",
        "  Ws = [W]\n",
        "  bs = [b]\n",
        "  Us = [U]\n",
        "  Vs = [V]\n",
        "  row = [I1]\n",
        "  col = [I2]\n",
        "\n",
        "  cr_row_size = I1\n",
        "  cr_col_size = I2\n",
        "  size = 4\n",
        "  avg_size = 2\n",
        "  for i in range(1,layers-1):\n",
        "    std = math.sqrt(1/hidden_size)\n",
        "    W = torch.FloatTensor((cr_row_size - size+1)*(cr_col_size - size+1),cr_row_size*cr_col_size).uniform_(-std, std)\n",
        "    b = torch.FloatTensor((cr_row_size - size+1)*(cr_col_size - size+1),1).uniform_(-std, std)\n",
        "    b = b.to(device = device)\n",
        "    W = filter_conv(W.to(device = device),cr_col_size,cr_row_size,size)\n",
        "    row.append(cr_row_size)\n",
        "    col.append(cr_col_size)\n",
        "    cr_row_size = cr_row_size - size+1\n",
        "    cr_col_size = cr_col_size - size+1\n",
        "    U = torch.addmm(b.repeat(1, N), W, Vs[-1])\n",
        "    V = nn.ReLU()(U)\n",
        "    Ws.append(W)\n",
        "    bs.append(b)\n",
        "    Us.append(U)\n",
        "    Vs.append(V)\n",
        "  \n",
        "  row.append(cr_row_size)\n",
        "  col.append(cr_col_size)\n",
        "  std = math.sqrt(1/hidden_size)\n",
        "  W = torch.FloatTensor(output_size, cr_row_size*cr_col_size).uniform_(-std, std)\n",
        "  b = torch.FloatTensor(output_size, 1).uniform_(-std, std)\n",
        "\n",
        "  # we move them to GPU\n",
        "  b = b.to(device = device)\n",
        "  W = W.to(device = device)\n",
        "  U = torch.addmm(b.repeat(1, N), W, Vs[-1])\n",
        "  V = U\n",
        "  Ws.append(W)\n",
        "  bs.append(b)\n",
        "  Us.append(U)\n",
        "  Vs.append(V)\n",
        "  \n",
        "  # constant initialization\n",
        "\n",
        "  gamma1 = gamma2 = gamma3 = gamma4 = gamma\n",
        "\n",
        "  rho = gamma\n",
        "  rho1 = rho2 = rho3 = rho4 = rho\n",
        "\n",
        "  alpha1 = alpha2 = alpha3 = alpha4 = alpha5 = alpha6 = alpha7 \\\n",
        "  = alpha8 = alpha9 = alpha10 = alpha\n",
        "\n",
        "  # vector of performance initialization\n",
        "\n",
        "  loss1 = np.empty(niter)\n",
        "  loss2 = np.empty(niter)\n",
        "  loss_class = np.empty(niter)\n",
        "  accuracy_train = np.empty(niter)\n",
        "  accuracy_test = np.empty(niter)\n",
        "  time1 = np.empty(niter)\n",
        "\n",
        "  opt_accuracy = 0\n",
        "  early_Ws = Ws\n",
        "  early_bs = bs\n",
        "  print('Train on', N, 'samples, validate on', N_test, 'samples')\n",
        "  for k in range(niter):\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # update V3\n",
        "    if use_gradient == True:\n",
        "      if (k == 1):\n",
        "        Vs[-1] = (y_train_one_hot + gamma3*Us[-1] + alpha1*Vs[-1])/(1+ gamma3 + alpha1)\n",
        "      else:\n",
        "        for i in range(250):\n",
        "          Vs[-1] = Vs[-1] - (torch.exp(Vs[-1])/torch.sum(torch.exp(Vs[-1]),dim=0)-y_train_one_hot) * 0.01/(i+1)\n",
        "    else:\n",
        "      Vs[-1] = (y_train_one_hot + gamma3*Us[-1] + alpha1*Vs[-1])/(1+ gamma3 + alpha1)\n",
        "\n",
        "    # update U3 \n",
        "    Us[-1] = (gamma3*Vs[-1] + rho3*(torch.mm(Ws[-1],Vs[-2]) + bs[-1].repeat(1,N)))/(gamma3 + rho3)\n",
        "\n",
        "    # update W3 and b3\n",
        "    W, b = update_wb_js(Us[-1],Vs[-2],Ws[-1],bs[-1],alpha1, rho3)\n",
        "    Ws[-1] = W\n",
        "    bs[-1] = b\n",
        "\n",
        "    for i in range(len(Vs)-2,0,-1):\n",
        "      Vs[i] = update_v_js(Us[i],Us[i+1],Ws[i+1],bs[i+1],rho3,gamma2)\n",
        "      Us[i] = relu_prox(Vs[i],(rho2*torch.addmm(bs[i].repeat(1,N), Ws[i], Vs[i-1]) +\n",
        "                               alpha2*Us[i])/(rho2 + alpha2),(rho2 + alpha2)/gamma2, row[i+1]*col[i+1], N)\n",
        "      W,b = update_wb_js(Us[i],Vs[i-1],Ws[i],bs[i],alpha3,rho2)\n",
        "      Ws[i] = filter_conv(W,col[i],row[i],size)\n",
        "      bs[i]= b\n",
        "    \n",
        "    # update V1\n",
        "    Vs[0] = update_v_js(Us[0],Us[1],Ws[1],bs[1],rho2,gamma1)\n",
        "    \n",
        "    # update U1\n",
        "    Us[0] = relu_prox(Vs[0],(rho1*torch.addmm(bs[0].repeat(1,N), Ws[0], x_train) +\n",
        "                             alpha7*Us[0])/(rho1 + alpha7),(rho1 + alpha7)/gamma1, hidden_size, N)\n",
        "    \n",
        "    # update W1 and b1\n",
        "    W, b = update_wb_js(Us[0],x_train,Ws[0],bs[0],alpha8,rho1)\n",
        "    Ws[0] = W\n",
        "    bs[0] = b\n",
        "\n",
        "    #a1_train = nn.ReLU()(torch.addmm(b1.repeat(1, N), W1, x_train))\n",
        "    #a1_train = x_train\n",
        "    #for i in range(len(Vs)-1,0,-1):\n",
        "    #  a1_train = nn.ReLU()(torch.addmm(bs[i].repeat(1, N), Ws[i], a1_train))\n",
        "    #pred = torch.argmax(torch.addmm(bs[0].repeat(1, N), Ws[0], a1_train), dim=0)\n",
        "    pred,_ = make_pred(Ws,bs,x_train,N)\n",
        "\n",
        "    #a1_test = x_test\n",
        "    #a1_test = nn.ReLU()(torch.addmm(b1.repeat(1, N_test), W1, x_test))\n",
        "    #for i in range(len(Vs)-1,0,-1):\n",
        "    #  a1_test = nn.ReLU()(torch.addmm(bs[i].repeat(1, N_test), Ws[i], a1_test))\n",
        "    #pred_test = torch.argmax(torch.addmm(bs[0].repeat(1, N_test), Ws[0], a1_test), dim=0)\n",
        "    pred_test, prob_test = make_pred(Ws,bs,x_test,N_test) \n",
        "\n",
        "    \n",
        "    loss_class[k] = torch.sum(- y_test_one_hot * torch.log(prob_test))\n",
        "\n",
        "    loss1[k] = gamma/2*torch.pow(torch.dist(Vs[-1],y_train_one_hot,2),2).cpu().numpy()\n",
        "    loss2[k] = loss1[k] + gamma/2 * torch.pow(torch.dist(torch.addmm(bs[0].repeat(1,N), Ws[0], x_train),Us[0],2),2).cpu().numpy()\n",
        "\n",
        "    for i in range(1,layers):\n",
        "      loss2[k] = loss2[k] + gamma/2 * torch.pow(torch.dist(torch.addmm(bs[i].repeat(1,N), Ws[i], Vs[i-1]),Us[i],2),2).cpu().numpy()\n",
        "\n",
        "    #loss2[k] = loss1[k] + rho1/2*torch.pow(torch.dist(torch.addmm(b1.repeat(1,N), W1, x_train),U1,2),2).cpu().numpy() \\\n",
        "    #+rho2/2*torch.pow(torch.dist(torch.addmm(b2.repeat(1,N), W2, V1),U2,2),2).cpu().numpy() \\\n",
        "    #+rho3/2*torch.pow(torch.dist(torch.addmm(b3.repeat(1,N), W3, V2),U3,2),2).cpu().numpy()\n",
        "        \n",
        "    # compute training accuracy\n",
        "    correct_train = pred == train_labels\n",
        "    accuracy_train[k] = np.mean(correct_train.cpu().numpy())\n",
        "        \n",
        "    # compute validation accuracy\n",
        "    correct_test = pred_test == val_labels\n",
        "    accuracy_test[k] = np.mean(correct_test.cpu().numpy())\n",
        "        \n",
        "    # compute training time\n",
        "    stop = time.time()\n",
        "    duration = stop - start\n",
        "    time1[k] = duration\n",
        "        \n",
        "    # print results\n",
        "    print('Epoch', k + 1, '/', niter, '\\n', \n",
        "          '-', 'time:', time1[k], '-', 'sq_loss:', loss1[k], '-', 'tot_loss:',\n",
        "          loss2[k], '-', 'loss_class:', loss_class[k], '-', 'acc:',\n",
        "          accuracy_train[k], '-', 'val_acc:', accuracy_test[k])\n",
        "    if(accuracy_test[k]>opt_accuracy):\n",
        "      early_Ws = Ws\n",
        "      early_bs = bs\n",
        "      opt_accuracy = accuracy_test[k]  \n",
        "\n",
        "  print('The total time spent is:', np.sum(time1), 's')\n",
        "  print('\\n\\n')\n",
        "  print('Early stopping accuracy:',opt_accuracy)\n",
        "  return loss1,loss_class,accuracy_train,accuracy_test,early_Ws,early_bs"
      ],
      "metadata": {
        "id": "lJSDpRPRJ0xb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, loss_class , accuracy_train, accuracy_test,Ws,bs = execute_training(4, input_size, hidden_size, output_size, x_train, x_test, y_train, y_test,\n",
        "                                         False, niter = 100, gamma = 0.1, alpha = 2)"
      ],
      "metadata": {
        "id": "PLkWZMwqJ1vH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6366b47-f563-4ca2-d65f-7fe67e5ad9a7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1 / 100 \n",
            " - time: 1.1748695373535156 - sq_loss: 1378.5663085937501 - tot_loss: 1428.345263945102 - loss_class: 23026.830078125 - acc: 0.09863333333333334 - val_acc: 0.0958\n",
            "Epoch 2 / 100 \n",
            " - time: 1.1446926593780518 - sq_loss: 647.0463378906251 - tot_loss: 654.5904849146988 - loss_class: 23012.47265625 - acc: 0.14405 - val_acc: 0.1376\n",
            "Epoch 3 / 100 \n",
            " - time: 1.1516189575195312 - sq_loss: 311.63774414062505 - tot_loss: 315.1818785616313 - loss_class: 22999.58984375 - acc: 0.11248333333333334 - val_acc: 0.1137\n",
            "Epoch 4 / 100 \n",
            " - time: 1.148057460784912 - sq_loss: 151.592724609375 - tot_loss: 153.6339026550471 - loss_class: 22988.55859375 - acc: 0.11236666666666667 - val_acc: 0.1135\n",
            "Epoch 5 / 100 \n",
            " - time: 1.1394073963165283 - sq_loss: 74.21776733398438 - tot_loss: 75.75281491090136 - loss_class: 22978.966796875 - acc: 0.11236666666666667 - val_acc: 0.1136\n",
            "Epoch 6 / 100 \n",
            " - time: 1.146827220916748 - sq_loss: 36.51699523925782 - tot_loss: 37.941257097703065 - loss_class: 22970.33203125 - acc: 0.11263333333333334 - val_acc: 0.1139\n",
            "Epoch 7 / 100 \n",
            " - time: 1.1442816257476807 - sq_loss: 18.047039794921876 - tot_loss: 19.534478697815214 - loss_class: 22962.2734375 - acc: 0.11388333333333334 - val_acc: 0.1156\n",
            "Epoch 8 / 100 \n",
            " - time: 1.1321098804473877 - sq_loss: 8.959795379638672 - tot_loss: 10.596040707774229 - loss_class: 22954.52734375 - acc: 0.11755 - val_acc: 0.1184\n",
            "Epoch 9 / 100 \n",
            " - time: 1.1398310661315918 - sq_loss: 4.4718467712402346 - tot_loss: 6.300522122887196 - loss_class: 22946.923828125 - acc: 0.12273333333333333 - val_acc: 0.1213\n",
            "Epoch 10 / 100 \n",
            " - time: 1.1456573009490967 - sq_loss: 2.2467002868652344 - tot_loss: 4.288873786793556 - loss_class: 22939.35546875 - acc: 0.13091666666666665 - val_acc: 0.1286\n",
            "Epoch 11 / 100 \n",
            " - time: 1.1473321914672852 - sq_loss: 1.1384800910949708 - tot_loss: 3.402572345605586 - loss_class: 22931.75390625 - acc: 0.14053333333333334 - val_acc: 0.1406\n",
            "Epoch 12 / 100 \n",
            " - time: 1.1342432498931885 - sq_loss: 0.583445692062378 - tot_loss: 3.0700721196248195 - loss_class: 22924.0703125 - acc: 0.15151666666666666 - val_acc: 0.1545\n",
            "Epoch 13 / 100 \n",
            " - time: 1.1538009643554688 - sq_loss: 0.3034686326980591 - tot_loss: 3.0083368922700178 - loss_class: 22916.27734375 - acc: 0.16566666666666666 - val_acc: 0.1682\n",
            "Epoch 14 / 100 \n",
            " - time: 1.1397383213043213 - sq_loss: 0.16092437505722046 - tot_loss: 3.0766320266819096 - loss_class: 22908.357421875 - acc: 0.18226666666666666 - val_acc: 0.185\n",
            "Epoch 15 / 100 \n",
            " - time: 1.1387410163879395 - sq_loss: 0.0874791979789734 - tot_loss: 3.2044772456458306 - loss_class: 22900.306640625 - acc: 0.20041666666666666 - val_acc: 0.2049\n",
            "Epoch 16 / 100 \n",
            " - time: 1.1388309001922607 - sq_loss: 0.04905820488929749 - tot_loss: 3.356468255928485 - loss_class: 22892.11328125 - acc: 0.21995 - val_acc: 0.2278\n",
            "Epoch 17 / 100 \n",
            " - time: 1.1500489711761475 - sq_loss: 0.028575652837753297 - tot_loss: 3.514681919047143 - loss_class: 22883.78125 - acc: 0.24063333333333334 - val_acc: 0.2478\n",
            "Epoch 18 / 100 \n",
            " - time: 1.1396615505218506 - sq_loss: 0.017403292655944824 - tot_loss: 3.670050119835651 - loss_class: 22875.310546875 - acc: 0.26098333333333334 - val_acc: 0.2669\n",
            "Epoch 19 / 100 \n",
            " - time: 1.136648178100586 - sq_loss: 0.01114344894886017 - tot_loss: 3.8180040053150153 - loss_class: 22866.7109375 - acc: 0.2813333333333333 - val_acc: 0.2844\n",
            "Epoch 20 / 100 \n",
            " - time: 1.1437125205993652 - sq_loss: 0.007528028637170792 - tot_loss: 3.956370852288092 - loss_class: 22857.984375 - acc: 0.30193333333333333 - val_acc: 0.3019\n",
            "Epoch 21 / 100 \n",
            " - time: 1.1437675952911377 - sq_loss: 0.0053698476403951645 - tot_loss: 4.084157113276888 - loss_class: 22849.12890625 - acc: 0.32211666666666666 - val_acc: 0.321\n",
            "Epoch 22 / 100 \n",
            " - time: 1.1444904804229736 - sq_loss: 0.004036261886358261 - tot_loss: 4.201072962215403 - loss_class: 22840.16015625 - acc: 0.34203333333333336 - val_acc: 0.3396\n",
            "Epoch 23 / 100 \n",
            " - time: 1.1415984630584717 - sq_loss: 0.00318279005587101 - tot_loss: 4.307177607278572 - loss_class: 22831.0859375 - acc: 0.36106666666666665 - val_acc: 0.3593\n",
            "Epoch 24 / 100 \n",
            " - time: 1.1455698013305664 - sq_loss: 0.00261722132563591 - tot_loss: 4.402751335443463 - loss_class: 22821.904296875 - acc: 0.38075 - val_acc: 0.3804\n",
            "Epoch 25 / 100 \n",
            " - time: 1.1383917331695557 - sq_loss: 0.0022294428199529648 - tot_loss: 4.488159865420312 - loss_class: 22812.623046875 - acc: 0.39915 - val_acc: 0.4019\n",
            "Epoch 26 / 100 \n",
            " - time: 1.137509822845459 - sq_loss: 0.001954602636396885 - tot_loss: 4.563902144646272 - loss_class: 22803.248046875 - acc: 0.4189 - val_acc: 0.4237\n",
            "Epoch 27 / 100 \n",
            " - time: 1.143996000289917 - sq_loss: 0.0017533665522933006 - tot_loss: 4.630489121348365 - loss_class: 22793.787109375 - acc: 0.43801666666666667 - val_acc: 0.4439\n",
            "Epoch 28 / 100 \n",
            " - time: 1.1389944553375244 - sq_loss: 0.0016013292595744135 - tot_loss: 4.688430830574362 - loss_class: 22784.24609375 - acc: 0.45621666666666666 - val_acc: 0.4618\n",
            "Epoch 29 / 100 \n",
            " - time: 1.1628949642181396 - sq_loss: 0.0014829447492957116 - tot_loss: 4.7382797096273865 - loss_class: 22774.625 - acc: 0.47368333333333335 - val_acc: 0.4798\n",
            "Epoch 30 / 100 \n",
            " - time: 1.1899869441986084 - sq_loss: 0.0013880993239581585 - tot_loss: 4.780487852543592 - loss_class: 22764.9296875 - acc: 0.49156666666666665 - val_acc: 0.4968\n",
            "Epoch 31 / 100 \n",
            " - time: 1.16536545753479 - sq_loss: 0.0013101098127663137 - tot_loss: 4.815634583251086 - loss_class: 22755.162109375 - acc: 0.5085666666666666 - val_acc: 0.5144\n",
            "Epoch 32 / 100 \n",
            " - time: 1.1381261348724365 - sq_loss: 0.001244373619556427 - tot_loss: 4.844171740510501 - loss_class: 22745.3359375 - acc: 0.52485 - val_acc: 0.5295\n",
            "Epoch 33 / 100 \n",
            " - time: 1.1369473934173584 - sq_loss: 0.001187801454216242 - tot_loss: 4.866609665477881 - loss_class: 22735.4453125 - acc: 0.5402 - val_acc: 0.5447\n",
            "Epoch 34 / 100 \n",
            " - time: 1.142026424407959 - sq_loss: 0.001138197723776102 - tot_loss: 4.883378670935053 - loss_class: 22725.498046875 - acc: 0.55525 - val_acc: 0.5624\n",
            "Epoch 35 / 100 \n",
            " - time: 1.132624864578247 - sq_loss: 0.0010940599255263807 - tot_loss: 4.894918905495434 - loss_class: 22715.5 - acc: 0.5686666666666667 - val_acc: 0.5771\n",
            "Epoch 36 / 100 \n",
            " - time: 1.1359009742736816 - sq_loss: 0.0010542727075517177 - tot_loss: 4.901642976771108 - loss_class: 22705.4453125 - acc: 0.5828833333333333 - val_acc: 0.5909\n",
            "Epoch 37 / 100 \n",
            " - time: 1.1646840572357178 - sq_loss: 0.0010180290788412095 - tot_loss: 4.903906527452637 - loss_class: 22695.34375 - acc: 0.5954833333333334 - val_acc: 0.6037\n",
            "Epoch 38 / 100 \n",
            " - time: 1.13515305519104 - sq_loss: 0.0009847454726696016 - tot_loss: 4.902104216179578 - loss_class: 22685.19140625 - acc: 0.6070833333333333 - val_acc: 0.6161\n",
            "Epoch 39 / 100 \n",
            " - time: 1.135411262512207 - sq_loss: 0.00095394067466259 - tot_loss: 4.896593880082947 - loss_class: 22675.001953125 - acc: 0.6177166666666667 - val_acc: 0.6285\n",
            "Epoch 40 / 100 \n",
            " - time: 1.135143518447876 - sq_loss: 0.0009252508170902729 - tot_loss: 4.887667329568648 - loss_class: 22664.76953125 - acc: 0.6269166666666667 - val_acc: 0.6385\n",
            "Epoch 41 / 100 \n",
            " - time: 1.1385471820831299 - sq_loss: 0.0008984425105154515 - tot_loss: 4.875647744064917 - loss_class: 22654.498046875 - acc: 0.6363 - val_acc: 0.647\n",
            "Epoch 42 / 100 \n",
            " - time: 1.133965015411377 - sq_loss: 0.000873261597007513 - tot_loss: 4.860798566869926 - loss_class: 22644.193359375 - acc: 0.6450166666666667 - val_acc: 0.6556\n",
            "Epoch 43 / 100 \n",
            " - time: 1.1442220211029053 - sq_loss: 0.0008495616726577282 - tot_loss: 4.843383135279874 - loss_class: 22633.8515625 - acc: 0.65315 - val_acc: 0.6656\n",
            "Epoch 44 / 100 \n",
            " - time: 1.1875722408294678 - sq_loss: 0.0008271649479866029 - tot_loss: 4.82364233028493 - loss_class: 22623.46484375 - acc: 0.6607166666666666 - val_acc: 0.6736\n",
            "Epoch 45 / 100 \n",
            " - time: 1.133781909942627 - sq_loss: 0.0008059547282755375 - tot_loss: 4.80181227234425 - loss_class: 22613.0625 - acc: 0.66795 - val_acc: 0.6828\n",
            "Epoch 46 / 100 \n",
            " - time: 1.1353340148925781 - sq_loss: 0.0007858211174607277 - tot_loss: 4.778105264942861 - loss_class: 22602.626953125 - acc: 0.6749333333333334 - val_acc: 0.6882\n",
            "Epoch 47 / 100 \n",
            " - time: 1.1351678371429443 - sq_loss: 0.0007666779216378927 - tot_loss: 4.752714182506315 - loss_class: 22592.162109375 - acc: 0.6808166666666666 - val_acc: 0.6951\n",
            "Epoch 48 / 100 \n",
            " - time: 1.1400270462036133 - sq_loss: 0.0007484342902898789 - tot_loss: 4.725807626405731 - loss_class: 22581.6796875 - acc: 0.6866166666666667 - val_acc: 0.702\n",
            "Epoch 49 / 100 \n",
            " - time: 1.134134292602539 - sq_loss: 0.0007310425397008658 - tot_loss: 4.697545396181523 - loss_class: 22571.16015625 - acc: 0.6927666666666666 - val_acc: 0.7077\n",
            "Epoch 50 / 100 \n",
            " - time: 1.1375555992126465 - sq_loss: 0.0007144217379391193 - tot_loss: 4.668093904847047 - loss_class: 22560.625 - acc: 0.6972333333333334 - val_acc: 0.7123\n",
            "Epoch 51 / 100 \n",
            " - time: 1.1332600116729736 - sq_loss: 0.0006985076237469912 - tot_loss: 4.637597549578641 - loss_class: 22550.076171875 - acc: 0.70215 - val_acc: 0.7173\n",
            "Epoch 52 / 100 \n",
            " - time: 1.1442911624908447 - sq_loss: 0.0006832449231296778 - tot_loss: 4.606186064257054 - loss_class: 22539.494140625 - acc: 0.70605 - val_acc: 0.7211\n",
            "Epoch 53 / 100 \n",
            " - time: 1.1446247100830078 - sq_loss: 0.0006685812026262284 - tot_loss: 4.573966099467362 - loss_class: 22528.8984375 - acc: 0.7102666666666667 - val_acc: 0.725\n",
            "Epoch 54 / 100 \n",
            " - time: 1.135709285736084 - sq_loss: 0.0006545075215399266 - tot_loss: 4.541063223697711 - loss_class: 22518.29296875 - acc: 0.71385 - val_acc: 0.7284\n",
            "Epoch 55 / 100 \n",
            " - time: 1.1376214027404785 - sq_loss: 0.0006409765221178532 - tot_loss: 4.507570557313739 - loss_class: 22507.669921875 - acc: 0.7175833333333334 - val_acc: 0.7322\n",
            "Epoch 56 / 100 \n",
            " - time: 1.1326603889465332 - sq_loss: 0.0006279406137764455 - tot_loss: 4.473566486977507 - loss_class: 22497.0390625 - acc: 0.7206 - val_acc: 0.7346\n",
            "Epoch 57 / 100 \n",
            " - time: 1.1349985599517822 - sq_loss: 0.0006153791677206756 - tot_loss: 4.439152834337437 - loss_class: 22486.3828125 - acc: 0.7235166666666667 - val_acc: 0.737\n",
            "Epoch 58 / 100 \n",
            " - time: 1.1490533351898193 - sq_loss: 0.0006032536271959543 - tot_loss: 4.4044101841165695 - loss_class: 22475.71875 - acc: 0.7271833333333333 - val_acc: 0.74\n",
            "Epoch 59 / 100 \n",
            " - time: 1.1351146697998047 - sq_loss: 0.0005915319081395865 - tot_loss: 4.369362718291814 - loss_class: 22465.037109375 - acc: 0.7301166666666666 - val_acc: 0.7436\n",
            "Epoch 60 / 100 \n",
            " - time: 1.1433343887329102 - sq_loss: 0.0005802150815725327 - tot_loss: 4.33417193662026 - loss_class: 22454.36328125 - acc: 0.7325833333333334 - val_acc: 0.7463\n",
            "Epoch 61 / 100 \n",
            " - time: 1.1406738758087158 - sq_loss: 0.0005692422855645418 - tot_loss: 4.298794786934741 - loss_class: 22443.669921875 - acc: 0.7349833333333333 - val_acc: 0.749\n",
            "Epoch 62 / 100 \n",
            " - time: 1.1383373737335205 - sq_loss: 0.0005586375948041678 - tot_loss: 4.263345218449831 - loss_class: 22432.9765625 - acc: 0.7374666666666667 - val_acc: 0.7507\n",
            "Epoch 63 / 100 \n",
            " - time: 1.142545223236084 - sq_loss: 0.0005483653862029314 - tot_loss: 4.2278301928425215 - loss_class: 22422.28125 - acc: 0.7395666666666667 - val_acc: 0.7527\n",
            "Epoch 64 / 100 \n",
            " - time: 1.1374318599700928 - sq_loss: 0.0005384092219173908 - tot_loss: 4.192328578839079 - loss_class: 22411.572265625 - acc: 0.7414833333333334 - val_acc: 0.7547\n",
            "Epoch 65 / 100 \n",
            " - time: 1.1375470161437988 - sq_loss: 0.0005287254229187965 - tot_loss: 4.156851086358074 - loss_class: 22400.84765625 - acc: 0.7430666666666667 - val_acc: 0.7579\n",
            "Epoch 66 / 100 \n",
            " - time: 1.131258249282837 - sq_loss: 0.0005193241871893406 - tot_loss: 4.121448939619587 - loss_class: 22390.1328125 - acc: 0.7452 - val_acc: 0.7599\n",
            "Epoch 67 / 100 \n",
            " - time: 1.1455469131469727 - sq_loss: 0.0005101836752146483 - tot_loss: 4.0861657027970075 - loss_class: 22379.41796875 - acc: 0.7474333333333333 - val_acc: 0.7618\n",
            "Epoch 68 / 100 \n",
            " - time: 1.134404182434082 - sq_loss: 0.0005012874957174063 - tot_loss: 4.0510262928903105 - loss_class: 22368.705078125 - acc: 0.7489666666666667 - val_acc: 0.763\n",
            "Epoch 69 / 100 \n",
            " - time: 1.133927822113037 - sq_loss: 0.0004926261492073536 - tot_loss: 4.01603493350558 - loss_class: 22357.990234375 - acc: 0.7505666666666667 - val_acc: 0.7648\n",
            "Epoch 70 / 100 \n",
            " - time: 1.1540164947509766 - sq_loss: 0.00048421574756503106 - tot_loss: 3.981224823219236 - loss_class: 22347.2734375 - acc: 0.7521166666666667 - val_acc: 0.7664\n",
            "Epoch 71 / 100 \n",
            " - time: 1.139585256576538 - sq_loss: 0.00047602229751646523 - tot_loss: 3.946635947912 - loss_class: 22336.5625 - acc: 0.7533 - val_acc: 0.7681\n",
            "Epoch 72 / 100 \n",
            " - time: 1.1473796367645264 - sq_loss: 0.00046803150326013565 - tot_loss: 3.912261621805374 - loss_class: 22325.86328125 - acc: 0.7546666666666667 - val_acc: 0.7698\n",
            "Epoch 73 / 100 \n",
            " - time: 1.133596658706665 - sq_loss: 0.0004602346103638411 - tot_loss: 3.87812140948372 - loss_class: 22315.15625 - acc: 0.7560333333333333 - val_acc: 0.771\n",
            "Epoch 74 / 100 \n",
            " - time: 1.1407692432403564 - sq_loss: 0.00045259771868586543 - tot_loss: 3.844251966103912 - loss_class: 22304.45703125 - acc: 0.7573333333333333 - val_acc: 0.7723\n",
            "Epoch 75 / 100 \n",
            " - time: 1.134502649307251 - sq_loss: 0.00044514825567603116 - tot_loss: 3.8106487087323333 - loss_class: 22293.76171875 - acc: 0.75825 - val_acc: 0.7727\n",
            "Epoch 76 / 100 \n",
            " - time: 1.1415305137634277 - sq_loss: 0.0004378828685730696 - tot_loss: 3.7773123746388597 - loss_class: 22283.083984375 - acc: 0.7598 - val_acc: 0.7743\n",
            "Epoch 77 / 100 \n",
            " - time: 1.1353113651275635 - sq_loss: 0.00043079890310764317 - tot_loss: 3.7442648589611056 - loss_class: 22272.3984375 - acc: 0.7609 - val_acc: 0.7753\n",
            "Epoch 78 / 100 \n",
            " - time: 1.137040376663208 - sq_loss: 0.00042389673180878165 - tot_loss: 3.711530840303749 - loss_class: 22261.73828125 - acc: 0.7622 - val_acc: 0.7767\n",
            "Epoch 79 / 100 \n",
            " - time: 1.1328659057617188 - sq_loss: 0.00041715037077665333 - tot_loss: 3.679104057129007 - loss_class: 22251.07421875 - acc: 0.7633 - val_acc: 0.7782\n",
            "Epoch 80 / 100 \n",
            " - time: 1.1326565742492676 - sq_loss: 0.0004105430096387863 - tot_loss: 3.646976387442556 - loss_class: 22240.435546875 - acc: 0.7643 - val_acc: 0.78\n",
            "Epoch 81 / 100 \n",
            " - time: 1.1346275806427002 - sq_loss: 0.0004040773492306471 - tot_loss: 3.6151457404019314 - loss_class: 22229.791015625 - acc: 0.7652 - val_acc: 0.7815\n",
            "Epoch 82 / 100 \n",
            " - time: 1.1370117664337158 - sq_loss: 0.0003977486863732338 - tot_loss: 3.5836597172077744 - loss_class: 22219.162109375 - acc: 0.7661666666666667 - val_acc: 0.7826\n",
            "Epoch 83 / 100 \n",
            " - time: 1.1322665214538574 - sq_loss: 0.00039154272526502614 - tot_loss: 3.5524803112144587 - loss_class: 22208.55859375 - acc: 0.7672666666666667 - val_acc: 0.783\n",
            "Epoch 84 / 100 \n",
            " - time: 1.1386125087738037 - sq_loss: 0.00038547134026885034 - tot_loss: 3.5216580741456713 - loss_class: 22197.97265625 - acc: 0.7681166666666667 - val_acc: 0.7841\n",
            "Epoch 85 / 100 \n",
            " - time: 1.1369521617889404 - sq_loss: 0.0003795148804783821 - tot_loss: 3.491122638108209 - loss_class: 22187.3828125 - acc: 0.76895 - val_acc: 0.7848\n",
            "Epoch 86 / 100 \n",
            " - time: 1.1363394260406494 - sq_loss: 0.00037369278725236655 - tot_loss: 3.460949654213619 - loss_class: 22176.828125 - acc: 0.76975 - val_acc: 0.7857\n",
            "Epoch 87 / 100 \n",
            " - time: 1.1402039527893066 - sq_loss: 0.0003679882269352675 - tot_loss: 3.431086363561917 - loss_class: 22166.271484375 - acc: 0.7707166666666667 - val_acc: 0.7867\n",
            "Epoch 88 / 100 \n",
            " - time: 1.133493423461914 - sq_loss: 0.0003624144941568375 - tot_loss: 3.4015624023508284 - loss_class: 22155.734375 - acc: 0.7714333333333333 - val_acc: 0.7869\n",
            "Epoch 89 / 100 \n",
            " - time: 1.1325116157531738 - sq_loss: 0.00035694802645593884 - tot_loss: 3.3723600311670454 - loss_class: 22145.21484375 - acc: 0.7718666666666667 - val_acc: 0.7877\n",
            "Epoch 90 / 100 \n",
            " - time: 1.1322481632232666 - sq_loss: 0.000351619184948504 - tot_loss: 3.343470481864643 - loss_class: 22134.705078125 - acc: 0.7726166666666666 - val_acc: 0.7881\n",
            "Epoch 91 / 100 \n",
            " - time: 1.1359381675720215 - sq_loss: 0.000346383941359818 - tot_loss: 3.3149242708692332 - loss_class: 22124.240234375 - acc: 0.7734 - val_acc: 0.7885\n",
            "Epoch 92 / 100 \n",
            " - time: 1.1422615051269531 - sq_loss: 0.0003412504913285375 - tot_loss: 3.286697013594676 - loss_class: 22113.775390625 - acc: 0.7742833333333333 - val_acc: 0.7891\n",
            "Epoch 93 / 100 \n",
            " - time: 1.1443400382995605 - sq_loss: 0.0003362005110830069 - tot_loss: 3.258786917035468 - loss_class: 22103.337890625 - acc: 0.7751833333333333 - val_acc: 0.7895\n",
            "Epoch 94 / 100 \n",
            " - time: 1.1422555446624756 - sq_loss: 0.00033124196343123916 - tot_loss: 3.231210689712316 - loss_class: 22092.90625 - acc: 0.7757833333333334 - val_acc: 0.7901\n",
            "Epoch 95 / 100 \n",
            " - time: 1.1411359310150146 - sq_loss: 0.00032636539544910197 - tot_loss: 3.203923132945784 - loss_class: 22082.5078125 - acc: 0.7765 - val_acc: 0.7905\n",
            "Epoch 96 / 100 \n",
            " - time: 1.1445972919464111 - sq_loss: 0.00032158677931874995 - tot_loss: 3.176991342706606 - loss_class: 22072.13671875 - acc: 0.7769833333333334 - val_acc: 0.7912\n",
            "Epoch 97 / 100 \n",
            " - time: 1.1324236392974854 - sq_loss: 0.00031687945593148473 - tot_loss: 3.150364082399756 - loss_class: 22061.78515625 - acc: 0.7775333333333333 - val_acc: 0.7919\n",
            "Epoch 98 / 100 \n",
            " - time: 1.134462594985962 - sq_loss: 0.0003122488269582391 - tot_loss: 3.1240484651410956 - loss_class: 22051.4453125 - acc: 0.7782833333333333 - val_acc: 0.7926\n",
            "Epoch 99 / 100 \n",
            " - time: 1.1290066242218018 - sq_loss: 0.00030772949103266003 - tot_loss: 3.0980156412115325 - loss_class: 22041.13671875 - acc: 0.7789666666666667 - val_acc: 0.793\n",
            "Epoch 100 / 100 \n",
            " - time: 1.1406333446502686 - sq_loss: 0.00030329606961458926 - tot_loss: 3.072297590272501 - loss_class: 22030.849609375 - acc: 0.7794833333333333 - val_acc: 0.7932\n",
            "The total time spent is: 114.12191843986511 s\n",
            "\n",
            "\n",
            "\n",
            "Early stopping accuracy: 0.7932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pickle_results(name,train_loss,loss_class,accuracy_train,accuracy_val,weights,biases):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  dictionary_save = {\"Weights\": weights,\"Biases\":biases, \"train_loss\": train_loss, \"loss_class\":loss_class,\"accuracy_train\":accuracy_train,\"accuracy_test\":accuracy_val}\n",
        "\n",
        "  results_name = \"results_\"+name\n",
        "  a_file = open(results_name,\"wb\")\n",
        "  pickle.dump(dictionary_save,a_file)\n",
        "  a_file.close()\n",
        "  return"
      ],
      "metadata": {
        "id": "dNzB47TUNnHW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle_results(\"CFAR10-3-1500_class\",loss, loss_class , accuracy_train, accuracy_test,Ws,bs)"
      ],
      "metadata": {
        "id": "ERkwXe1KeDhV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyis of the results"
      ],
      "metadata": {
        "id": "0JYv4fCSKAgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## We plot the train losses\n",
        "\n",
        "plot_train_losses(loss.shape[0], loss, 'Coordinate_descent_CFAR10_3_class')"
      ],
      "metadata": {
        "id": "lar28JQyJ5zi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "7188a78c-1384-49f9-8608-5bd3fad01018"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1c3059fd5b57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## We plot the train losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplot_train_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Coordinate_descent_CFAR10_3_class'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/utilities.py\u001b[0m in \u001b[0;36mplot_train_losses\u001b[0;34m(num_epochs, losses, optimizer)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train losses'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'block_coordinates_figures/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train losses_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2126\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2127\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m                 _png.write_png(renderer._renderer, fh, self.figure.dpi,\n\u001b[1;32m    537\u001b[0m                                metadata={**default_metadata, **metadata})\n",
            "\u001b[0;32m/usr/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mopen_file_cm\u001b[0;34m(path_or_file, mode, encoding)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;34mr\"\"\"Pass through file objects and context-manage `.PathLike`\\s.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_filehandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mto_filehandle\u001b[0;34m(fname, flag, return_opened, encoding)\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seek'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'block_coordinates_figures/train losses_Coordinate_descent_CFAR10_3_class.png'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## We plot the test accuracy\n",
        "\n",
        "plot_test_accuracy(accuracy_test.shape[0], accuracy_test, 'Coordinate_descent_CFAR10_3_class')"
      ],
      "metadata": {
        "id": "hmBh9-j3KFVn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}