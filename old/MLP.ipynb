{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KDbRlCKv4U8",
        "outputId": "f493849a-1dd2-4460-aa4a-5c437e382eaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 1.11.0+cu113\n",
            "Torchvision Version: 0.12.0+cu113\n",
            "GPU is available? True\n"
          ]
        }
      ],
      "source": [
        "#@title Import and Utilities\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms, utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "from utilities import *\n",
        "from Torch_architectures import *\n",
        "from Train_functions import *\n",
        "from CD_utilities import *\n",
        "from layers import *\n",
        "\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "print(\"Torchvision Version:\", torchvision.__version__)\n",
        "print(\"GPU is available?\", torch.cuda.is_available())\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqvTMJAbw74Q"
      },
      "source": [
        "# Imported datasets\n",
        "For the testing and comparison of our algorithms we will use the following datasets:\n",
        "\n",
        "1. MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PEt-flsyEKM"
      },
      "source": [
        "# Train - test split\n",
        "\n",
        "The Code for the Block Coordinate Descent was mostly based on https://github.com/timlautk/BCD-for-DNNs-PyTorch/blob/master/bcd_dnn_mlp_mnist.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVkAgTGRo1is"
      },
      "outputs": [],
      "source": [
        "#@title Dataset & Optimizer Selection\n",
        "ts = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0,), (1,))])\n",
        "\n",
        "# change the flag to choose the dataset to work with\n",
        "dataset_flag = \"MNIST\" #@param ['MNIST','FMNIST','CIFAR10']\n",
        "batch_size = 256 #@param {type:\"integer\"}\n",
        "if dataset_flag =='MNIST':\n",
        "  trainset = datasets.MNIST('../data', train=True, download=True, transform=ts)\n",
        "  testset = datasets.MNIST(root='../data', train=False, download=True, transform=ts)\n",
        "  dataset_train = torch.utils.data.DataLoader(testset,batch_size = 128, shuffle = True)\n",
        "  dataset_test = torch.utils.data.DataLoader(trainset,batch_size = batch_size,shuffle = True)\n",
        "elif dataset_flag =='FMNIST':\n",
        "  trainset = datasets.FashionMNIST('../data', train=True, download=True, transform=ts)\n",
        "  testset = datasets.FashionMNIST(root='../data', train=False, download=True, transform=ts)\n",
        "  dataset_train = torch.utils.data.DataLoader(testset,batch_size = 128, shuffle = True)\n",
        "  dataset_test = torch.utils.data.DataLoader(trainset,batch_size = batch_size,shuffle = True)\n",
        "elif dataset_flag=='CIFAR10':\n",
        "  trainset = datasets.CIFAR10('../data', train=True, download=True, transform=ts)\n",
        "  testset = datasets.CIFAR10(root='../data', train=False, download=True, transform=ts)\n",
        "  dataset_train = torch.utils.data.DataLoader(testset,batch_size = 128, shuffle = True)\n",
        "  dataset_test = torch.utils.data.DataLoader(trainset,batch_size = batch_size,shuffle = True)\n",
        "\n",
        "x_train, y_train, x_test, y_test,y_train_one_hot, y_test_one_hot, I1, I2 = load_dataset(trainset, testset,10)\n",
        "\n",
        "# we move to device to use GPU\n",
        "\n",
        "x_train = x_train.to(device = device)\n",
        "x_test = x_test.to(device = device)\n",
        "y_train = y_train.to(device = device)\n",
        "y_test = y_test.to(device = device)\n",
        "y_train_one_hot = y_train_one_hot.to(device)\n",
        "y_test_one_hot = y_test_one_hot.to(device)\n",
        "input_size = x_train.shape[0]\n",
        "hidden_size = 2*input_size\n",
        "output_size = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lX28NWho-N4"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "#@title Model Selection\n",
        "model_name = 'Multilayer-Perceptron' #@param ['Multilayer-Perceptron']\n",
        "optimizer_name = \"Coordinate-Descent+SGD\" #@param ['SGD','Adam','Coordinate-Descent','Coordinate-Descent+SGD','Coordinate-Descent+Adam']\n",
        "momentum = 0.9 #@param {type:\"number\"}\n",
        "lr = 0.001 #@param {type:\"number\"}\n",
        "weight_decay = 0.00 #@param {type:\"number\"}\n",
        "beta_1 = 0.9 #@param {type:\"number\"}\n",
        "beta_2 = 0.999 #@param {type:\"number\"}\n",
        "epochs = 40 #@param {type:\"integer\"}\n",
        "#the ratio of the epochs for coordinate descent for mixed classifiers\n",
        "ratio =  0.7#@param {type:\"number\"}\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "if(model_name =='Multilayer-Perceptron'):\n",
        "  model = MultiLayerPerceptron(input_size,hidden_size,output_size) \n",
        "\n",
        "\n",
        "if (optimizer_name == \"SGD\" or optimizer_name == \"Coordinate-Descent+SGD\"):\n",
        "  #print(\"Got in SGD\")\n",
        "  optimizer = torch.optim.SGD(params=model.parameters(), lr=lr,\n",
        "                              momentum=momentum, weight_decay=weight_decay)\n",
        "  assert lr > 0\n",
        "  assert 0 <= momentum <= 1\n",
        "elif (optimizer_name == \"Adam\" or optimizer_name == \"Coordinate-Descent+Adam\"):\n",
        "  #print(\"Got in Adam\")\n",
        "  optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, \n",
        "                               betas=(beta_1, beta_2), weight_decay=weight_decay)\n",
        "if(optimizer_name != 'Coordinate-Descent'):\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCDiTSRJ5PHH"
      },
      "source": [
        "# Optimizers & Loss functions Definitions\n",
        "\n",
        "1. SGD from pytorch \n",
        "2. CrossEntropyLoss function criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrFu1jgT3kLz"
      },
      "source": [
        "# Training\n",
        "\n",
        "Note: Fix it so that it moves everything to device in the following function and that it does the label sample split here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_O44lBu4TC_",
        "outputId": "048a0280-49b0-4a46-94c8-e3856f547041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training BCD\n",
            "Layer  0  W  torch.Size([1568, 784])  Layer W  torch.Size([1568, 784])\n",
            "['Perceptron']\n",
            "Layer  1  W  torch.Size([1568, 1568])  Layer W  torch.Size([1568, 1568])\n",
            "['Perceptron']\n",
            "Layer  2  W  torch.Size([1568, 1568])  Layer W  torch.Size([1568, 1568])\n",
            "['Perceptron']\n",
            "1 1568\n",
            "torch.Size([10, 1568]) torch.Size([1568, 60000])\n",
            "Layer  0  W  torch.Size([1568, 1])  Layer W  torch.Size([1568, 1])\n",
            "['Perceptron']\n",
            "Layer  1  W  torch.Size([1568, 1])  Layer W  torch.Size([1568, 1])\n",
            "['Perceptron']\n",
            "Layer  2  W  torch.Size([1568, 1])  Layer W  torch.Size([1568, 1])\n",
            "['Perceptron']\n",
            "Layer  3  W  torch.Size([10, 1])  Layer W  torch.Size([10, 1])\n",
            "['Perceptron', 10]\n",
            "4 4\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1 / 8 \n",
            " - time: 1.6153268814086914 - sq_loss: 1925.2294921875 - tot_loss: 1925.2295242536131 - loss_class: 22549.66796875 - acc: 0.79645 - val_acc: 0.7981\n",
            "Epoch 2 / 8 \n",
            " - time: 1.6099071502685547 - sq_loss: 1251.66904296875 - tot_loss: 1251.6690744127702 - loss_class: 21911.873046875 - acc: 0.8749333333333333 - val_acc: 0.8782\n",
            "Epoch 3 / 8 \n",
            " - time: 1.6095201969146729 - sq_loss: 817.3739746093751 - tot_loss: 817.3740154671691 - loss_class: 21252.921875 - acc: 0.8897833333333334 - val_acc: 0.8927\n",
            "Epoch 4 / 8 \n",
            " - time: 1.6103434562683105 - sq_loss: 535.283203125 - tot_loss: 535.2832532323591 - loss_class: 20659.36328125 - acc: 0.89425 - val_acc: 0.8978\n",
            "Epoch 5 / 8 \n",
            " - time: 1.610306978225708 - sq_loss: 351.1233642578125 - tot_loss: 351.1234339419345 - loss_class: 20153.546875 - acc: 0.8965833333333333 - val_acc: 0.8988\n",
            "Epoch 6 / 8 \n",
            " - time: 1.6094927787780762 - sq_loss: 230.55532226562502 - tot_loss: 230.55541822471426 - loss_class: 19732.73046875 - acc: 0.8979833333333334 - val_acc: 0.8995\n",
            "Epoch 7 / 8 \n",
            " - time: 1.611361026763916 - sq_loss: 151.49202880859374 - tot_loss: 151.4921664100373 - loss_class: 19386.16015625 - acc: 0.8989666666666667 - val_acc: 0.9007\n",
            "Epoch 8 / 8 \n",
            " - time: 1.6115531921386719 - sq_loss: 99.59530639648438 - tot_loss: 99.59549788931035 - loss_class: 19101.7265625 - acc: 0.8995 - val_acc: 0.901\n",
            "The total time spent is: 12.887811660766602 s\n",
            "\n",
            "\n",
            "\n",
            "Early stopping accuracy: 0.901\n",
            "Epoch 0 | Test accuracy: 0.85485\n",
            "Epoch 1 | Test accuracy: 0.87583\n",
            "Epoch 2 | Test accuracy: 0.88570\n",
            "Epoch 3 | Test accuracy: 0.89019\n"
          ]
        }
      ],
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "accuracy_train = []\n",
        "accuracy_test = []\n",
        "epochs_times = []\n",
        "if(optimizer_name == 'Coordinate-Descent' or optimizer_name == 'Coordinate-Descent+SGD' or optimizer_name== 'Coordinate-Descent+Adam'):\n",
        "  print('training BCD')\n",
        "  if(optimizer_name != 'Coordinate-Descent'):\n",
        "    total_epochs = epochs\n",
        "    epochs = int(total_epochs * ratio)\n",
        "  train_losses, test_losses , accuracy_train, accuracy_test,epochs_times,Ws,bs = execute_training([[\"Perceptron\",hidden_size,1],[\"Perceptron\",hidden_size,1]], input_size, hidden_size, output_size, x_train, x_test, y_train, y_test, y_train_one_hot, y_test_one_hot,\n",
        "                                         False,I1 = hidden_size,I2=1, niter = epochs, gamma = 0.1, alpha = 4)\n",
        "  #Train using BCD\n",
        "  if(optimizer_name != 'Coordinate-Descent'):\n",
        "    epochs = total_epochs-epochs\n",
        "if(optimizer_name != 'Coordinate-Descent'):\n",
        "  model = model.to(device)\n",
        "  #train using sgd or adam\n",
        "  if(optimizer_name == 'Coordinate-Descent+SGD' or optimizer_name == 'Coordinate-Descent+Adam'):\n",
        "    i=0\n",
        "    for param in model.parameters():\n",
        "      if i%2 == 0:\n",
        "        param.data = Ws[int(i/2)]\n",
        "        #temp_W.pop()\n",
        "      else:\n",
        "        param.data = torch.flatten(bs[int(i/2)])\n",
        "        #temp_b.pop()\n",
        "      i+=1\n",
        "  train_loss, test_loss, acc_train, acc_test, times = train_model(model, dataset_train, dataset_test, optimizer, cross_entropy, epochs,scheduler,optimizer_name)\n",
        "  train_losses = list(train_losses) + train_loss\n",
        "  test_losses = list(test_losses) + test_loss\n",
        "  accuracy_train = list(accuracy_train) + acc_train\n",
        "  accuracy_test = list(accuracy_test) + acc_test\n",
        "  epochs_times = list(epochs_times) + times\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op31_mG1kcON"
      },
      "outputs": [],
      "source": [
        "#Replace this with the same function as DFW\n",
        "def pickle_results(name,train_loss,loss_class,accuracy_train,accuracy_val,weights,biases):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  dictionary_save = {\"Weights\": weights,\"Biases\":biases, \"train_loss\": train_loss, \"loss_class\":loss_class,\"accuracy_train\":accuracy_train,\"accuracy_test\":accuracy_val}\n",
        "\n",
        "  results_name = \"results_\"+name\n",
        "  a_file = open(results_name,\"wb\")\n",
        "  pickle.dump(dictionary_save,a_file)\n",
        "  a_file.close()\n",
        "  return"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "baseline_SGD.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}