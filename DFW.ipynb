{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "puJO6BZlNmuf",
    "outputId": "d4db712c-4e8d-462d-864b-a49b2f02b4d8"
   },
   "outputs": [],
   "source": [
    "#@title Mount Drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/Machine-Learning-Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mC7BWlWnoFBa",
    "outputId": "2679231c-c61d-4b83-f908-1a09e8be2ee3"
   },
   "outputs": [],
   "source": [
    "!pip install barbar\n",
    "\n",
    "#@title Import and utilities \n",
    "\n",
    "from Frank_Wolfe.utils.utils import *\n",
    "from Frank_Wolfe.DFW import *\n",
    "from Frank_Wolfe.architectures import *\n",
    "from Frank_Wolfe.MultiClassHingeLoss import *\n",
    "from barbar import Bar\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jf6SJcqjoUSA",
    "outputId": "5e6e9eb7-6603-455a-d747-e7b2b6fc43cd"
   },
   "outputs": [],
   "source": [
    "#@title Choose dataset name and architecture of the network \n",
    "\n",
    "# Select the dataset and the architecture\n",
    "\n",
    "dataset_name = 'CIFAR10' #@param ['CIFAR10', 'CIFAR100']\n",
    "model_type = 'WideResNet' #@param ['DenseNet', 'WideResNet', 'GoogLeNet']\n",
    "\n",
    "# load the model\n",
    "if model_type == 'GoogLeNet':\n",
    "    model = GoogleNet(num_class=10 if dataset_name == \"CIFAR10\" else 100)\n",
    "elif model_type == 'DenseNet':\n",
    "    model = torchvision.models.densenet121(pretrained=False)\n",
    "elif model_type == 'WideResNet':\n",
    "    model =  WideResNet(num_classes=10 if dataset_name == \"CIFAR10\" else 100)\n",
    "else:\n",
    "    raise ValueError(\"Please, select an available architecture\")\n",
    "\n",
    "# setting dataset attributes, dictionary useful to normalize the images\n",
    "datasetDict = setDatasetAttributes(dataset_name)\n",
    "# transform operation\n",
    "trainTransformDict, testTransformDict = setTrainAndTest(dataset_name) \n",
    "root = f\"{dataset_name}-dataset\"\n",
    "\n",
    "# prepare train and test datasets \n",
    "trainData = datasetDict['datasetDict'](root=root, train=True, download=True,\n",
    "                                            transform=trainTransformDict[dataset_name])\n",
    "testData = datasetDict['datasetDict'](root=root, train=False,\n",
    "                                        transform=testTransformDict[dataset_name])\n",
    "# move the model to GPU\n",
    "model = model.to(device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "C6IgJdqGoWkV"
   },
   "outputs": [],
   "source": [
    "#@title Choose optimizer and parameters \n",
    "\n",
    "# Choice of the optimizer and the parameters.\n",
    "\n",
    "# The parameters used in our experiments can be found\n",
    "# both in the report and at the end of the notebook.\n",
    "\n",
    "optimizer_name = \"DFW multistep\" #@param  ['DFW', 'Adam', 'SGD with scheduler', 'DFW multistep']\n",
    "eta =   0.1 #@param {type:\"number\"}\n",
    "momentum = 0.9 #@param {type:\"number\"}\n",
    "lr = 0.001 #@param {type:\"number\"}\n",
    "beta_1 = 0.9 #@param {type:\"number\"}\n",
    "beta_2 = 0.999 #@param {type:\"number\"}\n",
    "initial_prox_steps = 2 #@param {type: \"number\"}\n",
    "\n",
    "if optimizer_name != \"DFW multistep\":\n",
    "    initial_prox_steps = 1\n",
    "\n",
    "# define the optimizer\n",
    "\n",
    "if optimizer_name == \"DFW\" or optimizer_name == 'DFW multistep':\n",
    "    optimizer = DFW(params=model.parameters(), eta=eta, momentum=momentum,\n",
    "                    prox_steps=initial_prox_steps)\n",
    "    \n",
    "    assert initial_prox_steps > 0\n",
    "    assert eta > 0\n",
    "    assert 0 <= momentum <= 1\n",
    "elif optimizer_name == \"SGD with scheduler\":\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(), lr=lr,\n",
    "                              momentum=momentum)\n",
    "    scheduler = StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "    assert 0 <= momentum <= 1\n",
    "elif optimizer_name == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, \n",
    "                               betas=(beta_1, beta_2))\n",
    "    \n",
    "if optimizer_name == \"DFW\" or optimizer_name == \"DFW multistep\":\n",
    "    # we consider a convex and piece-wise linear objective for DFW\n",
    "    loss_criterion = MultiClassHingeLoss().to(device=\"cuda:0\")\n",
    "    # smoothening of the loss function for many classes\n",
    "    smoothing = True\n",
    "else: \n",
    "    # cross entropy otherwise\n",
    "    loss_criterion = nn.CrossEntropyLoss().to(device=\"cuda:0\")\n",
    "    smoothing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Cq-yV7BYplnM",
    "outputId": "5b507142-acc3-4124-c9bc-4fbf2ae077e6"
   },
   "outputs": [],
   "source": [
    "#@title Train the network  \n",
    "\n",
    "# parameters for the training phase\n",
    "nepochs = 50 #@param {type:\"integer\"}\n",
    "batch_size = 128  #@param {type:\"integer\"}\n",
    "\n",
    "# Loaders\n",
    "trainLoader = torch.utils.data.DataLoader(trainData, batch_size=batch_size, shuffle=True,\n",
    "                                      pin_memory=torch.cuda.is_available(), num_workers=2)\n",
    "testLoader = torch.utils.data.DataLoader(testData, batch_size=batch_size, shuffle=False,\n",
    "                                      pin_memory=torch.cuda.is_available(), num_workers=2)\n",
    "\n",
    "# initialize necessary metrics objects\n",
    "train_loss, train_accuracy = AverageMeter(), AverageMeter()\n",
    "test_loss, test_accuracy = AverageMeter(), AverageMeter()\n",
    "\n",
    "# function to reset metrics\n",
    "def reset_metrics():\n",
    "    train_loss.reset()\n",
    "    train_accuracy.reset()\n",
    "    test_loss.reset()\n",
    "    test_accuracy.reset()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(data=\"train\"):\n",
    "    if data == \"train\":\n",
    "        loader = trainLoader\n",
    "        mean_loss, mean_accuracy = train_loss, train_accuracy\n",
    "    elif data == \"test\":\n",
    "        loader = testLoader\n",
    "        mean_loss, mean_accuracy = test_loss, test_accuracy\n",
    "    \n",
    "    sys.stdout.write(f\"Evaluation of {data} data:\\n\")\n",
    "    \n",
    "    # iteration over the dataset\n",
    "    for x_input, y_target in Bar(loader):\n",
    "        x_input, y_target = x_input.to(device=\"cuda:0\"), y_target.to(device=\"cuda:0\") # we move to GPU\n",
    "        output = model.eval()(x_input)\n",
    "        loss = loss_criterion(output, y_target)\n",
    "        \n",
    "        # update metrics\n",
    "        mean_loss(loss.item(), len(y_target)) \n",
    "        mean_accuracy(Utilities.categorical_accuracy(y_true=y_target, output=output), len(y_target))\n",
    "\n",
    "    \n",
    "# Training\n",
    "for epoch in range(nepochs + 1):\n",
    "    \n",
    "    start = time.time() # start to time\n",
    "    reset_metrics() # reset the metrics from the previous epoch\n",
    "    sys.stdout.write(f\"\\n\\nEpoch {epoch}/{nepochs}\\n\")\n",
    "    \n",
    "    if epoch == 0:\n",
    "        # Evaluate the model once to get the metrics\n",
    "        evaluate_model(data='train')\n",
    "    else:\n",
    "        if epoch > int(0.2 * nepochs) and optimizer_name == \"DFW multistep\" and initial_prox_steps > 1:\n",
    "            \n",
    "            # continue with single steps\n",
    "            optimizer.prox_steps = 1\n",
    "            \n",
    "        sys.stdout.write(f\"Training:\\n\")\n",
    "        for x_input, y_target in Bar(trainLoader):\n",
    "            x_input, y_target = x_input.to(device=\"cuda:0\"), y_target.to(device=\"cuda:0\")\n",
    "            optimizer.zero_grad()  # Zero the gradient buffers\n",
    "            output = model.train()(x_input) # compute the output\n",
    "            if dataset_name == \"CIFAR100\" and smoothing:\n",
    "              # smoothing of the loss for DFW\n",
    "              with set_smoothing_enabled(True):\n",
    "                loss = loss_criterion(output, y_target) # compute the loss\n",
    "            else:\n",
    "                loss = loss_criterion(output, y_target) # without smoothing\n",
    "            loss.backward()  # Backpropagation\n",
    "            if optimizer_name == \"DFW\" or optimizer_name == 'DFW multistep':\n",
    "                optimizer.step(lambda: float(loss))\n",
    "            else:\n",
    "                optimizer.step() \n",
    "            train_loss(loss.item(), len(y_target))\n",
    "            train_accuracy(Utilities.categorical_accuracy(y_true=y_target, output=output), len(y_target))\n",
    "\n",
    "    if optimizer_name == \"SGD with scheduler\":\n",
    "        scheduler.step()\n",
    "\n",
    "    # evalutate the model on the test set    \n",
    "    evaluate_model(data='test')\n",
    "    sys.stdout.write(f\"\\n Finished epoch {epoch}/{nepochs}: Train Loss {train_loss.result()} | Test Loss {test_loss.result()} | Train Acc {train_accuracy.result()} | Test Acc {test_accuracy.result()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldLaA2by6VYV"
   },
   "source": [
    "# Parameters used in the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7F6Ai3G6bFd"
   },
   "source": [
    "In order to reproduce our results (i.e. the training trends shown in the report), the following set of parameters should be used.\\\n",
    "If not specified otherwise, the remaining parameters (e.g. the stability perturbation $\\epsilon$ for Adam and Adagrad) are set to their default values.\n",
    "\n",
    "\n",
    "$\\text{Deep Frank Wolfe (single step and multistep)}$:\n",
    "```python\n",
    "eta = 0.1  # proximal coefficient\n",
    "momentum = 0.9  # momentum parameter\n",
    "optimizer = DFW(model.parameters(), eta=eta, \n",
    "            momentum=momentum, prox_steps=2) # or prox_steps=1\n",
    "```\n",
    "\n",
    "$\\textbf{Stochastic Gradient Descent with scheduler}$:\n",
    "```python\n",
    "lr = 0.1  # learning rate\n",
    "momentum = 0.9  # momentum parameter\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=lr,\n",
    "                              momentum=momentum)\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.5)  # define scheduler\n",
    "```\n",
    "\n",
    "$\\textbf{Adam}$:\n",
    "```python\n",
    "lr = 0.001 # learning rate\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, \n",
    "                               betas=(beta_1, beta_2))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "DFW_on_github (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
