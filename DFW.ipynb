{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Machine-Learning-Optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puJO6BZlNmuf",
        "outputId": "d4db712c-4e8d-462d-864b-a49b2f02b4d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Machine-Learning-Optimization_working\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC7BWlWnoFBa",
        "outputId": "2679231c-c61d-4b83-f908-1a09e8be2ee3",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting barbar\n",
            "  Downloading barbar-0.2.1-py3-none-any.whl (3.9 kB)\n",
            "Installing collected packages: barbar\n",
            "Successfully installed barbar-0.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install barbar\n",
        "\n",
        "#@title Import and utilities \n",
        "\n",
        "from Frank_Wolfe.utils.utils import *\n",
        "from Frank_Wolfe.DFW import *\n",
        "from Frank_Wolfe.architectures import *\n",
        "from Frank_Wolfe.MultiClassHingeLoss import *\n",
        "from barbar import Bar\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import sys\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "device = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf6SJcqjoUSA",
        "outputId": "5e6e9eb7-6603-455a-d747-e7b2b6fc43cd",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "#@title Choose dataset name and architecture of the network \n",
        "\n",
        "# Select the dataset and the architecture\n",
        "\n",
        "dataset_name = 'CIFAR10' #@param ['CIFAR10', 'CIFAR100']\n",
        "model_type = 'WideResNet' #@param ['DenseNet', 'WideResNet', 'GoogLeNet']\n",
        "\n",
        "# load the model\n",
        "if model_type == 'GoogLeNet':\n",
        "    model = GoogleNet(num_class=10 if dataset_name == \"CIFAR10\" else 100)\n",
        "elif model_type == 'DenseNet':\n",
        "    model = torchvision.models.densenet121(pretrained=False)\n",
        "elif model_type == 'WideResNet':\n",
        "    model =  WideResNet(num_classes=10 if dataset_name == \"CIFAR10\" else 100)\n",
        "else:\n",
        "    raise ValueError(\"Please, select an available architecture\")\n",
        "\n",
        "# setting dataset attributes, dictionary useful to normalize the images\n",
        "datasetDict = setDatasetAttributes(dataset_name)\n",
        "# transform operation\n",
        "trainTransformDict, testTransformDict = setTrainAndTest(dataset_name) \n",
        "root = f\"{dataset_name}-dataset\"\n",
        "\n",
        "# prepare train and test datasets \n",
        "trainData = datasetDict['datasetDict'](root=root, train=True, download=True,\n",
        "                                            transform=trainTransformDict[dataset_name])\n",
        "testData = datasetDict['datasetDict'](root=root, train=False,\n",
        "                                        transform=testTransformDict[dataset_name])\n",
        "# move the model to GPU\n",
        "model = model.to(device=\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6IgJdqGoWkV",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Choose optimizer and parameters \n",
        "\n",
        "# Choice of the optimizer and the parameters.\n",
        "\n",
        "# The parameters used in our experiments can be found\n",
        "# both in the report and at the end of the notebook.\n",
        "\n",
        "optimizer_name = \"DFW multistep\" #@param  ['DFW', 'Adam', 'SGD with scheduler', 'DFW multistep']\n",
        "eta =   0.1 #@param {type:\"number\"}\n",
        "momentum = 0.9 #@param {type:\"number\"}\n",
        "lr = 0.001 #@param {type:\"number\"}\n",
        "beta_1 = 0.9 #@param {type:\"number\"}\n",
        "beta_2 = 0.999 #@param {type:\"number\"}\n",
        "initial_prox_steps = 2 #@param {type: \"number\"}\n",
        "\n",
        "if optimizer_name != \"DFW multistep\":\n",
        "    initial_prox_steps = 1\n",
        "\n",
        "# define the optimizer\n",
        "\n",
        "if optimizer_name == \"DFW\" or optimizer_name == 'DFW multistep':\n",
        "    optimizer = DFW(params=model.parameters(), eta=eta, momentum=momentum,\n",
        "                    prox_steps=initial_prox_steps)\n",
        "    \n",
        "    assert initial_prox_steps > 0\n",
        "    assert eta > 0\n",
        "    assert 0 <= momentum <= 1\n",
        "elif optimizer_name == \"SGD with scheduler\":\n",
        "    optimizer = torch.optim.SGD(params=model.parameters(), lr=lr,\n",
        "                              momentum=momentum)\n",
        "    scheduler = StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "    assert 0 <= momentum <= 1\n",
        "elif optimizer_name == \"Adam\":\n",
        "    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, \n",
        "                               betas=(beta_1, beta_2))\n",
        "    \n",
        "if optimizer_name == \"DFW\" or optimizer_name == \"DFW multistep\":\n",
        "    # we consider a convex and piece-wise linear objective for DFW\n",
        "    loss_criterion = MultiClassHingeLoss().to(device=\"cuda:0\")\n",
        "    # smoothening of the loss function for many classes\n",
        "    smoothing = True\n",
        "else: \n",
        "    # cross entropy otherwise\n",
        "    loss_criterion = nn.CrossEntropyLoss().to(device=\"cuda:0\")\n",
        "    smoothing = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Cq-yV7BYplnM",
        "outputId": "5b507142-acc3-4124-c9bc-4fbf2ae077e6",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 0/50\n",
            "Evaluation of train data:\n",
            "50000/50000: [===============================>] - ETA 0.7s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.3s\n",
            "\n",
            " Finished epoch 0/50: Train Loss 1.1074069875717163 | Test Loss 1.097416378211975 | Train Acc 0.09998 | Test Acc 0.1002\n",
            "Time elapsed for the current epoch 116.83940601348877\n",
            "\n",
            "Epoch 1/50\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.9s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.3s\n",
            "\n",
            " Finished epoch 1/50: Train Loss 1.2463369568252562 | Test Loss 1.0503244495391846 | Train Acc 0.13936 | Test Acc 0.1632\n",
            "Time elapsed for the current epoch 331.5416555404663\n",
            "\n",
            "Epoch 2/50\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.9s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.3s\n",
            "\n",
            " Finished epoch 2/50: Train Loss 1.0777584143829346 | Test Loss 1.0660836534500122 | Train Acc 0.16654 | Test Acc 0.226\n",
            "Time elapsed for the current epoch 331.6231348514557\n",
            "\n",
            "Epoch 3/50\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.9s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.3s\n",
            "\n",
            " Finished epoch 3/50: Train Loss 1.0655776642608643 | Test Loss 1.043072096824646 | Train Acc 0.18616 | Test Acc 0.2315\n",
            "Time elapsed for the current epoch 331.66676235198975\n",
            "\n",
            "Epoch 4/50\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.9s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.3s\n",
            "\n",
            " Finished epoch 4/50: Train Loss 1.0524564706802368 | Test Loss 1.0486506763458252 | Train Acc 0.20848 | Test Acc 0.2673\n",
            "Time elapsed for the current epoch 331.6608805656433\n",
            "\n",
            "Epoch 5/50\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.9s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.3s\n",
            "\n",
            " Finished epoch 5/50: Train Loss 1.0378539235305786 | Test Loss 1.0379442756652832 | Train Acc 0.24008 | Test Acc 0.1802\n",
            "Time elapsed for the current epoch 331.596143245697\n",
            "\n",
            "Epoch 6/50\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.9s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.3s\n",
            "\n",
            " Finished epoch 6/50: Train Loss 1.0327531456756591 | Test Loss 1.0406832153320313 | Train Acc 0.25826 | Test Acc 0.2996\n",
            "Time elapsed for the current epoch 331.6459639072418\n",
            "\n",
            "Epoch 7/50\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.9s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.3s\n",
            "\n",
            " Finished epoch 7/50: Train Loss 1.0232938889694214 | Test Loss 1.020273078918457 | Train Acc 0.28408 | Test Acc 0.316\n",
            "Time elapsed for the current epoch 331.6892635822296\n",
            "\n",
            "Epoch 8/50\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.9s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.3s\n",
            "\n",
            " Finished epoch 8/50: Train Loss 1.0084881286239624 | Test Loss 1.066634535598755 | Train Acc 0.31362 | Test Acc 0.3056\n",
            "Time elapsed for the current epoch 332.0305120944977\n",
            "\n",
            "Epoch 9/50\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.9s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.3s\n",
            "\n",
            " Finished epoch 9/50: Train Loss 0.9862664909744263 | Test Loss 1.1467449785232544 | Train Acc 0.34588 | Test Acc 0.2954\n",
            "Time elapsed for the current epoch 335.5701081752777\n",
            "\n",
            "Epoch 10/50\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.9s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.3s\n",
            "\n",
            " Finished epoch 10/50: Train Loss 0.9610643991279602 | Test Loss 1.0010498298645019 | Train Acc 0.38212 | Test Acc 0.3913\n",
            "Time elapsed for the current epoch 332.8381645679474\n",
            "\n",
            "Epoch 11/50\n",
            "Training:\n",
            " 1920/50000: [=>..............................] - ETA 320.2s"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-648f67b83860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moptimizer_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"DFW\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0moptimizer_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DFW multistep'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title Train the network  \n",
        "\n",
        "# we will append our results on these lists\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "epochs_times = []\n",
        "\n",
        "# parameters for the training phase\n",
        "nepochs = 50 #@param {type:\"integer\"}\n",
        "batch_size = 128  #@param {type:\"integer\"}\n",
        "verbose = 0 #@param [0, 1]\n",
        "\n",
        "# Loaders\n",
        "trainLoader = torch.utils.data.DataLoader(trainData, batch_size=batch_size, shuffle=True,\n",
        "                                      pin_memory=torch.cuda.is_available(), num_workers=2)\n",
        "testLoader = torch.utils.data.DataLoader(testData, batch_size=batch_size, shuffle=False,\n",
        "                                      pin_memory=torch.cuda.is_available(), num_workers=2)\n",
        "\n",
        "# initialize necessary metrics objects\n",
        "train_loss, train_accuracy = AverageMeter(), AverageMeter()\n",
        "test_loss, test_accuracy = AverageMeter(), AverageMeter()\n",
        "\n",
        "# function to reset metrics\n",
        "def reset_metrics():\n",
        "    train_loss.reset()\n",
        "    train_accuracy.reset()\n",
        "    test_loss.reset()\n",
        "    test_accuracy.reset()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(data=\"train\"):\n",
        "    if data == \"train\":\n",
        "        loader = trainLoader\n",
        "        mean_loss, mean_accuracy = train_loss, train_accuracy\n",
        "    elif data == \"test\":\n",
        "        loader = testLoader\n",
        "        mean_loss, mean_accuracy = test_loss, test_accuracy\n",
        "    \n",
        "    sys.stdout.write(f\"Evaluation of {data} data:\\n\")\n",
        "    \n",
        "    # iteration over the dataset\n",
        "    for x_input, y_target in Bar(loader):\n",
        "        x_input, y_target = x_input.to(device=\"cuda:0\"), y_target.to(device=\"cuda:0\") # we move to GPU\n",
        "        output = model.eval()(x_input)\n",
        "        loss = loss_criterion(output, y_target)\n",
        "        \n",
        "        # update metrics\n",
        "        mean_loss(loss.item(), len(y_target)) \n",
        "        mean_accuracy(Utilities.categorical_accuracy(y_true=y_target, output=output), len(y_target))\n",
        "\n",
        "    \n",
        "# Training\n",
        "for epoch in range(nepochs + 1):\n",
        "    \n",
        "    start = time.time() # start to time\n",
        "    reset_metrics() # reset the metrics from the previous epoch\n",
        "    sys.stdout.write(f\"\\n\\nEpoch {epoch}/{nepochs}\\n\")\n",
        "    \n",
        "    if epoch == 0:\n",
        "        # First pass through the network to evaluate the model once to get the metrics\n",
        "        evaluate_model(data='train')\n",
        "    else:\n",
        "        if epoch > int(0.2 * nepochs) and optimizer_name == \"DFW multistep\" and asymptotic_prox_steps_num >1:\n",
        "            \n",
        "            # if we already finished the first 20% of the epochs we continue with single steps\n",
        "            optimizer.prox_steps = 1\n",
        "            \n",
        "        sys.stdout.write(f\"Training:\\n\")\n",
        "        for x_input, y_target in Bar(trainLoader):\n",
        "            x_input, y_target = x_input.to(device=\"cuda:0\"), y_target.to(device=\"cuda:0\")\n",
        "            optimizer.zero_grad()  # Zero the gradient buffers\n",
        "            output = model.train()(x_input) # compute the output\n",
        "            if dataset_name == \"CIFAR100\" and smoothing:\n",
        "              # smoothing of the loss for DFW\n",
        "              with set_smoothing_enabled(True):\n",
        "                loss = loss_criterion(output, y_target) # compute the loss\n",
        "            loss.backward()  # Backpropagation\n",
        "            if optimizer_name == \"DFW\" or optimizer_name == 'DFW multistep':\n",
        "                optimizer.step(lambda: float(loss))\n",
        "            else:\n",
        "                optimizer.step() \n",
        "            train_loss(loss.item(), len(y_target))\n",
        "            train_accuracy(Utilities.categorical_accuracy(y_true=y_target, output=output), len(y_target))\n",
        "\n",
        "    if optimizer_name == \"SGD with scheduler\":\n",
        "        scheduler.step()\n",
        "\n",
        "    # evalutate the model on the test set    \n",
        "    evaluate_model(data='test')\n",
        "    sys.stdout.write(f\"\\n Finished epoch {epoch}/{nepochs}: Train Loss {train_loss.result()} | Test Loss {test_loss.result()} | Train Acc {train_accuracy.result()} | Test Acc {test_accuracy.result()}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldLaA2by6VYV"
      },
      "source": [
        "# Parameters used in the report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7F6Ai3G6bFd"
      },
      "source": [
        "In order to reproduce our results (i.e. the training trends shown in the report), the following set of parameters should be used.\\\n",
        "If not specified otherwise, the remaining parameters (e.g. the stability perturbation $\\epsilon$ for Adam and Adagrad) are set to their default values.\n",
        "\n",
        "\n",
        "$\\text{Deep Frank Wolfe (single step and multistep)}$:\n",
        "```python\n",
        "eta = 0.1  # proximal coefficient\n",
        "momentum = 0.9  # momentum parameter\n",
        "optimizer = DFW(model.parameters(), eta=eta, \n",
        "            momentum=momentum, prox_steps=2) # or prox_steps=1\n",
        "```\n",
        "\n",
        "$\\textbf{Stochastic Gradient Descent with scheduler}$:\n",
        "```python\n",
        "lr = 0.1  # learning rate\n",
        "momentum = 0.9  # momentum parameter\n",
        "optimizer = torch.optim.SGD(params=model.parameters(), lr=lr,\n",
        "                              momentum=momentum)\n",
        "scheduler = StepLR(optimizer, step_size=20, gamma=0.5)  # define scheduler\n",
        "```\n",
        "\n",
        "$\\textbf{Adam}$:\n",
        "```python\n",
        "lr = 0.001 # learning rate\n",
        "beta_1 = 0.9\n",
        "beta_2 = 0.999\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, \n",
        "                               betas=(beta_1, beta_2))\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DFW_on_github (1).ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}