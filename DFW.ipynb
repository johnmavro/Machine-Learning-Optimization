{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "puJO6BZlNmuf",
    "outputId": "d4db712c-4e8d-462d-864b-a49b2f02b4d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/MyDrive/Machine-Learning-Optimization_working\n"
     ]
    }
   ],
   "source": [
    "#@title Mount Drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/Machine-Learning-Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mC7BWlWnoFBa",
    "outputId": "2679231c-c61d-4b83-f908-1a09e8be2ee3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: barbar in c:\\users\\federico betti\\anaconda3\\envs\\cs439\\lib\\site-packages (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install barbar\n",
    "\n",
    "#@title Import and utilities \n",
    "\n",
    "from Frank_Wolfe.utils.utils import *\n",
    "from Frank_Wolfe.DFW import *\n",
    "from Frank_Wolfe.architectures import *\n",
    "from Frank_Wolfe.MultiClassHingeLoss import *\n",
    "from barbar import Bar\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jf6SJcqjoUSA",
    "outputId": "5e6e9eb7-6603-455a-d747-e7b2b6fc43cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to CIFAR10-dataset\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07f3e8913864957a81405e2b1700589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# prepare train and test datasets \u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m trainData \u001b[38;5;241m=\u001b[39m \u001b[43mdatasetDict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatasetDict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainTransformDict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m testData \u001b[38;5;241m=\u001b[39m datasetDict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasetDict\u001b[39m\u001b[38;5;124m'\u001b[39m](root\u001b[38;5;241m=\u001b[39mroot, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     28\u001b[0m                                         transform\u001b[38;5;241m=\u001b[39mtestTransformDict[dataset_name])\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# move the model to GPU\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torchvision\\datasets\\cifar.py:65\u001b[0m, in \u001b[0;36mCIFAR10.__init__\u001b[1;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m=\u001b[39m train  \u001b[38;5;66;03m# training set or test set\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torchvision\\datasets\\cifar.py:141\u001b[0m, in \u001b[0;36mCIFAR10.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles already downloaded and verified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtgz_md5\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torchvision\\datasets\\utils.py:430\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[1;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[0;32m    428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[1;32m--> 430\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torchvision\\datasets\\utils.py:141\u001b[0m, in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fpath)\n\u001b[1;32m--> 141\u001b[0m     \u001b[43m_urlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m url[:\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torchvision\\datasets\\utils.py:35\u001b[0m, in \u001b[0;36m_urlretrieve\u001b[1;34m(url, filename, chunk_size)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mlength) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m---> 35\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m: response\u001b[38;5;241m.\u001b[39mread(chunk_size), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     36\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunk:\n\u001b[0;32m     37\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torchvision\\datasets\\utils.py:35\u001b[0m, in \u001b[0;36m_urlretrieve.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mlength) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m---> 35\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     36\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunk:\n\u001b[0;32m     37\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\http\\client.py:462\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    461\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 462\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\http\\client.py:506\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    501\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[0;32m    503\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 506\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@title Choose dataset name and architecture of the network \n",
    "\n",
    "# Select the dataset and the architecture\n",
    "\n",
    "dataset_name = 'CIFAR10' #@param ['CIFAR10', 'CIFAR100']\n",
    "model_type = 'WideResNet' #@param ['DenseNet', 'WideResNet', 'GoogLeNet']\n",
    "\n",
    "# load the model\n",
    "if model_type == 'GoogLeNet':\n",
    "    model = GoogleNet(num_class=10 if dataset_name == \"CIFAR10\" else 100)\n",
    "elif model_type == 'DenseNet':\n",
    "    model = torchvision.models.densenet121(pretrained=False)\n",
    "elif model_type == 'WideResNet':\n",
    "    model =  WideResNet(num_classes=10 if dataset_name == \"CIFAR10\" else 100)\n",
    "else:\n",
    "    raise ValueError(\"Please, select an available architecture\")\n",
    "\n",
    "# setting dataset attributes, dictionary useful to normalize the images\n",
    "datasetDict = setDatasetAttributes(dataset_name)\n",
    "# transform operation\n",
    "trainTransformDict, testTransformDict = setTrainAndTest(dataset_name) \n",
    "root = f\"{dataset_name}-dataset\"\n",
    "\n",
    "# prepare train and test datasets \n",
    "trainData = datasetDict['datasetDict'](root=root, train=True, download=True,\n",
    "                                            transform=trainTransformDict[dataset_name])\n",
    "testData = datasetDict['datasetDict'](root=root, train=False,\n",
    "                                        transform=testTransformDict[dataset_name])\n",
    "# move the model to GPU\n",
    "model = model.to(device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "C6IgJdqGoWkV"
   },
   "outputs": [],
   "source": [
    "#@title Choose optimizer and parameters \n",
    "\n",
    "# Choice of the optimizer and the parameters.\n",
    "\n",
    "# The parameters used in our experiments can be found\n",
    "# both in the report and at the end of the notebook.\n",
    "\n",
    "optimizer_name = \"DFW multistep\" #@param  ['DFW', 'Adam', 'SGD with scheduler', 'DFW multistep']\n",
    "eta =   0.1 #@param {type:\"number\"}\n",
    "momentum = 0.9 #@param {type:\"number\"}\n",
    "lr = 0.001 #@param {type:\"number\"}\n",
    "beta_1 = 0.9 #@param {type:\"number\"}\n",
    "beta_2 = 0.999 #@param {type:\"number\"}\n",
    "initial_prox_steps = 2 #@param {type: \"number\"}\n",
    "\n",
    "if optimizer_name != \"DFW multistep\":\n",
    "    initial_prox_steps = 1\n",
    "\n",
    "# define the optimizer\n",
    "\n",
    "if optimizer_name == \"DFW\" or optimizer_name == 'DFW multistep':\n",
    "    optimizer = DFW(params=model.parameters(), eta=eta, momentum=momentum,\n",
    "                    prox_steps=initial_prox_steps)\n",
    "    \n",
    "    assert initial_prox_steps > 0\n",
    "    assert eta > 0\n",
    "    assert 0 <= momentum <= 1\n",
    "elif optimizer_name == \"SGD with scheduler\":\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(), lr=lr,\n",
    "                              momentum=momentum)\n",
    "    scheduler = StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "    assert 0 <= momentum <= 1\n",
    "elif optimizer_name == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, \n",
    "                               betas=(beta_1, beta_2))\n",
    "    \n",
    "if optimizer_name == \"DFW\" or optimizer_name == \"DFW multistep\":\n",
    "    # we consider a convex and piece-wise linear objective for DFW\n",
    "    loss_criterion = MultiClassHingeLoss().to(device=\"cuda:0\")\n",
    "    # smoothening of the loss function for many classes\n",
    "    smoothing = True\n",
    "else: \n",
    "    # cross entropy otherwise\n",
    "    loss_criterion = nn.CrossEntropyLoss().to(device=\"cuda:0\")\n",
    "    smoothing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Cq-yV7BYplnM",
    "outputId": "5b507142-acc3-4124-c9bc-4fbf2ae077e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 0/50\n",
      "Evaluation of train data:\n",
      "50000/50000: [===============================>] - ETA 0.7s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.3s\n",
      "\n",
      " Finished epoch 0/50: Train Loss 1.1074069875717163 | Test Loss 1.097416378211975 | Train Acc 0.09998 | Test Acc 0.1002\n",
      "Time elapsed for the current epoch 116.83940601348877\n",
      "\n",
      "Epoch 1/50\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.9s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.3s\n",
      "\n",
      " Finished epoch 1/50: Train Loss 1.2463369568252562 | Test Loss 1.0503244495391846 | Train Acc 0.13936 | Test Acc 0.1632\n",
      "Time elapsed for the current epoch 331.5416555404663\n",
      "\n",
      "Epoch 2/50\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.9s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.3s\n",
      "\n",
      " Finished epoch 2/50: Train Loss 1.0777584143829346 | Test Loss 1.0660836534500122 | Train Acc 0.16654 | Test Acc 0.226\n",
      "Time elapsed for the current epoch 331.6231348514557\n",
      "\n",
      "Epoch 3/50\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.9s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.3s\n",
      "\n",
      " Finished epoch 3/50: Train Loss 1.0655776642608643 | Test Loss 1.043072096824646 | Train Acc 0.18616 | Test Acc 0.2315\n",
      "Time elapsed for the current epoch 331.66676235198975\n",
      "\n",
      "Epoch 4/50\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.9s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.3s\n",
      "\n",
      " Finished epoch 4/50: Train Loss 1.0524564706802368 | Test Loss 1.0486506763458252 | Train Acc 0.20848 | Test Acc 0.2673\n",
      "Time elapsed for the current epoch 331.6608805656433\n",
      "\n",
      "Epoch 5/50\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.9s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.3s\n",
      "\n",
      " Finished epoch 5/50: Train Loss 1.0378539235305786 | Test Loss 1.0379442756652832 | Train Acc 0.24008 | Test Acc 0.1802\n",
      "Time elapsed for the current epoch 331.596143245697\n",
      "\n",
      "Epoch 6/50\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.9s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.3s\n",
      "\n",
      " Finished epoch 6/50: Train Loss 1.0327531456756591 | Test Loss 1.0406832153320313 | Train Acc 0.25826 | Test Acc 0.2996\n",
      "Time elapsed for the current epoch 331.6459639072418\n",
      "\n",
      "Epoch 7/50\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.9s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.3s\n",
      "\n",
      " Finished epoch 7/50: Train Loss 1.0232938889694214 | Test Loss 1.020273078918457 | Train Acc 0.28408 | Test Acc 0.316\n",
      "Time elapsed for the current epoch 331.6892635822296\n",
      "\n",
      "Epoch 8/50\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.9s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.3s\n",
      "\n",
      " Finished epoch 8/50: Train Loss 1.0084881286239624 | Test Loss 1.066634535598755 | Train Acc 0.31362 | Test Acc 0.3056\n",
      "Time elapsed for the current epoch 332.0305120944977\n",
      "\n",
      "Epoch 9/50\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.9s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.3s\n",
      "\n",
      " Finished epoch 9/50: Train Loss 0.9862664909744263 | Test Loss 1.1467449785232544 | Train Acc 0.34588 | Test Acc 0.2954\n",
      "Time elapsed for the current epoch 335.5701081752777\n",
      "\n",
      "Epoch 10/50\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.9s\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.3s\n",
      "\n",
      " Finished epoch 10/50: Train Loss 0.9610643991279602 | Test Loss 1.0010498298645019 | Train Acc 0.38212 | Test Acc 0.3913\n",
      "Time elapsed for the current epoch 332.8381645679474\n",
      "\n",
      "Epoch 11/50\n",
      "Training:\n",
      " 1920/50000: [=>..............................] - ETA 320.2s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-648f67b83860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moptimizer_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"DFW\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0moptimizer_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DFW multistep'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@title Train the network  \n",
    "\n",
    "# we will append our results on these lists\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "epochs_times = []\n",
    "\n",
    "# parameters for the training phase\n",
    "nepochs = 50 #@param {type:\"integer\"}\n",
    "batch_size = 128  #@param {type:\"integer\"}\n",
    "verbose = 0 #@param [0, 1]\n",
    "\n",
    "# Loaders\n",
    "trainLoader = torch.utils.data.DataLoader(trainData, batch_size=batch_size, shuffle=True,\n",
    "                                      pin_memory=torch.cuda.is_available(), num_workers=2)\n",
    "testLoader = torch.utils.data.DataLoader(testData, batch_size=batch_size, shuffle=False,\n",
    "                                      pin_memory=torch.cuda.is_available(), num_workers=2)\n",
    "\n",
    "# initialize necessary metrics objects\n",
    "train_loss, train_accuracy = AverageMeter(), AverageMeter()\n",
    "test_loss, test_accuracy = AverageMeter(), AverageMeter()\n",
    "\n",
    "# function to reset metrics\n",
    "def reset_metrics():\n",
    "    train_loss.reset()\n",
    "    train_accuracy.reset()\n",
    "    test_loss.reset()\n",
    "    test_accuracy.reset()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(data=\"train\"):\n",
    "    if data == \"train\":\n",
    "        loader = trainLoader\n",
    "        mean_loss, mean_accuracy = train_loss, train_accuracy\n",
    "    elif data == \"test\":\n",
    "        loader = testLoader\n",
    "        mean_loss, mean_accuracy = test_loss, test_accuracy\n",
    "    \n",
    "    sys.stdout.write(f\"Evaluation of {data} data:\\n\")\n",
    "    \n",
    "    # iteration over the dataset\n",
    "    for x_input, y_target in Bar(loader):\n",
    "        x_input, y_target = x_input.to(device=\"cuda:0\"), y_target.to(device=\"cuda:0\") # we move to GPU\n",
    "        output = model.eval()(x_input)\n",
    "        loss = loss_criterion(output, y_target)\n",
    "        \n",
    "        # update metrics\n",
    "        mean_loss(loss.item(), len(y_target)) \n",
    "        mean_accuracy(Utilities.categorical_accuracy(y_true=y_target, output=output), len(y_target))\n",
    "\n",
    "    \n",
    "# Training\n",
    "for epoch in range(nepochs + 1):\n",
    "    \n",
    "    start = time.time() # start to time\n",
    "    reset_metrics() # reset the metrics from the previous epoch\n",
    "    sys.stdout.write(f\"\\n\\nEpoch {epoch}/{nepochs}\\n\")\n",
    "    \n",
    "    if epoch == 0:\n",
    "        # First pass through the network to evaluate the model once to get the metrics\n",
    "        evaluate_model(data='train')\n",
    "    else:\n",
    "        if epoch > int(0.2 * nepochs) and optimizer_name == \"DFW multistep\" and asymptotic_prox_steps_num >1:\n",
    "            \n",
    "            # if we already finished the first 20% of the epochs we continue with single steps\n",
    "            optimizer.prox_steps = 1\n",
    "            \n",
    "        sys.stdout.write(f\"Training:\\n\")\n",
    "        for x_input, y_target in Bar(trainLoader):\n",
    "            x_input, y_target = x_input.to(device=\"cuda:0\"), y_target.to(device=\"cuda:0\")\n",
    "            optimizer.zero_grad()  # Zero the gradient buffers\n",
    "            output = model.train()(x_input) # compute the output\n",
    "            if dataset_name == \"CIFAR100\" and smoothing:\n",
    "              # smoothing of the loss for DFW\n",
    "              with set_smoothing_enabled(True):\n",
    "                loss = loss_criterion(output, y_target) # compute the loss\n",
    "            else:\n",
    "                loss = loss_critertion(output, y_target) # without smoothing\n",
    "            loss.backward()  # Backpropagation\n",
    "            if optimizer_name == \"DFW\" or optimizer_name == 'DFW multistep':\n",
    "                optimizer.step(lambda: float(loss))\n",
    "            else:\n",
    "                optimizer.step() \n",
    "            train_loss(loss.item(), len(y_target))\n",
    "            train_accuracy(Utilities.categorical_accuracy(y_true=y_target, output=output), len(y_target))\n",
    "\n",
    "    if optimizer_name == \"SGD with scheduler\":\n",
    "        scheduler.step()\n",
    "\n",
    "    # evalutate the model on the test set    \n",
    "    evaluate_model(data='test')\n",
    "    sys.stdout.write(f\"\\n Finished epoch {epoch}/{nepochs}: Train Loss {train_loss.result()} | Test Loss {test_loss.result()} | Train Acc {train_accuracy.result()} | Test Acc {test_accuracy.result()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldLaA2by6VYV"
   },
   "source": [
    "# Parameters used in the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7F6Ai3G6bFd"
   },
   "source": [
    "In order to reproduce our results (i.e. the training trends shown in the report), the following set of parameters should be used.\\\n",
    "If not specified otherwise, the remaining parameters (e.g. the stability perturbation $\\epsilon$ for Adam and Adagrad) are set to their default values.\n",
    "\n",
    "\n",
    "$\\text{Deep Frank Wolfe (single step and multistep)}$:\n",
    "```python\n",
    "eta = 0.1  # proximal coefficient\n",
    "momentum = 0.9  # momentum parameter\n",
    "optimizer = DFW(model.parameters(), eta=eta, \n",
    "            momentum=momentum, prox_steps=2) # or prox_steps=1\n",
    "```\n",
    "\n",
    "$\\textbf{Stochastic Gradient Descent with scheduler}$:\n",
    "```python\n",
    "lr = 0.1  # learning rate\n",
    "momentum = 0.9  # momentum parameter\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=lr,\n",
    "                              momentum=momentum)\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.5)  # define scheduler\n",
    "```\n",
    "\n",
    "$\\textbf{Adam}$:\n",
    "```python\n",
    "lr = 0.001 # learning rate\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, \n",
    "                               betas=(beta_1, beta_2))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'SGD'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCIFAR10\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#@param ['CIFAR10', 'CIFAR100']\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWideResNet\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#@param ['DenseNet', 'WideResNet', 'GoogLeNet']\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mplot_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\Machine-Learning-Optimization_new\\Frank_Wolfe\\utils\\utils.py:161\u001b[0m, in \u001b[0;36mplot_stats\u001b[1;34m(dataset_name, model_type)\u001b[0m\n\u001b[0;32m    159\u001b[0m list_optimizers_tilda \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSGD\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDFW\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDFW\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    160\u001b[0m nepochs \u001b[38;5;241m=\u001b[39m stats_dict_list[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 161\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([stats_dict_list[i][list_optimizers_tilda[i]][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    162\u001b[0m                      \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(list_optimizers_tilda))])\n\u001b[0;32m    163\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([stats_dict_list[i][list_optimizers_tilda[i]][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_losses\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    164\u001b[0m                          \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(list_optimizers_tilda))])\n\u001b[0;32m    166\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m13.4\u001b[39m, \u001b[38;5;241m4.8\u001b[39m), squeeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\Machine-Learning-Optimization_new\\Frank_Wolfe\\utils\\utils.py:161\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    159\u001b[0m list_optimizers_tilda \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSGD\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDFW\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDFW\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    160\u001b[0m nepochs \u001b[38;5;241m=\u001b[39m stats_dict_list[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 161\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mstats_dict_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlist_optimizers_tilda\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    162\u001b[0m                      \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(list_optimizers_tilda))])\n\u001b[0;32m    163\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([stats_dict_list[i][list_optimizers_tilda[i]][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_losses\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    164\u001b[0m                          \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(list_optimizers_tilda))])\n\u001b[0;32m    166\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m13.4\u001b[39m, \u001b[38;5;241m4.8\u001b[39m), squeeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'SGD'"
     ]
    }
   ],
   "source": [
    "dataset_name = 'CIFAR10' #@param ['CIFAR10', 'CIFAR100']\n",
    "model_type = 'WideResNet' #@param ['DenseNet', 'WideResNet', 'GoogLeNet']\n",
    "plot_stats(dataset_name, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = os.path.join(os.getcwd(), 'results/' + dataset_name + '/' + model_type)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "fname = output_folder + '/stats_dict_Adam.pkl'\n",
    "with open(fname, 'rb') as handle:\n",
    "    stats_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elapsed_time': 378.786297082901,\n",
       " 'epochs': 50,\n",
       " 'test_acc': [0.0974,\n",
       "  0.4342,\n",
       "  0.4221,\n",
       "  0.2544,\n",
       "  0.4904,\n",
       "  0.4457,\n",
       "  0.4917,\n",
       "  0.4978,\n",
       "  0.6532,\n",
       "  0.6385,\n",
       "  0.6763,\n",
       "  0.6775,\n",
       "  0.6949,\n",
       "  0.7144,\n",
       "  0.7738,\n",
       "  0.7231,\n",
       "  0.7361,\n",
       "  0.8259,\n",
       "  0.7432,\n",
       "  0.7143,\n",
       "  0.8334,\n",
       "  0.8265,\n",
       "  0.8104,\n",
       "  0.7999,\n",
       "  0.7986,\n",
       "  0.8778,\n",
       "  0.857,\n",
       "  0.8706,\n",
       "  0.8487,\n",
       "  0.8656,\n",
       "  0.8625,\n",
       "  0.839,\n",
       "  0.8434,\n",
       "  0.8751,\n",
       "  0.8536,\n",
       "  0.8669,\n",
       "  0.8835,\n",
       "  0.8842,\n",
       "  0.8752,\n",
       "  0.8745,\n",
       "  0.8882,\n",
       "  0.8766,\n",
       "  0.8999,\n",
       "  0.8852,\n",
       "  0.9008,\n",
       "  0.8795,\n",
       "  0.8765,\n",
       "  0.8818,\n",
       "  0.9025,\n",
       "  0.9057,\n",
       "  0.9131],\n",
       " 'test_losses': [2.302371311569214,\n",
       "  1.6326094013214112,\n",
       "  2.185473705291748,\n",
       "  4.39153924293518,\n",
       "  1.9566892610549926,\n",
       "  2.4382002353668213,\n",
       "  2.2759645847320558,\n",
       "  2.3302810316085814,\n",
       "  1.7475504920959473,\n",
       "  1.8336089906692505,\n",
       "  1.959045574951172,\n",
       "  1.898783680820465,\n",
       "  1.4180903673171996,\n",
       "  1.7816963075637817,\n",
       "  0.9857006398200989,\n",
       "  1.4415303993225097,\n",
       "  1.4836187653541566,\n",
       "  0.8002985520362854,\n",
       "  1.375640837097168,\n",
       "  1.6621715759277345,\n",
       "  0.7979419228553772,\n",
       "  0.902545068359375,\n",
       "  1.1073052798748015,\n",
       "  1.1192901781082154,\n",
       "  1.0736501230955124,\n",
       "  0.6169566803693771,\n",
       "  0.7211227334022522,\n",
       "  0.6881239440917969,\n",
       "  0.9071779748916626,\n",
       "  0.7885805995941162,\n",
       "  0.8062758358001709,\n",
       "  1.0224900785446167,\n",
       "  0.9865866587162018,\n",
       "  0.7661164599418641,\n",
       "  0.9298111859798431,\n",
       "  0.7731278094291687,\n",
       "  0.6972151126742363,\n",
       "  0.6813602777481079,\n",
       "  0.7898342290878296,\n",
       "  0.8068375074386597,\n",
       "  0.6776213050603866,\n",
       "  0.8400608357429504,\n",
       "  0.583261642742157,\n",
       "  0.7884324414253235,\n",
       "  0.6112164390295743,\n",
       "  0.8545423186302185,\n",
       "  0.8224597259521484,\n",
       "  0.8169990732908249,\n",
       "  0.6295882719516754,\n",
       "  0.6145522795677185,\n",
       "  0.5125778142929077],\n",
       " 'train_acc': [0.10498,\n",
       "  0.39398,\n",
       "  0.58832,\n",
       "  0.6744,\n",
       "  0.71954,\n",
       "  0.7613,\n",
       "  0.79286,\n",
       "  0.81758,\n",
       "  0.83706,\n",
       "  0.85094,\n",
       "  0.86222,\n",
       "  0.87496,\n",
       "  0.88272,\n",
       "  0.89112,\n",
       "  0.89764,\n",
       "  0.9037,\n",
       "  0.91152,\n",
       "  0.91872,\n",
       "  0.92352,\n",
       "  0.93032,\n",
       "  0.93122,\n",
       "  0.93868,\n",
       "  0.94162,\n",
       "  0.94472,\n",
       "  0.94838,\n",
       "  0.95116,\n",
       "  0.9535,\n",
       "  0.95802,\n",
       "  0.95906,\n",
       "  0.96176,\n",
       "  0.96372,\n",
       "  0.96522,\n",
       "  0.96708,\n",
       "  0.96634,\n",
       "  0.97282,\n",
       "  0.9724,\n",
       "  0.97322,\n",
       "  0.97456,\n",
       "  0.9766,\n",
       "  0.97304,\n",
       "  0.97736,\n",
       "  0.9783,\n",
       "  0.97966,\n",
       "  0.97994,\n",
       "  0.98084,\n",
       "  0.981,\n",
       "  0.98188,\n",
       "  0.98344,\n",
       "  0.98326,\n",
       "  0.98292,\n",
       "  0.98456],\n",
       " 'train_losses': [2.302814902191162,\n",
       "  1.6168525099945068,\n",
       "  1.138068056716919,\n",
       "  0.9168870031738281,\n",
       "  0.79557968542099,\n",
       "  0.6868325177574157,\n",
       "  0.5929914237117767,\n",
       "  0.5273568414402008,\n",
       "  0.47622221452713015,\n",
       "  0.4351149567699432,\n",
       "  0.4002766014480591,\n",
       "  0.36390425362586976,\n",
       "  0.34201617431640624,\n",
       "  0.31264539785861967,\n",
       "  0.29433092648506165,\n",
       "  0.27500523152828216,\n",
       "  0.25261060572624205,\n",
       "  0.2333773316192627,\n",
       "  0.21892635502815247,\n",
       "  0.20084880249023437,\n",
       "  0.19551191159248352,\n",
       "  0.173232714138031,\n",
       "  0.17035639667510985,\n",
       "  0.15477909198760986,\n",
       "  0.14575732932567598,\n",
       "  0.1374379783344269,\n",
       "  0.13161436477661131,\n",
       "  0.11900858927249909,\n",
       "  0.11613208847522735,\n",
       "  0.10678222413063049,\n",
       "  0.10117474291324616,\n",
       "  0.0986530128800869,\n",
       "  0.09227190858840942,\n",
       "  0.09348938143968583,\n",
       "  0.07830508845567703,\n",
       "  0.07813331291913986,\n",
       "  0.07441913784742356,\n",
       "  0.07312963316917419,\n",
       "  0.0649752086687088,\n",
       "  0.07450468730807304,\n",
       "  0.06455149380683899,\n",
       "  0.05911019393205643,\n",
       "  0.05821404686808586,\n",
       "  0.05609160322904587,\n",
       "  0.05353627966880798,\n",
       "  0.05292379380583763,\n",
       "  0.05070161071896553,\n",
       "  0.0484745462179184,\n",
       "  0.04710264061808586,\n",
       "  0.04796873436868191,\n",
       "  0.04580354317516089]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dict_ = {}\n",
    "stats_dict_.update({\"Adam\": stats_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adam': {'elapsed_time': 378.786297082901,\n",
       "  'epochs': 50,\n",
       "  'test_acc': [0.0974,\n",
       "   0.4342,\n",
       "   0.4221,\n",
       "   0.2544,\n",
       "   0.4904,\n",
       "   0.4457,\n",
       "   0.4917,\n",
       "   0.4978,\n",
       "   0.6532,\n",
       "   0.6385,\n",
       "   0.6763,\n",
       "   0.6775,\n",
       "   0.6949,\n",
       "   0.7144,\n",
       "   0.7738,\n",
       "   0.7231,\n",
       "   0.7361,\n",
       "   0.8259,\n",
       "   0.7432,\n",
       "   0.7143,\n",
       "   0.8334,\n",
       "   0.8265,\n",
       "   0.8104,\n",
       "   0.7999,\n",
       "   0.7986,\n",
       "   0.8778,\n",
       "   0.857,\n",
       "   0.8706,\n",
       "   0.8487,\n",
       "   0.8656,\n",
       "   0.8625,\n",
       "   0.839,\n",
       "   0.8434,\n",
       "   0.8751,\n",
       "   0.8536,\n",
       "   0.8669,\n",
       "   0.8835,\n",
       "   0.8842,\n",
       "   0.8752,\n",
       "   0.8745,\n",
       "   0.8882,\n",
       "   0.8766,\n",
       "   0.8999,\n",
       "   0.8852,\n",
       "   0.9008,\n",
       "   0.8795,\n",
       "   0.8765,\n",
       "   0.8818,\n",
       "   0.9025,\n",
       "   0.9057,\n",
       "   0.9131],\n",
       "  'test_losses': [2.302371311569214,\n",
       "   1.6326094013214112,\n",
       "   2.185473705291748,\n",
       "   4.39153924293518,\n",
       "   1.9566892610549926,\n",
       "   2.4382002353668213,\n",
       "   2.2759645847320558,\n",
       "   2.3302810316085814,\n",
       "   1.7475504920959473,\n",
       "   1.8336089906692505,\n",
       "   1.959045574951172,\n",
       "   1.898783680820465,\n",
       "   1.4180903673171996,\n",
       "   1.7816963075637817,\n",
       "   0.9857006398200989,\n",
       "   1.4415303993225097,\n",
       "   1.4836187653541566,\n",
       "   0.8002985520362854,\n",
       "   1.375640837097168,\n",
       "   1.6621715759277345,\n",
       "   0.7979419228553772,\n",
       "   0.902545068359375,\n",
       "   1.1073052798748015,\n",
       "   1.1192901781082154,\n",
       "   1.0736501230955124,\n",
       "   0.6169566803693771,\n",
       "   0.7211227334022522,\n",
       "   0.6881239440917969,\n",
       "   0.9071779748916626,\n",
       "   0.7885805995941162,\n",
       "   0.8062758358001709,\n",
       "   1.0224900785446167,\n",
       "   0.9865866587162018,\n",
       "   0.7661164599418641,\n",
       "   0.9298111859798431,\n",
       "   0.7731278094291687,\n",
       "   0.6972151126742363,\n",
       "   0.6813602777481079,\n",
       "   0.7898342290878296,\n",
       "   0.8068375074386597,\n",
       "   0.6776213050603866,\n",
       "   0.8400608357429504,\n",
       "   0.583261642742157,\n",
       "   0.7884324414253235,\n",
       "   0.6112164390295743,\n",
       "   0.8545423186302185,\n",
       "   0.8224597259521484,\n",
       "   0.8169990732908249,\n",
       "   0.6295882719516754,\n",
       "   0.6145522795677185,\n",
       "   0.5125778142929077],\n",
       "  'train_acc': [0.10498,\n",
       "   0.39398,\n",
       "   0.58832,\n",
       "   0.6744,\n",
       "   0.71954,\n",
       "   0.7613,\n",
       "   0.79286,\n",
       "   0.81758,\n",
       "   0.83706,\n",
       "   0.85094,\n",
       "   0.86222,\n",
       "   0.87496,\n",
       "   0.88272,\n",
       "   0.89112,\n",
       "   0.89764,\n",
       "   0.9037,\n",
       "   0.91152,\n",
       "   0.91872,\n",
       "   0.92352,\n",
       "   0.93032,\n",
       "   0.93122,\n",
       "   0.93868,\n",
       "   0.94162,\n",
       "   0.94472,\n",
       "   0.94838,\n",
       "   0.95116,\n",
       "   0.9535,\n",
       "   0.95802,\n",
       "   0.95906,\n",
       "   0.96176,\n",
       "   0.96372,\n",
       "   0.96522,\n",
       "   0.96708,\n",
       "   0.96634,\n",
       "   0.97282,\n",
       "   0.9724,\n",
       "   0.97322,\n",
       "   0.97456,\n",
       "   0.9766,\n",
       "   0.97304,\n",
       "   0.97736,\n",
       "   0.9783,\n",
       "   0.97966,\n",
       "   0.97994,\n",
       "   0.98084,\n",
       "   0.981,\n",
       "   0.98188,\n",
       "   0.98344,\n",
       "   0.98326,\n",
       "   0.98292,\n",
       "   0.98456],\n",
       "  'train_losses': [2.302814902191162,\n",
       "   1.6168525099945068,\n",
       "   1.138068056716919,\n",
       "   0.9168870031738281,\n",
       "   0.79557968542099,\n",
       "   0.6868325177574157,\n",
       "   0.5929914237117767,\n",
       "   0.5273568414402008,\n",
       "   0.47622221452713015,\n",
       "   0.4351149567699432,\n",
       "   0.4002766014480591,\n",
       "   0.36390425362586976,\n",
       "   0.34201617431640624,\n",
       "   0.31264539785861967,\n",
       "   0.29433092648506165,\n",
       "   0.27500523152828216,\n",
       "   0.25261060572624205,\n",
       "   0.2333773316192627,\n",
       "   0.21892635502815247,\n",
       "   0.20084880249023437,\n",
       "   0.19551191159248352,\n",
       "   0.173232714138031,\n",
       "   0.17035639667510985,\n",
       "   0.15477909198760986,\n",
       "   0.14575732932567598,\n",
       "   0.1374379783344269,\n",
       "   0.13161436477661131,\n",
       "   0.11900858927249909,\n",
       "   0.11613208847522735,\n",
       "   0.10678222413063049,\n",
       "   0.10117474291324616,\n",
       "   0.0986530128800869,\n",
       "   0.09227190858840942,\n",
       "   0.09348938143968583,\n",
       "   0.07830508845567703,\n",
       "   0.07813331291913986,\n",
       "   0.07441913784742356,\n",
       "   0.07312963316917419,\n",
       "   0.0649752086687088,\n",
       "   0.07450468730807304,\n",
       "   0.06455149380683899,\n",
       "   0.05911019393205643,\n",
       "   0.05821404686808586,\n",
       "   0.05609160322904587,\n",
       "   0.05353627966880798,\n",
       "   0.05292379380583763,\n",
       "   0.05070161071896553,\n",
       "   0.0484745462179184,\n",
       "   0.04710264061808586,\n",
       "   0.04796873436868191,\n",
       "   0.04580354317516089]}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = os.path.join(os.getcwd(), 'results/' + dataset_name + '/' + model_type)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "fname = output_folder + '/stats_dict_Adam.pkl'\n",
    "with open(fname, 'wb') as handle:\n",
    "    pickle.dump(stats_dict_, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "DFW_on_github (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
