{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6zNvQRlf9pN"
   },
   "source": [
    "# Setup instructions\n",
    "\n",
    "We provide a user-ready interface to reproduce our results concerning the Deep Frank-Wolfe algorithm. Therefore, we strongly recommend using Google Colab to perform training. To run this notebook on Google Colab, please import from the **Frank_Wolfe** directory the following files:\n",
    "\n",
    "1. architectures.py\n",
    "2. MultiClassHingeLoss.py\n",
    "3. DFW.py\n",
    "4. utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mC7BWlWnoFBa"
   },
   "outputs": [],
   "source": [
    "#@title Import and utilities \n",
    "\n",
    "from utils import *\n",
    "from DFW import *\n",
    "from architectures import *\n",
    "from MultiClassHingeLoss import *\n",
    "!pip install barbar\n",
    "from barbar import Bar\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Jf6SJcqjoUSA"
   },
   "outputs": [],
   "source": [
    "#@title Choose dataset name and architecture of the network \n",
    "\n",
    "#@markdown Please, run this cell before any new optimizer as to reinitialize the model\n",
    "\n",
    "# Select the dataset and the architecture\n",
    "\n",
    "dataset_name = 'CIFAR10' #@param ['CIFAR10', 'CIFAR100']\n",
    "model_type = 'DenseNet' #@param ['DenseNet', 'WideResNet', 'GoogLeNet']\n",
    "\n",
    "# load the model\n",
    "if model_type == 'GoogLeNet':\n",
    "    model = GoogleNet(num_class=10 if dataset_name == \"CIFAR10\" else 100)\n",
    "elif model_type == 'DenseNet':\n",
    "    model = torchvision.models.densenet121(pretrained=False)\n",
    "elif model_type == 'WideResNet':\n",
    "    model =  WideResNet(num_classes=10 if dataset_name == \"CIFAR10\" else 100)\n",
    "else:\n",
    "    raise ValueError(\"Please, select an available architecture\")\n",
    "\n",
    "# setting dataset attributes, dictionary useful to normalize the images\n",
    "datasetDict = setDatasetAttributes(dataset_name)\n",
    "\n",
    "# transform operation\n",
    "trainTransformDict, testTransformDict = setTrainAndTest(dataset_name) \n",
    "root = f\"{dataset_name}-dataset\"\n",
    "\n",
    "# prepare train and test datasets \n",
    "trainData = datasetDict['datasetDict'](root=root, train=True, download=True,\n",
    "                                            transform=trainTransformDict[dataset_name])\n",
    "testData = datasetDict['datasetDict'](root=root, train=False,\n",
    "                                        transform=testTransformDict[dataset_name])\n",
    "# move the model to GPU\n",
    "model = model.to(device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6IgJdqGoWkV"
   },
   "outputs": [],
   "source": [
    "#@title Choose optimizer and parameters \n",
    "\n",
    "# Choice of the optimizer and the parameters.\n",
    "\n",
    "# The parameters used in our experiments can be found\n",
    "# in the report and at the end of the notebook.\n",
    "\n",
    "optimizer_name = \"DFW multistep\" #@param  ['DFW', 'Adam', 'SGD with scheduler', 'DFW multistep']\n",
    "eta =   0.1 #@param {type:\"number\"}\n",
    "momentum = 0.9 #@param {type:\"number\"}\n",
    "lr = 0.001 #@param {type:\"number\"}\n",
    "beta_1 = 0.9 #@param {type:\"number\"}\n",
    "beta_2 = 0.999 #@param {type:\"number\"}\n",
    "initial_prox_steps = 2 #@param {type: \"number\"}\n",
    "\n",
    "if optimizer_name != \"DFW multistep\":\n",
    "    initial_prox_steps = 1\n",
    "\n",
    "# define the optimizer\n",
    "\n",
    "if optimizer_name == \"DFW\" or optimizer_name == 'DFW multistep':\n",
    "    optimizer = DFW(params=model.parameters(), eta=eta, momentum=momentum,\n",
    "                    prox_steps=initial_prox_steps)\n",
    "    \n",
    "    assert initial_prox_steps > 0\n",
    "    assert eta > 0\n",
    "    assert 0 <= momentum <= 1\n",
    "    \n",
    "elif optimizer_name == \"SGD with scheduler\":\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(), lr=lr,\n",
    "                              momentum=momentum)\n",
    "    scheduler = StepLR(optimizer, step_size=20, gamma=0.2)\n",
    "    assert 0 <= momentum <= 1\n",
    "\n",
    "elif optimizer_name == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, \n",
    "                               betas=(beta_1, beta_2))\n",
    "    \n",
    "if optimizer_name == \"DFW\" or optimizer_name == \"DFW multistep\":\n",
    "    # we consider a convex and piece-wise linear objective for DFW\n",
    "    loss_criterion = MultiClassHingeLoss().to(device=\"cuda:0\")\n",
    "    # smoothing of the loss function for the case of many classes\n",
    "    smoothing = True\n",
    "else: \n",
    "    # cross entropy otherwise\n",
    "    loss_criterion = nn.CrossEntropyLoss().to(device=\"cuda:0\")\n",
    "    smoothing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cq-yV7BYplnM"
   },
   "outputs": [],
   "source": [
    "#@title Train the network\n",
    "\n",
    "# to save training stats\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "# parameters for the training phase\n",
    "nepochs =  5 #@param {type:\"integer\"}\n",
    "batch_size = 128  #@param {type:\"integer\"}\n",
    "\n",
    "# Loaders\n",
    "trainLoader = torch.utils.data.DataLoader(trainData, batch_size=batch_size, shuffle=True,\n",
    "                                      pin_memory=torch.cuda.is_available(), num_workers=2)\n",
    "testLoader = torch.utils.data.DataLoader(testData, batch_size=batch_size, shuffle=False,\n",
    "                                      pin_memory=torch.cuda.is_available(), num_workers=2)\n",
    "\n",
    "# initialize necessary metrics objects\n",
    "train_loss, train_accuracy = AverageMeter(), AverageMeter()\n",
    "test_loss, test_accuracy = AverageMeter(), AverageMeter()\n",
    "\n",
    "# function to reset metrics\n",
    "def reset_metrics():\n",
    "    train_loss.reset()\n",
    "    train_accuracy.reset()\n",
    "    test_loss.reset()\n",
    "    test_accuracy.reset()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(data=\"train\"):\n",
    "    if data == \"train\":\n",
    "        loader = trainLoader\n",
    "        mean_loss, mean_accuracy = train_loss, train_accuracy\n",
    "    elif data == \"test\":\n",
    "        loader = testLoader\n",
    "        mean_loss, mean_accuracy = test_loss, test_accuracy\n",
    "    \n",
    "    sys.stdout.write(f\"Evaluation of {data} data:\\n\")\n",
    "    \n",
    "    # iteration over the dataset\n",
    "    for x_input, y_target in Bar(loader):\n",
    "        x_input, y_target = x_input.to(device=\"cuda:0\"), y_target.to(device=\"cuda:0\") # we move to GPU\n",
    "        output = model.eval()(x_input)\n",
    "        loss = loss_criterion(output, y_target)\n",
    "        \n",
    "        # update metrics\n",
    "        mean_loss(loss.item(), len(y_target)) \n",
    "        mean_accuracy(Utilities.categorical_accuracy(y_true=y_target, output=output), len(y_target))\n",
    "\n",
    "    \n",
    "# Training\n",
    "for epoch in range(nepochs + 1):\n",
    "    \n",
    "    start = time.time() # start to time\n",
    "    reset_metrics() # reset the metrics from the previous epoch\n",
    "    sys.stdout.write(f\"\\n\\nEpoch {epoch}/{nepochs}\\n\")\n",
    "    \n",
    "    if epoch == 0:\n",
    "        # Evaluate the model once to get the metrics\n",
    "        evaluate_model(data='train')\n",
    "    else:\n",
    "        if epoch > int(0.2 * nepochs) and optimizer_name == \"DFW multistep\" and initial_prox_steps > 1:\n",
    "            \n",
    "            # continue with single steps\n",
    "            optimizer.prox_steps = 1\n",
    "            \n",
    "        sys.stdout.write(f\"Training:\\n\")\n",
    "        for x_input, y_target in Bar(trainLoader):\n",
    "            x_input, y_target = x_input.to(device=\"cuda:0\"), y_target.to(device=\"cuda:0\")\n",
    "            optimizer.zero_grad()  # Zero the gradient buffers\n",
    "            output = model.train()(x_input) # compute the output\n",
    "            if dataset_name == \"CIFAR100\" and smoothing:\n",
    "              # smoothing of the loss for DFW\n",
    "              with set_smoothing_enabled(True):\n",
    "                loss = loss_criterion(output, y_target) # compute the loss\n",
    "            else:\n",
    "                loss = loss_criterion(output, y_target) # without smoothing\n",
    "            loss.backward()  # Backpropagation\n",
    "            if optimizer_name == \"DFW\" or optimizer_name == 'DFW multistep':\n",
    "                optimizer.step(lambda: float(loss))\n",
    "            else:\n",
    "                optimizer.step() \n",
    "            train_loss(loss.item(), len(y_target))\n",
    "            train_accuracy(Utilities.categorical_accuracy(y_true=y_target, output=output), len(y_target))\n",
    "\n",
    "    if optimizer_name == \"SGD with scheduler\":\n",
    "        scheduler.step()\n",
    "\n",
    "    # evaluate the model on the test set    \n",
    "    evaluate_model(data='test')\n",
    "    sys.stdout.write(f\"\\n Finished epoch {epoch}/{nepochs}: Train Loss {train_loss.result()} | Test Loss {test_loss.result()} | Train Acc {train_accuracy.result()} | Test Acc {test_accuracy.result()}\\n\")\n",
    "    \n",
    "    # collect training statistics of the current epoch\n",
    "    train_losses.append(train_loss.result())\n",
    "    test_losses.append(test_loss.result())\n",
    "    train_acc.append(train_accuracy.result())\n",
    "    test_acc.append(test_accuracy.result())\n",
    "    elapsed_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vK_9rVoey_Z"
   },
   "outputs": [],
   "source": [
    "#@title Save training statistics from the previous cell\n",
    "\n",
    "results = {'epochs': nepochs, 'train_losses': train_losses, \n",
    "           'train_acc': train_acc, 'test_losses': test_losses, \n",
    "           'test_acc': test_acc, 'elapsed_time': elapsed_time}\n",
    "stats_dict = {}\n",
    "\n",
    "flag = False\n",
    "if(optimizer_name== \"SGD with scheduler\"):\n",
    "    optimizer_name = \"SGD\"\n",
    "if(optimizer_name == \"DFW multistep\"):\n",
    "    flag = True\n",
    "    optimizer_name = \"DFW\" # just for compatibility in the dictionary to be saved\n",
    "\n",
    "stats_dict.update({optimizer_name: results})\n",
    "\n",
    "if(flag):\n",
    "    optimizer_name = \"DFW_multistep\"\n",
    "\n",
    "output_folder = os.path.join(os.getcwd(), 'Frank_Wolfe/results/' + dataset_name + '/' + model_type)  # set the folder\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "fname = output_folder + '/stats_dict_' + optimizer_name + '.pkl'\n",
    "with open(fname, 'wb') as handle:\n",
    "    pickle.dump(stats_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7I_gWlT8aUH"
   },
   "outputs": [],
   "source": [
    "#@title Plot only latest training trends obtained in the previous cell\n",
    "\n",
    "# Test accuracy and training loss plots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(13.4, 4.8), squeeze=False)\n",
    "fig.tight_layout(pad=7.)\n",
    "fig.subplots_adjust(top=0.9, left=0.1, right=0.9, bottom=0.12)\n",
    "ax[0, 0].plot(np.arange(nepochs + 1), test_acc)\n",
    "ax[0, 0].set_ylim([0, 1] if dataset_name == \"CIFAR10\" else [0, 0.8])\n",
    "ax[0, 0].set_xlabel('Epoch', fontsize='x-large')\n",
    "ax[0, 0].set_ylabel('Test accuracy', fontsize='xx-large')\n",
    "ax[0, 0].legend([\"{}\".format(optimizer_name)])\n",
    "ax[0, 1].plot(np.arange(nepochs + 1), train_losses)\n",
    "ax[0, 1].set_xlabel('Epoch', fontsize='x-large')\n",
    "ax[0, 1].set_ylabel('Training loss', fontsize='xx-large')\n",
    "ax[0, 1].legend([\"{}\".format(optimizer_name)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgGN4Cphey_c"
   },
   "source": [
    "# Produce complete training plots\n",
    "\n",
    "### NOTE: here, you can select the training results you collected above to produce plots similar to the ones shown in the report. Please, select the dataset, the architecture and the optimizers of your liking. For the proper working of the plotting function called in the cell below, make sure that the dictionaries are in the corresponding folder. \n",
    "\n",
    "### Of course, you can select as many optimizers as you want for the same plot. We remark that the training loss for SGD and Adam is the cross-entropy loss, while it is the multi-class Hinge loss for DFW algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1dxb0Zx8aUH"
   },
   "outputs": [],
   "source": [
    "#@title Plot results presented in the report\n",
    "\n",
    "# pickle install\n",
    "\n",
    "dataset_name = 'CIFAR10' #@param ['CIFAR10', 'CIFAR100']\n",
    "model_type = 'DenseNet' #@param ['DenseNet', 'WideResNet', 'GoogLeNet']\n",
    "\n",
    "show_Adam = False #@param {type: \"boolean\"}\n",
    "show_SGD = False #@param {type: \"boolean\"}\n",
    "show_DFW = True #@param {type: \"boolean\"}\n",
    "show_DFW_multistep = True #@param {type: \"boolean\"}\n",
    "\n",
    "list_optimizers = []\n",
    "if show_Adam:\n",
    "    list_optimizers.append(\"Adam\")\n",
    "if show_SGD:\n",
    "    list_optimizers.append(\"SGD\")\n",
    "if show_DFW:\n",
    "    list_optimizers.append(\"DFW\")\n",
    "if show_DFW_multistep:\n",
    "    list_optimizers.append(\"DFW_multistep\")\n",
    "\n",
    "# for the proper working of the function below, the corresponding dictionaries should be stored in the results folder,\n",
    "# make sure that you saved them properly in the cell immediately below the training one\\n\",\n",
    "plot_stats(dataset_name, model_type, list_optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldLaA2by6VYV"
   },
   "source": [
    "# Hyper-parameters used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7F6Ai3G6bFd"
   },
   "source": [
    "In order to reproduce our results (i.e. the training trends shown in the report), the following set of parameters should be used.\\\n",
    "If not specified otherwise, other parameters (e.g. $\\epsilon$ for numerical stability) are set to their default values.\n",
    "\n",
    "\n",
    "$\\text{Deep Frank Wolfe (single step and multistep)}$:\n",
    "```python\n",
    "eta = 0.1  # proximal coefficient\n",
    "momentum = 0.9  # momentum parameter\n",
    "optimizer = DFW(model.parameters(), eta=eta, \n",
    "            momentum=momentum, prox_steps=2) # or prox_steps=1\n",
    "```\n",
    "\n",
    "$\\text{Stochastic Gradient Descent with scheduler}$:\n",
    "```python\n",
    "lr = 0.1  # learning rate\n",
    "momentum = 0.9  # momentum parameter\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=lr,\n",
    "                              momentum=momentum)\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.2)  # define scheduler\n",
    "```\n",
    "\n",
    "$\\text{Adam}$:\n",
    "```python\n",
    "lr = 0.001 # learning rate\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, \n",
    "                               betas=(beta_1, beta_2))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "DFW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CS439",
   "language": "python",
   "name": "cs439"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
