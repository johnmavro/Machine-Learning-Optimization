{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Block_coordinate_descent_Convolution_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjmJ5IsZInkj",
        "outputId": "89de8382-6034-4879-d9d3-71b0ddf23525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 1.11.0+cu113\n",
            "Torchvision Version: 0.12.0+cu113\n",
            "GPU is available? True\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "import copy\n",
        "import math\n",
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "from utilities import *\n",
        "from layers import *\n",
        "from CD_utilities import *\n",
        "\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "print(\"Torchvision Version:\", torchvision.__version__)\n",
        "print(\"GPU is available?\", torch.cuda.is_available())\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imported datasets\n",
        "For the testing and comparison of our algorithms we will use the following datasets:\n",
        "\n",
        "1. MNIST\n",
        "2. FashionMNIST\n",
        "3. CIFAR10"
      ],
      "metadata": {
        "id": "N_1aD5CdJCfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0,), (1,))])\n",
        "\n",
        "# change the flag to choose the dataset to work with\n",
        "dataset_flag = 0\n",
        "\n",
        "if dataset_flag ==0:\n",
        "  trainset = datasets.MNIST('../data', train=True, download=True, transform=ts)\n",
        "  testset = datasets.MNIST(root='../data', train=False, download=True, transform=ts)\n",
        "elif dataset_flag ==1:\n",
        "  trainset = datasets.FashionMNIST('../data', train=True, download=True, transform=ts)\n",
        "  testset = datasets.FashionMNIST(root='../data', train=False, download=True, transform=ts)\n",
        "else:\n",
        "  trainset = datasets.CIFAR10('../data', train=True, download=True, transform=ts)\n",
        "  testset = datasets.CIFAR10(root='../data', train=False, download=True, transform=ts)"
      ],
      "metadata": {
        "id": "FCpRsZlPIuew"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset preprocessing"
      ],
      "metadata": {
        "id": "dtekIhnJJJhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_test, y_test, y_train_one_hot, y_test_one_hot, I1, I2 = load_dataset(trainset, testset, 10)\n",
        "\n",
        "# We move to GPU\n",
        "x_train = x_train.to(device = device)\n",
        "x_test = x_test.to(device = device)\n",
        "y_train = y_train.to(device = device)\n",
        "y_test = y_test.to(device = device)\n",
        "y_train_one_hot = y_train_one_hot.to(device = device)\n",
        "y_test_one_hot = y_test_one_hot.to(device = device)\n",
        "\n",
        "cross_entropy = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "GJ9kefhFI6vI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def shift_right(l):\n",
        "#   \"\"\"\n",
        "#   Shifts right a python list by one element.\n",
        "#   \"\"\"\n",
        "#   return l[-1:]+l[:-1]\n",
        "\n",
        "# def filter_conv(W,I1,I2,size = 2):\n",
        "#   \"\"\"\n",
        "#   This function filters the entries of the matrix W so that it behaves like a Convolution.\n",
        "#   :param W: The weight matrix W that contains weights from one layer to the next.\n",
        "#   :param I1: The first dimension of the original 2D matrix\n",
        "#   :param I2: The first dimension of the original 2D matrix\n",
        "#   :return: The filtered weight matrix W\n",
        "#   \"\"\"\n",
        "#   device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#   mask_list = []\n",
        "#   for i in range(size):\n",
        "#     mask_list += [1]*size+[0]*(I2-size)\n",
        "#   mask_list +=[0]*(I1-size)*I2\n",
        "#   full_mask = [mask_list]\n",
        "#   counter = I2-size\n",
        "#   for i in range((I2-size+1)*(I1-size+1)-1):\n",
        "#     next_mask=shift_right(full_mask[-1])\n",
        "#     #print(counter)\n",
        "#     if(counter==0):\n",
        "#       counter = I2-size\n",
        "#       for j in range(size-1):\n",
        "#         next_mask=shift_right(next_mask)\n",
        "#     else:\n",
        "#       counter -=1\n",
        "#     full_mask.append(next_mask)\n",
        "#   if(torch.tensor(full_mask).shape[0]!=W.shape[0]):\n",
        "#     print(torch.tensor(full_mask).shape[0],W.shape[0])\n",
        "#   return torch.mul(torch.tensor(full_mask).to(device),W)\n",
        "\n",
        "# class Layer():\n",
        "#   \"\"\"\n",
        "#   A simple layer class for the three different types of layers of our network,\n",
        "#   Perceptron, Convolution, Average Pooling this class is not equivalent to a\n",
        "#   pytorch default layer since the update is done by the function that calculates\n",
        "#   the closed form solution for the optimal weights. This class is also\n",
        "#   constructed in such a way to be able to process input of up to 2 dimensions.\n",
        "#   \"\"\"\n",
        "#   device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#   def __init__(self,col_size,row_size,col_out,row_out,layer_type=[\"Perceptron\"]):\n",
        "#     \"\"\"\n",
        "#     The init function initializes the basic parameters of the layer:\n",
        "#     :param col_size: the column size of the input\n",
        "#     :param row_size: the row size of the input\n",
        "#     :param col_out: the column size of the output\n",
        "#     :param row_out: the row size of the output\n",
        "#     :param layer_type: the type of the layer,\n",
        "#                        if the layer is a Perceptron we expect a single string\n",
        "#                        that says that the layer is a perceptron\n",
        "#                        if the layer is a Convolution we expect a s string that\n",
        "#                        specifies it as a convolution and 1 parameter which is\n",
        "#                        the size of the convolution\n",
        "#                        if the layer is an Average Pooling layer then we expect\n",
        "#                        analogous paramaters as the convolution layer\n",
        "#     Note that apart from the average pooling, for the other types of layers the initial weight is assigned a random value.\n",
        "#     \"\"\"\n",
        "#     self.col_size = col_size\n",
        "#     self.row_size = row_size\n",
        "#     self.col_out = col_out\n",
        "#     self.row_out = row_out\n",
        "#     self.layer_type = layer_type\n",
        "#     std = math.sqrt(1/(row_size*col_size))\n",
        "#     if(self.layer_type[0] ==\"Average Pooling\"):\n",
        "#       self.weights = torch.add(torch.FloatTensor(self.row_out*self.col_out,self.row_size*self.col_size),1).to(self.device)\n",
        "#       print(torch.norm(self.weights))\n",
        "#       self.weights = filter_conv(self.weights,self.col_size,self.row_size,self.layer_type[1])\n",
        "#       print(torch.norm(self.weights))\n",
        "#       self.weights = torch.mul(self.weights,1/(self.layer_type[1]*self.layer_type[1]))\n",
        "#       self.bias = torch.FloatTensor(row_out*col_out,1).uniform_(-std, std).to(self.device)\n",
        "#     elif(self.layer_type[0]==\"Convolution\"):\n",
        "#       self.weights = torch.FloatTensor(self.row_out*self.col_out,self.row_size*self.col_size).uniform_(-std, std).to(self.device)\n",
        "#       self.weights = filter_conv(self.weights,self.col_size,self.row_size,self.layer_type[1])\n",
        "#     else:\n",
        "#       self.weights = torch.FloatTensor(row_out*col_out,row_size*col_size).uniform_(-std, std).to(self.device)\n",
        "#     self.bias = torch.FloatTensor(row_out*col_out,1).uniform_(-std, std).to(self.device)\n",
        "\n",
        "#   def forward_pass(self,input,N):\n",
        "#     \"\"\"\n",
        "#     A function that does a forward pass from the layer, is capable of doing it\n",
        "#     for N samples simultaniously.\n",
        "#     :param input: The input that we do the forward pass on, should have dimension\n",
        "#                   input_dimension_layer x number_of_samples\n",
        "#     :param N: The number of samples that are in the input\n",
        "#     \"\"\"\n",
        "#     return torch.addmm(self.bias.repeat(1, N), self.weights, input)\n",
        "\n",
        "#   def update_layer(self,output,input,alpha,rho):\n",
        "#     \"\"\"\n",
        "#     A function that applies the closed form solution, for all different types of layers\n",
        "#     :param output: The output of the layer with respect to which we should compute the closed form solution\n",
        "#     :param input: The input of the layer with respect to which we should compute the closed form solution\n",
        "#     :param alpha: The parameter alpha of the algorithm that is described on the paper\n",
        "#     :param rho: The parameter rho of the algorithm that is described on the paper\n",
        "#     \"\"\"\n",
        "#     if(self.layer_type[0]!=\"Average Pooling\"):\n",
        "#       self.weights,self.bias = update_wb_js(output,input,self.weights,self.bias,alpha,rho)\n",
        "#       if(self.layer_type[0]==\"Convolution\"):\n",
        "#         self.weights = filter_conv(self.weights,self.col_size,self.row_size,self.layer_type[1])\n",
        "\n",
        "#   def get_weights(self):\n",
        "#     \"\"\"\n",
        "#     This function returns the current weights of the layer\n",
        "#     \"\"\"\n",
        "#     return self.weights\n",
        "\n",
        "#   def get_bias(self):\n",
        "#     \"\"\"\n",
        "#     This function returns the bias of the layer\n",
        "#     \"\"\"\n",
        "#     return self.bias\n",
        "\n",
        "#   def get_type(self):\n",
        "#     \"\"\"\n",
        "#     This function returns the type of the layer\n",
        "#     \"\"\"\n",
        "#     return self.layer_type\n"
      ],
      "metadata": {
        "id": "dER6KR4EYX_H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prox_linear(true_labels,V,U,gamma,alpha):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  V_opt = (gamma*V+alpha*U+true_labels - V)/(gamma+alpha)\n",
        "  return V_opt"
      ],
      "metadata": {
        "id": "V-umeYu3eslJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture initialization\n",
        "\n",
        "For the MultiLayerPerceptron we have the parameters **input_size** , **hidden_size**,**output_size** corresponding to the size of the input layer, the hidden layer and the output layer, respectively.\n",
        "\n",
        "The MLP only has 3 layers like https://github.com/timlautk/BCD-for-DNNs-PyTorch/blob/master/bcd_dnn_mlp_mnist.ipynb as a starting point.\n",
        "\n",
        "Also we use ReLU currently for the same reason."
      ],
      "metadata": {
        "id": "w7M3uSc8JRdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = x_train.shape[0]\n",
        "hidden_size = 1600\n",
        "output_size = 10"
      ],
      "metadata": {
        "id": "QiyJP8y2I81R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "Note: Fix it so that it moves everything to device in the following function and that it does the label sample split here"
      ],
      "metadata": {
        "id": "QAm_re0mJbOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_training(layers, input_size, hidden_size, output_size, train_set, val_set,\n",
        "                     train_labels, val_labels, y_train_one_hot, y_test_one_hot, use_gradient, I1 = 40, I2 = 40,\n",
        "                     niter = 100, gamma = 1, alpha = 5):\n",
        "  \"\"\"\n",
        "  The function takes the following arguements and produces a list of weights and biases with which\n",
        "  you can use the make_pred function to get a list of predictions\n",
        "  :param layers: The total number of layers of the network\n",
        "  :param input_size: The total size of the input layer\n",
        "  :param hidden_size: The size of the hidden layer\n",
        "  :param output_size: The size of the output layer (usefull for multiclass classification)\n",
        "  :param train_set: The training set\n",
        "  :param val_set: The validation set\n",
        "  :param train_labels: The training labels\n",
        "  :param val labels: The validation labels\n",
        "  :param use_gradient: True if the first update of V is carried out without linearization but using the gradient\n",
        "  :param niter: The default number of epochs to train the network\n",
        "  :param gamma: The gamma parameter of the algorithm\n",
        "  :param alpha: The alpha parameter of the algorithm\n",
        "  :return Ws,bs: Returns two lists that go in order from the input to the output layer of the weights and the biases of each layer\n",
        "  \"\"\"\n",
        "\n",
        "  N = len(train_labels)\n",
        "  N_test = len(val_labels)\n",
        "\n",
        "  # weight initialization (we replicate pytorch weight initialization)\n",
        "\n",
        "  std = math.sqrt(1/input_size)\n",
        "  Layer1 = Layer(input_size,1,hidden_size,1,[\"Perceptron\"])\n",
        "  W = Layer1.get_weights()\n",
        "  b = Layer1.get_bias()\n",
        "  #W = torch.FloatTensor(hidden_size, input_size).uniform_(-std, std)\n",
        "  #b = torch.FloatTensor(hidden_size, 1).uniform_(-std, std)\n",
        "\n",
        "  #b = b.to(device = device)\n",
        "  #W = W.to(device = device)\n",
        "\n",
        "  U = torch.addmm(b.repeat(1, N), W, train_set) # equivalent to W1@train_set+b1.repeat(1,N)\n",
        "  V = nn.ReLU()(U)\n",
        "\n",
        "  Ws = [W]\n",
        "  bs = [b]\n",
        "  Us = [U]\n",
        "  Vs = [V]\n",
        "  Layers = [Layer1]\n",
        "  row = [I1]\n",
        "  col = [I2]\n",
        "\n",
        "  cr_row_size = I1\n",
        "  cr_col_size = I2\n",
        "  size = 4\n",
        "  avg_size = 2\n",
        "  for cr_layer in layers:\n",
        "    std = math.sqrt(1/hidden_size)\n",
        "    if(cr_layer[0] !=\"Perceptron\"):\n",
        "      Layer_i = Layer(cr_col_size,cr_row_size,cr_col_size-cr_layer[1]+1,cr_row_size-cr_layer[1]+1,cr_layer)\n",
        "      W = Layer_i.get_weights()\n",
        "      b = Layer_i.get_bias()\n",
        "      Layers.append(Layer_i)\n",
        "      row.append(cr_row_size)\n",
        "      col.append(cr_col_size)\n",
        "      cr_row_size = cr_row_size - cr_layer[1]+1\n",
        "      cr_col_size = cr_col_size - cr_layer[1]+1\n",
        "    else:\n",
        "      Layer_i = Layer(cr_col_size,cr_row_size,cr_layer[1],cr_layer[2],[cr_layer[0]])\n",
        "      W = Layer_i.get_weights()\n",
        "      b = Layer_i.get_bias()\n",
        "      Layers.append(Layer_i)\n",
        "      row.append(cr_row_size)\n",
        "      col.append(cr_col_size)\n",
        "      cr_row_size = cr_layer[1]\n",
        "      cr_col_size = cr_layer[2]\n",
        "    if(cr_layer[0] != \"Average Pooling\"):\n",
        "      #print(W.shape,Vs[-1].shape)\n",
        "      U = torch.addmm(b.repeat(1, N), W, Vs[-1])\n",
        "      V = nn.ReLU()(U)\n",
        "    else:\n",
        "      U = torch.addmm(b.repeat(1, N), W, Vs[-1])\n",
        "      V = U\n",
        "    Ws.append(W)\n",
        "    bs.append(b)\n",
        "    Us.append(U)\n",
        "    Vs.append(V)\n",
        "  for i in range(len(bs)):\n",
        "    print(\"Layer \",i,\" W \", Ws[i].shape,\" Layer W \",Layers[i].get_weights().shape)\n",
        "    print(Layers[i].get_type())\n",
        "\n",
        "  row.append(cr_row_size)\n",
        "  col.append(cr_col_size)\n",
        "  std = math.sqrt(1/hidden_size)\n",
        "  Layer_out = Layer(cr_col_size,cr_row_size,output_size,1,[\"Perceptron\",10])\n",
        "  W = Layer_out.get_weights()\n",
        "  b = Layer_out.get_bias()\n",
        "  #print(W.shape)\n",
        "  # we move them to GPU\n",
        "  #b = b.to(device = device)\n",
        "  #W = W.to(device = device)\n",
        "  #U = torch.addmm(b.repeat(1, N), W, Vs[-1])\n",
        "  #V = U\n",
        "  #print(U.shape)\n",
        "  #Ws.append(W)\n",
        "  Layers.append(Layer_out)\n",
        "  #W = torch.FloatTensor(output_size, cr_row_size*cr_col_size).uniform_(-std, std)\n",
        "  #b = torch.FloatTensor(output_size, 1).uniform_(-std, std)\n",
        "\n",
        "  # we move them to GPU\n",
        "  #b = b.to(device = device)\n",
        "  #W = W.to(device = device)\n",
        "  print(cr_col_size,cr_row_size)\n",
        "  print(W.shape,Vs[-1].shape)\n",
        "  U = torch.addmm(b.repeat(1, N), W, Vs[-1])\n",
        "  V = U\n",
        "  Ws.append(W)\n",
        "  bs.append(b)\n",
        "  Us.append(U)\n",
        "  Vs.append(V)\n",
        "\n",
        "  for i in range(len(bs)):\n",
        "    print(\"Layer \",i,\" W \", bs[i].shape,\" Layer W \",Layers[i].get_bias().shape)\n",
        "    print(Layers[i].get_type())\n",
        "\n",
        "  # constant initialization\n",
        "\n",
        "  gamma1 = gamma2 = gamma3 = gamma4 = gamma\n",
        "\n",
        "  rho = gamma\n",
        "  rho1 = rho2 = rho3 = rho4 = rho\n",
        "\n",
        "  alpha1 = alpha2 = alpha3 = alpha4 = alpha5 = alpha6 = alpha7 \\\n",
        "  = alpha8 = alpha9 = alpha10 = alpha\n",
        "\n",
        "  # vector of performance initialization\n",
        "\n",
        "  loss1 = np.empty(niter)\n",
        "  loss2 = np.empty(niter)\n",
        "  loss_class = np.empty(niter)\n",
        "  accuracy_train = np.empty(niter)\n",
        "  accuracy_test = np.empty(niter)\n",
        "  time1 = np.empty(niter)\n",
        "\n",
        "  opt_accuracy = 0\n",
        "  print(len(Ws),len(Layers))\n",
        "  early_Ws = Ws\n",
        "  early_bs = bs\n",
        "  print('Train on', N, 'samples, validate on', N_test, 'samples')\n",
        "  for k in range(niter):\n",
        "\n",
        "    start = time.time()\n",
        "    Last_layer = Layers[-1]\n",
        "    W = Last_layer.get_weights()\n",
        "    b = Last_layer.get_bias()\n",
        "    # update V3\n",
        "    if use_gradient == True:\n",
        "      if (k == 1):\n",
        "        Vs[-1] = (y_train_one_hot + gamma3*Us[-1] + alpha1*Vs[-1])/(1+ gamma3 + alpha1)\n",
        "      else:\n",
        "        for i in range(250):\n",
        "          #Vs[-1] = (alpha1*Vs[-1] + gamma3*Us[-1] - (torch.exp(Vs[-1])/torch.sum(torch.exp(Vs[-1]),dim=0)-y_train_one_hot))/(gamma3+alpha1)\n",
        "          #Vs[-1] = Vs[-1] - (alpha1*Vs[-1] + gamma3*Us[-1] + (torch.exp(Vs[-1])/torch.sum(torch.exp(Vs[-1]),dim=0)-y_train_one_hot))/(gamma3+alpha1)*0.01/(i+1)\n",
        "          Vs[-1] = Vs[-1] - (gamma3*(Vs[-1]-Us[-1])+torch.exp(Vs[-1])/torch.sum(torch.exp(Vs[-1]),dim=0)-y_train_one_hot) * 0.01/(i+1)\n",
        "    else:\n",
        "      #Vs[-1] = (gamma*Vs[-1]+alpha*Us[-1]+ y_train_one_hot - Vs[-1])/(gamma+alpha)\n",
        "      #(y_train_one_hot + gamma3*Us[-1] + alpha1*Vs[-1])/(1+ gamma3 + alpha1)\n",
        "      Vs[-1] = (y_train_one_hot + gamma3*Us[-1] + alpha1*Vs[-1])/(1+ gamma3 + alpha1)\n",
        "\n",
        "    # update U3\n",
        "    Us[-1] = (gamma3*Vs[-1] + rho3*(torch.mm(W,Vs[-2]) + b.repeat(1,N)))/(gamma3 + rho3)\n",
        "\n",
        "    # update W3 and b3\n",
        "    W, b = update_wb_js(Us[-1],Vs[-2],Ws[-1],bs[-1],alpha1, rho3)\n",
        "    Ws[-1] = W\n",
        "    bs[-1] = b\n",
        "    Layers[-1].update_layer(Us[-1],Vs[-2],alpha1, rho3)\n",
        "\n",
        "    for i in range(len(Vs)-2,0,-1):\n",
        "      Layer_next = Layers[i+1]\n",
        "      Layer_cur = Layers[i]\n",
        "      L_next_type = Layer_next.get_type()\n",
        "      W_next = Layer_next.get_weights()\n",
        "      W_cur = Layer_cur.get_weights()\n",
        "      b_next = Layer_next.get_bias()\n",
        "      b_cur = Layer_cur.get_bias()\n",
        "      if(L_next_type[0]==\"Average Pooling\"):\n",
        "        Vs[i] = update_no_activation(Us[i],Us[i+1],W_next,b_next,rho3,gamma2)\n",
        "        Us[i] = Vs[i]\n",
        "      else:\n",
        "        Vs[i] = update_v_js(Us[i],Us[i+1],W_next,b_next,rho3,gamma2)\n",
        "        Us[i] = relu_prox(Vs[i],(rho2*torch.addmm(b_cur.repeat(1,N), W_cur, Vs[i-1]) +\n",
        "                                alpha2*Us[i])/(rho2 + alpha2),(rho2 + alpha2)/gamma2, row[i+1]*col[i+1], N)\n",
        "        W,b = update_wb_js(Us[i],Vs[i-1],W_cur,b_cur,alpha3,rho2)\n",
        "        Layers[i].update_layer(Us[i],Vs[i-1],alpha3,rho2)\n",
        "\n",
        "    # update V1\n",
        "    Vs[0] = update_v_js(Us[0],Us[1],Ws[1],bs[1],rho2,gamma1)\n",
        "\n",
        "    # update U1\n",
        "    Us[0] = relu_prox(Vs[0],(rho1*torch.addmm(bs[0].repeat(1,N), Ws[0], train_set) +\n",
        "                             alpha7*Us[0])/(rho1 + alpha7),(rho1 + alpha7)/gamma1, hidden_size, N)\n",
        "\n",
        "    # update W1 and b1\n",
        "    W, b = update_wb_js(Us[0],train_set,Ws[0],bs[0],alpha8,rho1)\n",
        "    Ws[0] = W\n",
        "    bs[0] = b\n",
        "    Layers[0].update_layer(Us[0],train_set,alpha8,rho1)\n",
        "\n",
        "    #a1_train = nn.ReLU()(torch.addmm(b1.repeat(1, N), W1, train_set))\n",
        "    #a1_train = train_set\n",
        "    #for i in range(len(Vs)-1,0,-1):\n",
        "    #  a1_train = nn.ReLU()(torch.addmm(bs[i].repeat(1, N), Ws[i], a1_train))\n",
        "    #pred = torch.argmax(torch.addmm(bs[0].repeat(1, N), Ws[0], a1_train), dim=0)\n",
        "    #pred,_ = make_pred(Ws,bs,train_set,N)\n",
        "\n",
        "    #a1_test = val_set\n",
        "    #a1_test = nn.ReLU()(torch.addmm(b1.repeat(1, N_test), W1, val_set))\n",
        "    #for i in range(len(Vs)-1,0,-1):\n",
        "    #  a1_test = nn.ReLU()(torch.addmm(bs[i].repeat(1, N_test), Ws[i], a1_test))\n",
        "    #pred_test = torch.argmax(torch.addmm(bs[0].repeat(1, N_test), Ws[0], a1_test), dim=0)\n",
        "    #pred_test, prob_test = make_pred(Ws,bs,val_set,N_test)\n",
        "    pred_Ws = [l.get_weights() for l in Layers]\n",
        "    pred_bs = [l.get_bias() for l in Layers]\n",
        "    pred,_ = make_pred(pred_Ws,pred_bs,x_train,N)\n",
        "\n",
        "    #a1_test = x_test\n",
        "    #a1_test = nn.ReLU()(torch.addmm(b1.repeat(1, N_test), W1, x_test))\n",
        "    #for i in range(len(Vs)-1,0,-1):\n",
        "    #  a1_test = nn.ReLU()(torch.addmm(bs[i].repeat(1, N_test), Ws[i], a1_test))\n",
        "    #pred_test = torch.argmax(torch.addmm(bs[0].repeat(1, N_test), Ws[0], a1_test), dim=0)\n",
        "    pred_test, prob_test = make_pred(pred_Ws,pred_bs,x_test,N_test)\n",
        "\n",
        "    loss_class[k] = torch.sum(- y_test_one_hot * torch.log(prob_test))\n",
        "\n",
        "    loss1[k] = gamma/2*torch.pow(torch.dist(Vs[-1],y_train_one_hot,2),2).cpu().numpy()\n",
        "    loss2[k] = loss1[k] + gamma/2 * torch.pow(torch.dist(torch.addmm(bs[0].repeat(1,N), Ws[0], train_set),Us[0],2),2).cpu().numpy()\n",
        "\n",
        "    for i in range(1,len(layers)):\n",
        "      loss2[k] = loss2[k] + gamma/2 * torch.pow(torch.dist(torch.addmm(bs[i].repeat(1,N), Ws[i], Vs[i-1]),Us[i],2),2).cpu().numpy()\n",
        "\n",
        "    #loss2[k] = loss1[k] + rho1/2*torch.pow(torch.dist(torch.addmm(b1.repeat(1,N), W1, train_set),U1,2),2).cpu().numpy() \\\n",
        "    #+rho2/2*torch.pow(torch.dist(torch.addmm(b2.repeat(1,N), W2, V1),U2,2),2).cpu().numpy() \\\n",
        "    #+rho3/2*torch.pow(torch.dist(torch.addmm(b3.repeat(1,N), W3, V2),U3,2),2).cpu().numpy()\n",
        "\n",
        "    # compute training accuracy\n",
        "    correct_train = pred == train_labels\n",
        "    accuracy_train[k] = np.mean(correct_train.cpu().numpy())\n",
        "\n",
        "    # compute validation accuracy\n",
        "    correct_test = pred_test == val_labels\n",
        "    accuracy_test[k] = np.mean(correct_test.cpu().numpy())\n",
        "\n",
        "    # compute training time\n",
        "    stop = time.time()\n",
        "    duration = stop - start\n",
        "    time1[k] = duration\n",
        "\n",
        "    # print results\n",
        "    print('Epoch', k + 1, '/', niter, '\\n',\n",
        "          '-', 'time:', time1[k], '-', 'sq_loss:', loss1[k], '-', 'tot_loss:',\n",
        "          loss2[k], '-', 'loss_class:', loss_class[k], '-', 'acc:',\n",
        "          accuracy_train[k], '-', 'val_acc:', accuracy_test[k])\n",
        "    if(accuracy_test[k]>opt_accuracy):\n",
        "      early_Ws = Ws\n",
        "      early_bs = bs\n",
        "      opt_accuracy = accuracy_test[k]\n",
        "\n",
        "  print('The total time spent is:', np.sum(time1), 's')\n",
        "  print('\\n\\n')\n",
        "  print('Early stopping accuracy:',opt_accuracy)\n",
        "  return loss1,loss_class,accuracy_train,accuracy_test,time1,early_Ws,early_bs"
      ],
      "metadata": {
        "id": "lJSDpRPRJ0xb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_,_,_,_,_,_,_ = execute_training([[\"Convolution\",4],[\"Average Pooling\",3]], input_size, hidden_size, output_size, x_train, x_test, y_train, y_test,y_train_one_hot,y_test_one_hot,\n",
        "                                         False, niter = 100, gamma = 0.1, alpha = 4)"
      ],
      "metadata": {
        "id": "PLkWZMwqJ1vH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbc359bd-98ac-40db-bd32-3d243dc4e125"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1295., device='cuda:0')\n",
            "tensor(105., device='cuda:0')\n",
            "Layer  0  W  torch.Size([1600, 784])  Layer W  torch.Size([1600, 784])\n",
            "['Perceptron']\n",
            "Layer  1  W  torch.Size([1369, 1600])  Layer W  torch.Size([1369, 1600])\n",
            "['Convolution', 4]\n",
            "Layer  2  W  torch.Size([1225, 1369])  Layer W  torch.Size([1225, 1369])\n",
            "['Average Pooling', 3]\n",
            "35 35\n",
            "torch.Size([10, 1225]) torch.Size([1225, 60000])\n",
            "Layer  0  W  torch.Size([1600, 1])  Layer W  torch.Size([1600, 1])\n",
            "['Perceptron']\n",
            "Layer  1  W  torch.Size([1369, 1])  Layer W  torch.Size([1369, 1])\n",
            "['Convolution', 4]\n",
            "Layer  2  W  torch.Size([1225, 1])  Layer W  torch.Size([1225, 1])\n",
            "['Average Pooling', 3]\n",
            "Layer  3  W  torch.Size([10, 1])  Layer W  torch.Size([10, 1])\n",
            "['Perceptron', 10]\n",
            "4 4\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1 / 100 \n",
            " - time: 1.3868930339813232 - sq_loss: 1957.901171875 - tot_loss: 2007.0082885830664 - loss_class: 22999.5390625 - acc: 0.09863333333333334 - val_acc: 0.0958\n",
            "Epoch 2 / 100 \n",
            " - time: 1.3784492015838623 - sq_loss: 1272.91025390625 - tot_loss: 1386.4352713085711 - loss_class: 22929.1015625 - acc: 0.26315 - val_acc: 0.2629\n",
            "Epoch 3 / 100 \n",
            " - time: 1.371537685394287 - sq_loss: 832.1627929687501 - tot_loss: 991.1729233082617 - loss_class: 22846.15625 - acc: 0.5168166666666667 - val_acc: 0.5235\n",
            "Epoch 4 / 100 \n",
            " - time: 1.3793129920959473 - sq_loss: 545.819677734375 - tot_loss: 733.6018502848922 - loss_class: 22770.84375 - acc: 0.6691333333333334 - val_acc: 0.6824\n",
            "Epoch 5 / 100 \n",
            " - time: 1.3779106140136719 - sq_loss: 358.58273925781253 - tot_loss: 564.5255102270748 - loss_class: 22706.46875 - acc: 0.6868833333333333 - val_acc: 0.704\n",
            "Epoch 6 / 100 \n",
            " - time: 1.3860092163085938 - sq_loss: 235.7906494140625 - tot_loss: 453.6995947756339 - loss_class: 22652.484375 - acc: 0.6759666666666667 - val_acc: 0.6932\n",
            "Epoch 7 / 100 \n",
            " - time: 1.3899650573730469 - sq_loss: 155.14409179687502 - tot_loss: 381.4566424668767 - loss_class: 22607.40625 - acc: 0.6615333333333333 - val_acc: 0.6766\n",
            "Epoch 8 / 100 \n",
            " - time: 1.3879458904266357 - sq_loss: 102.13555297851563 - tot_loss: 334.76811104500666 - loss_class: 22569.69140625 - acc: 0.6480333333333334 - val_acc: 0.6609\n",
            "Epoch 9 / 100 \n",
            " - time: 1.3907389640808105 - sq_loss: 67.27604370117187 - tot_loss: 304.9652499642223 - loss_class: 22537.96484375 - acc: 0.6386333333333334 - val_acc: 0.6527\n",
            "Epoch 10 / 100 \n",
            " - time: 1.4013762474060059 - sq_loss: 44.342813110351564 - tot_loss: 286.2851425494999 - loss_class: 22511.099609375 - acc: 0.6325666666666667 - val_acc: 0.6462\n",
            "Epoch 11 / 100 \n",
            " - time: 1.3857944011688232 - sq_loss: 29.249731445312502 - tot_loss: 274.9059643421322 - loss_class: 22488.181640625 - acc: 0.6295166666666666 - val_acc: 0.6429\n",
            "Epoch 12 / 100 \n",
            " - time: 1.4025137424468994 - sq_loss: 19.312031555175782 - tot_loss: 268.30040218327196 - loss_class: 22468.47265625 - acc: 0.6279 - val_acc: 0.6422\n",
            "Epoch 13 / 100 \n",
            " - time: 1.3976149559020996 - sq_loss: 12.765195465087892 - tot_loss: 264.8022852536291 - loss_class: 22451.384765625 - acc: 0.6279166666666667 - val_acc: 0.642\n",
            "Epoch 14 / 100 \n",
            " - time: 1.400679349899292 - sq_loss: 8.449325561523438 - tot_loss: 263.3163051057607 - loss_class: 22436.447265625 - acc: 0.6288666666666667 - val_acc: 0.6425\n",
            "Epoch 15 / 100 \n",
            " - time: 1.3986260890960693 - sq_loss: 5.6018417358398445 - tot_loss: 263.1244840927422 - loss_class: 22423.275390625 - acc: 0.63055 - val_acc: 0.6443\n",
            "Epoch 16 / 100 \n",
            " - time: 1.399862289428711 - sq_loss: 3.721288299560547 - tot_loss: 263.7578454945236 - loss_class: 22411.5625 - acc: 0.6328 - val_acc: 0.6461\n",
            "Epoch 17 / 100 \n",
            " - time: 1.4024755954742432 - sq_loss: 2.4778362274169923 - tot_loss: 264.9111521352083 - loss_class: 22401.05859375 - acc: 0.6350666666666667 - val_acc: 0.6497\n",
            "Epoch 18 / 100 \n",
            " - time: 1.3980021476745605 - sq_loss: 1.6544710159301759 - tot_loss: 266.38667275533084 - loss_class: 22391.56640625 - acc: 0.6377333333333334 - val_acc: 0.6521\n",
            "Epoch 19 / 100 \n",
            " - time: 1.3972258567810059 - sq_loss: 1.1083422660827638 - tot_loss: 268.05728152021766 - loss_class: 22382.91796875 - acc: 0.6407833333333334 - val_acc: 0.6544\n",
            "Epoch 20 / 100 \n",
            " - time: 1.3990228176116943 - sq_loss: 0.7453721523284913 - tot_loss: 269.8416364252567 - loss_class: 22374.9765625 - acc: 0.6436666666666667 - val_acc: 0.6571\n",
            "Epoch 21 / 100 \n",
            " - time: 1.4000647068023682 - sq_loss: 0.5035598754882813 - tot_loss: 271.6886515028775 - loss_class: 22367.63671875 - acc: 0.6464 - val_acc: 0.6598\n",
            "Epoch 22 / 100 \n",
            " - time: 1.400312900543213 - sq_loss: 0.3420143127441406 - tot_loss: 273.56654748842124 - loss_class: 22360.8046875 - acc: 0.6495833333333333 - val_acc: 0.6629\n",
            "Epoch 23 / 100 \n",
            " - time: 1.392820119857788 - sq_loss: 0.233739972114563 - tot_loss: 275.456135737896 - loss_class: 22354.41015625 - acc: 0.6522166666666667 - val_acc: 0.6658\n",
            "Epoch 24 / 100 \n",
            " - time: 1.3734853267669678 - sq_loss: 0.1608946204185486 - tot_loss: 277.34621032103894 - loss_class: 22348.38671875 - acc: 0.6551166666666667 - val_acc: 0.6685\n",
            "Epoch 25 / 100 \n",
            " - time: 1.3862829208374023 - sq_loss: 0.11167038679122926 - tot_loss: 279.2308494381607 - loss_class: 22342.69140625 - acc: 0.6576666666666666 - val_acc: 0.6712\n",
            "Epoch 26 / 100 \n",
            " - time: 1.3961055278778076 - sq_loss: 0.07824004888534547 - tot_loss: 281.1072046145797 - loss_class: 22337.2734375 - acc: 0.66005 - val_acc: 0.6741\n",
            "Epoch 27 / 100 \n",
            " - time: 1.3995487689971924 - sq_loss: 0.05540510416030884 - tot_loss: 282.9743231028318 - loss_class: 22332.10546875 - acc: 0.6623166666666667 - val_acc: 0.6762\n",
            "Epoch 28 / 100 \n",
            " - time: 1.401808500289917 - sq_loss: 0.0397057056427002 - tot_loss: 284.832561711967 - loss_class: 22327.15625 - acc: 0.6644166666666667 - val_acc: 0.6791\n",
            "Epoch 29 / 100 \n",
            " - time: 1.4064404964447021 - sq_loss: 0.028833025693893434 - tot_loss: 286.6826990246773 - loss_class: 22322.404296875 - acc: 0.6661666666666667 - val_acc: 0.6806\n",
            "Epoch 30 / 100 \n",
            " - time: 1.4148900508880615 - sq_loss: 0.021241717040538788 - tot_loss: 288.5260838732124 - loss_class: 22317.828125 - acc: 0.6680666666666667 - val_acc: 0.6829\n",
            "Epoch 31 / 100 \n",
            " - time: 1.4029622077941895 - sq_loss: 0.015893907845020296 - tot_loss: 290.3639092087746 - loss_class: 22313.40625 - acc: 0.6698666666666667 - val_acc: 0.6847\n",
            "Epoch 32 / 100 \n",
            " - time: 1.3964612483978271 - sq_loss: 0.012089700251817704 - tot_loss: 292.197673920542 - loss_class: 22309.12890625 - acc: 0.6716333333333333 - val_acc: 0.6864\n",
            "Epoch 33 / 100 \n",
            " - time: 1.4108402729034424 - sq_loss: 0.009355024993419647 - tot_loss: 294.0285244494677 - loss_class: 22304.984375 - acc: 0.67305 - val_acc: 0.6881\n",
            "Epoch 34 / 100 \n",
            " - time: 1.40974760055542 - sq_loss: 0.00736716240644455 - tot_loss: 295.8576200693846 - loss_class: 22300.9609375 - acc: 0.6745333333333333 - val_acc: 0.6902\n",
            "Epoch 35 / 100 \n",
            " - time: 1.406933069229126 - sq_loss: 0.005905134230852128 - tot_loss: 297.68591682985425 - loss_class: 22297.05078125 - acc: 0.6762666666666667 - val_acc: 0.6916\n",
            "Epoch 36 / 100 \n",
            " - time: 1.4108772277832031 - sq_loss: 0.00481664314866066 - tot_loss: 299.51427977606653 - loss_class: 22293.2421875 - acc: 0.6779666666666667 - val_acc: 0.6932\n",
            "Epoch 37 / 100 \n",
            " - time: 1.4035894870758057 - sq_loss: 0.0039959967136383055 - tot_loss: 301.34334643036124 - loss_class: 22289.53515625 - acc: 0.6792666666666667 - val_acc: 0.6956\n",
            "Epoch 38 / 100 \n",
            " - time: 1.4170386791229248 - sq_loss: 0.0033692926168441776 - tot_loss: 303.17382106482984 - loss_class: 22285.91796875 - acc: 0.6804666666666667 - val_acc: 0.6974\n",
            "Epoch 39 / 100 \n",
            " - time: 1.4088246822357178 - sq_loss: 0.0028844509273767472 - tot_loss: 305.00606231428685 - loss_class: 22282.388671875 - acc: 0.6817666666666666 - val_acc: 0.6986\n",
            "Epoch 40 / 100 \n",
            " - time: 1.4114432334899902 - sq_loss: 0.0025044243782758715 - tot_loss: 306.840466869995 - loss_class: 22278.94140625 - acc: 0.6830333333333334 - val_acc: 0.6998\n",
            "Epoch 41 / 100 \n",
            " - time: 1.443336009979248 - sq_loss: 0.0022026797756552695 - tot_loss: 308.6772251596675 - loss_class: 22275.57421875 - acc: 0.6839166666666666 - val_acc: 0.7013\n",
            "Epoch 42 / 100 \n",
            " - time: 1.4226806163787842 - sq_loss: 0.0019599709659814837 - tot_loss: 310.5165123973042 - loss_class: 22272.28125 - acc: 0.6849166666666666 - val_acc: 0.7031\n",
            "Epoch 43 / 100 \n",
            " - time: 1.4209463596343994 - sq_loss: 0.001762245036661625 - tot_loss: 312.35838729534305 - loss_class: 22269.0625 - acc: 0.6857833333333333 - val_acc: 0.7045\n",
            "Epoch 44 / 100 \n",
            " - time: 1.419344425201416 - sq_loss: 0.0015991467982530594 - tot_loss: 314.20290593989193 - loss_class: 22265.91015625 - acc: 0.6867833333333333 - val_acc: 0.7053\n",
            "Epoch 45 / 100 \n",
            " - time: 1.4190833568572998 - sq_loss: 0.0014629595912992956 - tot_loss: 316.04993037516255 - loss_class: 22262.822265625 - acc: 0.6875666666666667 - val_acc: 0.7057\n",
            "Epoch 46 / 100 \n",
            " - time: 1.4164221286773682 - sq_loss: 0.0013478946872055532 - tot_loss: 317.89938449980696 - loss_class: 22259.80078125 - acc: 0.6885166666666667 - val_acc: 0.7063\n",
            "Epoch 47 / 100 \n",
            " - time: 1.4064264297485352 - sq_loss: 0.0012495658360421658 - tot_loss: 319.751113860961 - loss_class: 22256.83984375 - acc: 0.6894166666666667 - val_acc: 0.7068\n",
            "Epoch 48 / 100 \n",
            " - time: 1.4176828861236572 - sq_loss: 0.0011646218597888946 - tot_loss: 321.60497301593426 - loss_class: 22253.9375 - acc: 0.6901 - val_acc: 0.7073\n",
            "Epoch 49 / 100 \n",
            " - time: 1.4227871894836426 - sq_loss: 0.00109048867598176 - tot_loss: 323.4607577093877 - loss_class: 22251.08984375 - acc: 0.6908 - val_acc: 0.7079\n",
            "Epoch 50 / 100 \n",
            " - time: 1.4324703216552734 - sq_loss: 0.0010251504369080068 - tot_loss: 325.31810793103654 - loss_class: 22248.302734375 - acc: 0.6915666666666667 - val_acc: 0.7084\n",
            "Epoch 51 / 100 \n",
            " - time: 1.4228665828704834 - sq_loss: 0.0009670373983681202 - tot_loss: 327.1767958668061 - loss_class: 22245.56640625 - acc: 0.692 - val_acc: 0.709\n",
            "Epoch 52 / 100 \n",
            " - time: 1.4379405975341797 - sq_loss: 0.0009149375371634961 - tot_loss: 329.03650990156456 - loss_class: 22242.8828125 - acc: 0.6928 - val_acc: 0.7098\n",
            "Epoch 53 / 100 \n",
            " - time: 1.4195771217346191 - sq_loss: 0.0008678740821778775 - tot_loss: 330.896999669727 - loss_class: 22240.244140625 - acc: 0.69325 - val_acc: 0.7104\n",
            "Epoch 54 / 100 \n",
            " - time: 1.4347355365753174 - sq_loss: 0.0008250677958130837 - tot_loss: 332.75792062822734 - loss_class: 22237.66015625 - acc: 0.69385 - val_acc: 0.7111\n",
            "Epoch 55 / 100 \n",
            " - time: 1.4248669147491455 - sq_loss: 0.0007858887314796449 - tot_loss: 334.6188616976142 - loss_class: 22235.119140625 - acc: 0.69455 - val_acc: 0.7115\n",
            "Epoch 56 / 100 \n",
            " - time: 1.4344120025634766 - sq_loss: 0.0007498251274228097 - tot_loss: 336.47954828608783 - loss_class: 22232.62890625 - acc: 0.6949 - val_acc: 0.7122\n",
            "Epoch 57 / 100 \n",
            " - time: 1.427779197692871 - sq_loss: 0.0007164519280195236 - tot_loss: 338.339571268484 - loss_class: 22230.177734375 - acc: 0.6953 - val_acc: 0.7126\n",
            "Epoch 58 / 100 \n",
            " - time: 1.4316356182098389 - sq_loss: 0.0006854459177702666 - tot_loss: 340.1986577763688 - loss_class: 22227.775390625 - acc: 0.6957833333333333 - val_acc: 0.7129\n",
            "Epoch 59 / 100 \n",
            " - time: 1.4235484600067139 - sq_loss: 0.000656526628881693 - tot_loss: 342.05645775729795 - loss_class: 22225.4140625 - acc: 0.6960833333333334 - val_acc: 0.7133\n",
            "Epoch 60 / 100 \n",
            " - time: 1.4358084201812744 - sq_loss: 0.000629458948969841 - tot_loss: 343.91252473406496 - loss_class: 22223.095703125 - acc: 0.6963666666666667 - val_acc: 0.7133\n",
            "Epoch 61 / 100 \n",
            " - time: 1.4316673278808594 - sq_loss: 0.0006040418054908515 - tot_loss: 345.766527547827 - loss_class: 22220.8125 - acc: 0.6967333333333333 - val_acc: 0.7138\n",
            "Epoch 62 / 100 \n",
            " - time: 1.422189474105835 - sq_loss: 0.0005801268853247166 - tot_loss: 347.6182520517148 - loss_class: 22218.572265625 - acc: 0.6973 - val_acc: 0.7141\n",
            "Epoch 63 / 100 \n",
            " - time: 1.434337854385376 - sq_loss: 0.0005575500428676606 - tot_loss: 349.4671895287931 - loss_class: 22216.3671875 - acc: 0.6978 - val_acc: 0.7139\n",
            "Epoch 64 / 100 \n",
            " - time: 1.4371466636657715 - sq_loss: 0.0005362046882510185 - tot_loss: 351.31307808291166 - loss_class: 22214.205078125 - acc: 0.69815 - val_acc: 0.7146\n",
            "Epoch 65 / 100 \n",
            " - time: 1.4402847290039062 - sq_loss: 0.0005159820429980755 - tot_loss: 353.15557082546877 - loss_class: 22212.07421875 - acc: 0.6986 - val_acc: 0.7147\n",
            "Epoch 66 / 100 \n",
            " - time: 1.4372775554656982 - sq_loss: 0.0004967983346432448 - tot_loss: 354.99433519798333 - loss_class: 22209.98046875 - acc: 0.699 - val_acc: 0.7152\n",
            "Epoch 67 / 100 \n",
            " - time: 1.437690258026123 - sq_loss: 0.0004785617347806692 - tot_loss: 356.82906359634364 - loss_class: 22207.921875 - acc: 0.69945 - val_acc: 0.7155\n",
            "Epoch 68 / 100 \n",
            " - time: 1.3952021598815918 - sq_loss: 0.00046120956540107727 - tot_loss: 358.65940894559026 - loss_class: 22205.89453125 - acc: 0.69975 - val_acc: 0.7157\n",
            "Epoch 69 / 100 \n",
            " - time: 1.4234356880187988 - sq_loss: 0.0004446863662451506 - tot_loss: 360.4850526809227 - loss_class: 22203.904296875 - acc: 0.70015 - val_acc: 0.716\n",
            "Epoch 70 / 100 \n",
            " - time: 1.4239797592163086 - sq_loss: 0.0004289291799068451 - tot_loss: 362.3057872258127 - loss_class: 22201.9453125 - acc: 0.7004333333333334 - val_acc: 0.716\n",
            "Epoch 71 / 100 \n",
            " - time: 1.4333550930023193 - sq_loss: 0.0004138872027397156 - tot_loss: 364.12122092098 - loss_class: 22200.015625 - acc: 0.7006666666666667 - val_acc: 0.7165\n",
            "Epoch 72 / 100 \n",
            " - time: 1.4355194568634033 - sq_loss: 0.00039952048100531104 - tot_loss: 365.93114278563297 - loss_class: 22198.119140625 - acc: 0.7011333333333334 - val_acc: 0.7171\n",
            "Epoch 73 / 100 \n",
            " - time: 1.4281880855560303 - sq_loss: 0.0003857866628095508 - tot_loss: 367.73517038978173 - loss_class: 22196.25 - acc: 0.70145 - val_acc: 0.7172\n",
            "Epoch 74 / 100 \n",
            " - time: 1.4376513957977295 - sq_loss: 0.0003726465627551079 - tot_loss: 369.53320199083544 - loss_class: 22194.41015625 - acc: 0.7018 - val_acc: 0.7171\n",
            "Epoch 75 / 100 \n",
            " - time: 1.4337854385375977 - sq_loss: 0.00036007505841553213 - tot_loss: 371.3248307917733 - loss_class: 22192.6015625 - acc: 0.70215 - val_acc: 0.7175\n",
            "Epoch 76 / 100 \n",
            " - time: 1.428617238998413 - sq_loss: 0.0003480319166556001 - tot_loss: 373.10987498203764 - loss_class: 22190.8203125 - acc: 0.70275 - val_acc: 0.7175\n",
            "Epoch 77 / 100 \n",
            " - time: 1.440802812576294 - sq_loss: 0.00033649455290287735 - tot_loss: 374.8880992079852 - loss_class: 22189.06640625 - acc: 0.7030333333333333 - val_acc: 0.7178\n",
            "Epoch 78 / 100 \n",
            " - time: 1.4327938556671143 - sq_loss: 0.000325424992479384 - tot_loss: 376.65929250868504 - loss_class: 22187.337890625 - acc: 0.7033 - val_acc: 0.718\n",
            "Epoch 79 / 100 \n",
            " - time: 1.4291307926177979 - sq_loss: 0.00031480181496590376 - tot_loss: 378.42310390460773 - loss_class: 22185.63671875 - acc: 0.7036333333333333 - val_acc: 0.7182\n",
            "Epoch 80 / 100 \n",
            " - time: 1.446373701095581 - sq_loss: 0.00030460637062788013 - tot_loss: 380.1795417575166 - loss_class: 22183.9609375 - acc: 0.7037166666666667 - val_acc: 0.7182\n",
            "Epoch 81 / 100 \n",
            " - time: 1.4406623840332031 - sq_loss: 0.0002948195673525334 - tot_loss: 381.9281897635199 - loss_class: 22182.3125 - acc: 0.7039666666666666 - val_acc: 0.7182\n",
            "Epoch 82 / 100 \n",
            " - time: 1.4240617752075195 - sq_loss: 0.0002854176796972752 - tot_loss: 383.6690224179067 - loss_class: 22180.6875 - acc: 0.7042333333333334 - val_acc: 0.7182\n",
            "Epoch 83 / 100 \n",
            " - time: 1.442772388458252 - sq_loss: 0.00027637262828648094 - tot_loss: 385.4016126431991 - loss_class: 22179.0859375 - acc: 0.7045333333333333 - val_acc: 0.7185\n",
            "Epoch 84 / 100 \n",
            " - time: 1.4297363758087158 - sq_loss: 0.0002676736796274781 - tot_loss: 387.12610826606397 - loss_class: 22177.509765625 - acc: 0.7046833333333333 - val_acc: 0.7193\n",
            "Epoch 85 / 100 \n",
            " - time: 1.4338319301605225 - sq_loss: 0.00025930292904376986 - tot_loss: 388.84199328310785 - loss_class: 22175.95703125 - acc: 0.70475 - val_acc: 0.7194\n",
            "Epoch 86 / 100 \n",
            " - time: 1.4370050430297852 - sq_loss: 0.00025124584790319203 - tot_loss: 390.549339938513 - loss_class: 22174.423828125 - acc: 0.7049833333333333 - val_acc: 0.7198\n",
            "Epoch 87 / 100 \n",
            " - time: 1.4391062259674072 - sq_loss: 0.00024349605664610865 - tot_loss: 392.24793610488996 - loss_class: 22172.9140625 - acc: 0.7052333333333334 - val_acc: 0.7198\n",
            "Epoch 88 / 100 \n",
            " - time: 1.441702127456665 - sq_loss: 0.0002360418438911438 - tot_loss: 393.93762130290276 - loss_class: 22171.427734375 - acc: 0.7054833333333334 - val_acc: 0.7199\n",
            "Epoch 89 / 100 \n",
            " - time: 1.4345781803131104 - sq_loss: 0.00022886169608682394 - tot_loss: 395.6181860007113 - loss_class: 22169.96484375 - acc: 0.7055166666666667 - val_acc: 0.72\n",
            "Epoch 90 / 100 \n",
            " - time: 1.4399254322052002 - sq_loss: 0.0002219398505985737 - tot_loss: 397.28959859209135 - loss_class: 22168.517578125 - acc: 0.7057333333333333 - val_acc: 0.7201\n",
            "Epoch 91 / 100 \n",
            " - time: 1.4427876472473145 - sq_loss: 0.0002152666449546814 - tot_loss: 398.9516727551818 - loss_class: 22167.09375 - acc: 0.7059833333333333 - val_acc: 0.7204\n",
            "Epoch 92 / 100 \n",
            " - time: 1.4380462169647217 - sq_loss: 0.00020882759708911182 - tot_loss: 400.6043683615746 - loss_class: 22165.69140625 - acc: 0.7061 - val_acc: 0.7203\n",
            "Epoch 93 / 100 \n",
            " - time: 1.4438667297363281 - sq_loss: 0.00020261267200112344 - tot_loss: 402.2474141643383 - loss_class: 22164.30859375 - acc: 0.7063 - val_acc: 0.7207\n",
            "Epoch 94 / 100 \n",
            " - time: 1.4473209381103516 - sq_loss: 0.00019661847036331893 - tot_loss: 403.8807848280995 - loss_class: 22162.9453125 - acc: 0.7064666666666667 - val_acc: 0.7209\n",
            "Epoch 95 / 100 \n",
            " - time: 1.438553810119629 - sq_loss: 0.00019083634251728656 - tot_loss: 405.50442152634497 - loss_class: 22161.599609375 - acc: 0.70675 - val_acc: 0.7213\n",
            "Epoch 96 / 100 \n",
            " - time: 1.4445040225982666 - sq_loss: 0.0001852583372965455 - tot_loss: 407.1181544970954 - loss_class: 22160.2734375 - acc: 0.7069166666666666 - val_acc: 0.7213\n",
            "Epoch 97 / 100 \n",
            " - time: 1.4367396831512451 - sq_loss: 0.00017987388418987395 - tot_loss: 408.72200188535504 - loss_class: 22158.966796875 - acc: 0.7070833333333333 - val_acc: 0.7215\n",
            "Epoch 98 / 100 \n",
            " - time: 1.4417567253112793 - sq_loss: 0.00017466858262196185 - tot_loss: 410.3157224567258 - loss_class: 22157.67578125 - acc: 0.7073 - val_acc: 0.7216\n",
            "Epoch 99 / 100 \n",
            " - time: 1.437169075012207 - sq_loss: 0.00016964352689683438 - tot_loss: 411.8993901935872 - loss_class: 22156.40234375 - acc: 0.7075 - val_acc: 0.7217\n",
            "Epoch 100 / 100 \n",
            " - time: 1.438662052154541 - sq_loss: 0.00016478592297062278 - tot_loss: 413.47274744802854 - loss_class: 22155.1484375 - acc: 0.7076333333333333 - val_acc: 0.7218\n",
            "The total time spent is: 141.799001455307 s\n",
            "\n",
            "\n",
            "\n",
            "Early stopping accuracy: 0.7218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_,_,_,_,_,_,_ = execute_training([[\"Perceptron\",37,37],[\"Perceptron\",35,35]], input_size, hidden_size, output_size, x_train, x_test, y_train, y_test,y_train_one_hot,y_test_one_hot,\n",
        "                                         False, niter = 100, gamma = 0.1, alpha = 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI0WNdcW9J_Y",
        "outputId": "d8fb1e36-cd47-4350-84ef-d676c752e0bf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer  0  W  torch.Size([1600, 784])  Layer W  torch.Size([1600, 784])\n",
            "['Perceptron']\n",
            "Layer  1  W  torch.Size([1369, 1600])  Layer W  torch.Size([1369, 1600])\n",
            "['Perceptron']\n",
            "Layer  2  W  torch.Size([1225, 1369])  Layer W  torch.Size([1225, 1369])\n",
            "['Perceptron']\n",
            "35 35\n",
            "torch.Size([10, 1225]) torch.Size([1225, 60000])\n",
            "Layer  0  W  torch.Size([1600, 1])  Layer W  torch.Size([1600, 1])\n",
            "['Perceptron']\n",
            "Layer  1  W  torch.Size([1369, 1])  Layer W  torch.Size([1369, 1])\n",
            "['Perceptron']\n",
            "Layer  2  W  torch.Size([1225, 1])  Layer W  torch.Size([1225, 1])\n",
            "['Perceptron']\n",
            "Layer  3  W  torch.Size([10, 1])  Layer W  torch.Size([10, 1])\n",
            "['Perceptron', 10]\n",
            "4 4\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1 / 100 \n",
            " - time: 2.2707924842834473 - sq_loss: 1980.4478515625 - tot_loss: 1980.4479200002836 - loss_class: 22580.66796875 - acc: 0.7233666666666667 - val_acc: 0.7354\n",
            "Epoch 2 / 100 \n",
            " - time: 2.3186275959014893 - sq_loss: 1287.5687500000001 - tot_loss: 1287.5688229401585 - loss_class: 21957.033203125 - acc: 0.86305 - val_acc: 0.8774\n",
            "Epoch 3 / 100 \n",
            " - time: 2.312398910522461 - sq_loss: 840.7740234375001 - tot_loss: 840.7741044609519 - loss_class: 21306.74609375 - acc: 0.8827666666666667 - val_acc: 0.8921\n",
            "Epoch 4 / 100 \n",
            " - time: 2.3103275299072266 - sq_loss: 550.543115234375 - tot_loss: 550.5432033896839 - loss_class: 20715.74609375 - acc: 0.8886666666666667 - val_acc: 0.8973\n",
            "Epoch 5 / 100 \n",
            " - time: 2.3132081031799316 - sq_loss: 361.0743408203125 - tot_loss: 361.07444338366764 - loss_class: 20206.5234375 - acc: 0.8912333333333333 - val_acc: 0.8997\n",
            "Epoch 6 / 100 \n",
            " - time: 2.313089370727539 - sq_loss: 237.04484863281252 - tot_loss: 237.0449671387149 - loss_class: 19777.47265625 - acc: 0.8935 - val_acc: 0.9021\n",
            "Epoch 7 / 100 \n",
            " - time: 2.3347229957580566 - sq_loss: 155.72476806640626 - tot_loss: 155.72491732599448 - loss_class: 19418.55078125 - acc: 0.8957333333333334 - val_acc: 0.9033\n",
            "Epoch 8 / 100 \n",
            " - time: 2.3273017406463623 - sq_loss: 102.35665283203126 - tot_loss: 102.35684258235852 - loss_class: 19118.86328125 - acc: 0.8979333333333334 - val_acc: 0.9051\n",
            "Epoch 9 / 100 \n",
            " - time: 2.3318588733673096 - sq_loss: 67.31139526367188 - tot_loss: 67.31164585578955 - loss_class: 18867.6796875 - acc: 0.8996 - val_acc: 0.9069\n",
            "Epoch 10 / 100 \n",
            " - time: 2.319495439529419 - sq_loss: 44.288125610351564 - tot_loss: 44.288456850219525 - loss_class: 18656.537109375 - acc: 0.9012166666666667 - val_acc: 0.9082\n",
            "Epoch 11 / 100 \n",
            " - time: 2.330538511276245 - sq_loss: 29.157061767578128 - tot_loss: 29.157498563011178 - loss_class: 18477.4453125 - acc: 0.90295 - val_acc: 0.9101\n",
            "Epoch 12 / 100 \n",
            " - time: 2.3318281173706055 - sq_loss: 19.20897674560547 - tot_loss: 19.20954926755512 - loss_class: 18325.1328125 - acc: 0.9045333333333333 - val_acc: 0.9113\n",
            "Epoch 13 / 100 \n",
            " - time: 2.3213605880737305 - sq_loss: 12.665672302246094 - tot_loss: 12.66641041074181 - loss_class: 18194.078125 - acc: 0.9061 - val_acc: 0.913\n",
            "Epoch 14 / 100 \n",
            " - time: 2.324504852294922 - sq_loss: 8.35965576171875 - tot_loss: 8.360599008493592 - loss_class: 18080.669921875 - acc: 0.9079166666666667 - val_acc: 0.9147\n",
            "Epoch 15 / 100 \n",
            " - time: 2.302464485168457 - sq_loss: 5.52423210144043 - tot_loss: 5.525419048091862 - loss_class: 17981.6796875 - acc: 0.90995 - val_acc: 0.9162\n",
            "Epoch 16 / 100 \n",
            " - time: 2.317882537841797 - sq_loss: 3.655812454223633 - tot_loss: 3.6572913349606098 - loss_class: 17894.734375 - acc: 0.9112 - val_acc: 0.9179\n",
            "Epoch 17 / 100 \n",
            " - time: 2.337097644805908 - sq_loss: 2.4235401153564453 - tot_loss: 2.425352474488318 - loss_class: 17817.28515625 - acc: 0.9129333333333334 - val_acc: 0.9188\n",
            "Epoch 18 / 100 \n",
            " - time: 2.3365108966827393 - sq_loss: 1.6099828720092775 - tot_loss: 1.6121862346306444 - loss_class: 17748.001953125 - acc: 0.9143666666666667 - val_acc: 0.9201\n",
            "Epoch 19 / 100 \n",
            " - time: 2.3180902004241943 - sq_loss: 1.0722038269042968 - tot_loss: 1.074852591776289 - loss_class: 17685.6015625 - acc: 0.9156666666666666 - val_acc: 0.9215\n",
            "Epoch 20 / 100 \n",
            " - time: 2.3397579193115234 - sq_loss: 0.7161995887756348 - tot_loss: 0.719347007723991 - loss_class: 17628.529296875 - acc: 0.9172333333333333 - val_acc: 0.9224\n",
            "Epoch 21 / 100 \n",
            " - time: 2.337134838104248 - sq_loss: 0.4801194667816162 - tot_loss: 0.4838313461281359 - loss_class: 17576.65625 - acc: 0.9188166666666666 - val_acc: 0.9239\n",
            "Epoch 22 / 100 \n",
            " - time: 2.331214427947998 - sq_loss: 0.32324507236480715 - tot_loss: 0.3275778535753489 - loss_class: 17528.048828125 - acc: 0.9202833333333333 - val_acc: 0.9246\n",
            "Epoch 23 / 100 \n",
            " - time: 2.3320438861846924 - sq_loss: 0.21875014305114748 - tot_loss: 0.223768010083586 - loss_class: 17483.517578125 - acc: 0.9214833333333333 - val_acc: 0.9256\n",
            "Epoch 24 / 100 \n",
            " - time: 2.3321375846862793 - sq_loss: 0.14894764423370363 - tot_loss: 0.15471142319729553 - loss_class: 17441.333984375 - acc: 0.9228833333333334 - val_acc: 0.9269\n",
            "Epoch 25 / 100 \n",
            " - time: 2.3329756259918213 - sq_loss: 0.10216369628906251 - tot_loss: 0.10874396564904601 - loss_class: 17401.998046875 - acc: 0.9239333333333334 - val_acc: 0.9274\n",
            "Epoch 26 / 100 \n",
            " - time: 2.3304696083068848 - sq_loss: 0.07068480253219604 - tot_loss: 0.07814313574926927 - loss_class: 17364.529296875 - acc: 0.9254166666666667 - val_acc: 0.9282\n",
            "Epoch 27 / 100 \n",
            " - time: 2.334277629852295 - sq_loss: 0.04940750598907471 - tot_loss: 0.05781321360263974 - loss_class: 17329.111328125 - acc: 0.9264666666666667 - val_acc: 0.929\n",
            "Epoch 28 / 100 \n",
            " - time: 2.3424251079559326 - sq_loss: 0.03494933247566223 - tot_loss: 0.04436908528441563 - loss_class: 17295.0859375 - acc: 0.9274333333333333 - val_acc: 0.9301\n",
            "Epoch 29 / 100 \n",
            " - time: 2.3245418071746826 - sq_loss: 0.025064706802368164 - tot_loss: 0.03556691519916058 - loss_class: 17262.71875 - acc: 0.9281166666666667 - val_acc: 0.9312\n",
            "Epoch 30 / 100 \n",
            " - time: 2.333263874053955 - sq_loss: 0.01825951337814331 - tot_loss: 0.02991024226648733 - loss_class: 17231.54296875 - acc: 0.9291166666666667 - val_acc: 0.9318\n",
            "Epoch 31 / 100 \n",
            " - time: 2.331209659576416 - sq_loss: 0.01353684663772583 - tot_loss: 0.026408090116456152 - loss_class: 17201.51953125 - acc: 0.9298833333333333 - val_acc: 0.9321\n",
            "Epoch 32 / 100 \n",
            " - time: 2.3364531993865967 - sq_loss: 0.010229772329330445 - tot_loss: 0.02438275234308094 - loss_class: 17172.5703125 - acc: 0.9311 - val_acc: 0.9329\n",
            "Epoch 33 / 100 \n",
            " - time: 2.3305466175079346 - sq_loss: 0.007890377938747407 - tot_loss: 0.023392453370615844 - loss_class: 17144.458984375 - acc: 0.9317166666666666 - val_acc: 0.9335\n",
            "Epoch 34 / 100 \n",
            " - time: 2.3400840759277344 - sq_loss: 0.006216881424188614 - tot_loss: 0.023132769973017275 - loss_class: 17117.541015625 - acc: 0.9325333333333333 - val_acc: 0.9345\n",
            "Epoch 35 / 100 \n",
            " - time: 2.329035520553589 - sq_loss: 0.0050048988312482836 - tot_loss: 0.02340107122436166 - loss_class: 17090.90234375 - acc: 0.9332666666666667 - val_acc: 0.9359\n",
            "Epoch 36 / 100 \n",
            " - time: 2.3220298290252686 - sq_loss: 0.0041154116392135625 - tot_loss: 0.024052162095904352 - loss_class: 17065.6484375 - acc: 0.93395 - val_acc: 0.9364\n",
            "Epoch 37 / 100 \n",
            " - time: 2.3233704566955566 - sq_loss: 0.0034532759338617327 - tot_loss: 0.024988609808497132 - loss_class: 17040.689453125 - acc: 0.9345833333333333 - val_acc: 0.9366\n",
            "Epoch 38 / 100 \n",
            " - time: 2.3193018436431885 - sq_loss: 0.0029529113322496418 - tot_loss: 0.026146055408753456 - loss_class: 17016.39453125 - acc: 0.9352833333333334 - val_acc: 0.9368\n",
            "Epoch 39 / 100 \n",
            " - time: 2.354682445526123 - sq_loss: 0.0025688502937555313 - tot_loss: 0.027479669358581305 - loss_class: 16993.076171875 - acc: 0.9358833333333333 - val_acc: 0.9373\n",
            "Epoch 40 / 100 \n",
            " - time: 2.33547306060791 - sq_loss: 0.0022693324834108354 - tot_loss: 0.028953792573884134 - loss_class: 16970.078125 - acc: 0.9366166666666667 - val_acc: 0.9377\n",
            "Epoch 41 / 100 \n",
            " - time: 2.3249118328094482 - sq_loss: 0.002031967230141163 - tot_loss: 0.03054337021894753 - loss_class: 16947.6875 - acc: 0.9372166666666667 - val_acc: 0.9378\n",
            "Epoch 42 / 100 \n",
            " - time: 2.3384177684783936 - sq_loss: 0.0018408069387078287 - tot_loss: 0.03222882035188377 - loss_class: 16925.88671875 - acc: 0.9376833333333333 - val_acc: 0.9383\n",
            "Epoch 43 / 100 \n",
            " - time: 2.3329265117645264 - sq_loss: 0.0016844430938363075 - tot_loss: 0.03399877236224711 - loss_class: 16904.703125 - acc: 0.9381833333333334 - val_acc: 0.9393\n",
            "Epoch 44 / 100 \n",
            " - time: 2.336449146270752 - sq_loss: 0.0015545869246125221 - tot_loss: 0.03583787675015629 - loss_class: 16883.74609375 - acc: 0.9388166666666666 - val_acc: 0.9398\n",
            "Epoch 45 / 100 \n",
            " - time: 2.33060884475708 - sq_loss: 0.0014451862312853338 - tot_loss: 0.03774501057341695 - loss_class: 16863.953125 - acc: 0.9395833333333333 - val_acc: 0.9403\n",
            "Epoch 46 / 100 \n",
            " - time: 2.3437745571136475 - sq_loss: 0.001351758185774088 - tot_loss: 0.03970898371189833 - loss_class: 16843.22265625 - acc: 0.9400666666666667 - val_acc: 0.9413\n",
            "Epoch 47 / 100 \n",
            " - time: 2.335552930831909 - sq_loss: 0.001270962692797184 - tot_loss: 0.04172467994503677 - loss_class: 16824.90625 - acc: 0.9404333333333333 - val_acc: 0.9416\n",
            "Epoch 48 / 100 \n",
            " - time: 2.344646692276001 - sq_loss: 0.0012002658098936082 - tot_loss: 0.043785656848922376 - loss_class: 16805.12890625 - acc: 0.9409 - val_acc: 0.9416\n",
            "Epoch 49 / 100 \n",
            " - time: 2.347499370574951 - sq_loss: 0.0011377451941370965 - tot_loss: 0.04589372128248215 - loss_class: 16787.48046875 - acc: 0.9414166666666667 - val_acc: 0.9421\n",
            "Epoch 50 / 100 \n",
            " - time: 2.347087860107422 - sq_loss: 0.001081926841288805 - tot_loss: 0.04803747907280922 - loss_class: 16768.6953125 - acc: 0.94185 - val_acc: 0.9427\n",
            "Epoch 51 / 100 \n",
            " - time: 2.346013307571411 - sq_loss: 0.0010316549800336362 - tot_loss: 0.050215889699757106 - loss_class: 16752.1171875 - acc: 0.9421833333333334 - val_acc: 0.9429\n",
            "Epoch 52 / 100 \n",
            " - time: 2.344043016433716 - sq_loss: 0.0009860320016741753 - tot_loss: 0.0524245897307992 - loss_class: 16733.724609375 - acc: 0.94255 - val_acc: 0.9432\n",
            "Epoch 53 / 100 \n",
            " - time: 2.347412347793579 - sq_loss: 0.0009443414397537708 - tot_loss: 0.05466623203828931 - loss_class: 16718.12109375 - acc: 0.9429666666666666 - val_acc: 0.9438\n",
            "Epoch 54 / 100 \n",
            " - time: 2.3554892539978027 - sq_loss: 0.0009060062468051911 - tot_loss: 0.05692956065759063 - loss_class: 16700.31640625 - acc: 0.9434 - val_acc: 0.9437\n",
            "Epoch 55 / 100 \n",
            " - time: 2.3472845554351807 - sq_loss: 0.0008705739863216878 - tot_loss: 0.05922175198793412 - loss_class: 16685.7265625 - acc: 0.944 - val_acc: 0.944\n",
            "Epoch 56 / 100 \n",
            " - time: 2.33998441696167 - sq_loss: 0.0008376735262572766 - tot_loss: 0.061528851836919786 - loss_class: 16668.2734375 - acc: 0.9445166666666667 - val_acc: 0.944\n",
            "Epoch 57 / 100 \n",
            " - time: 2.349609375 - sq_loss: 0.0008069866336882115 - tot_loss: 0.06385678406804801 - loss_class: 16654.51953125 - acc: 0.9450333333333333 - val_acc: 0.9443\n",
            "Epoch 58 / 100 \n",
            " - time: 2.3544342517852783 - sq_loss: 0.0007782669272273779 - tot_loss: 0.06619780627079308 - loss_class: 16638.11328125 - acc: 0.9454 - val_acc: 0.9446\n",
            "Epoch 59 / 100 \n",
            " - time: 2.3591525554656982 - sq_loss: 0.0007513008546084166 - tot_loss: 0.06855397601611912 - loss_class: 16624.548828125 - acc: 0.9458666666666666 - val_acc: 0.9452\n",
            "Epoch 60 / 100 \n",
            " - time: 2.3556766510009766 - sq_loss: 0.0007259085308760405 - tot_loss: 0.07091943831183017 - loss_class: 16608.92578125 - acc: 0.9462 - val_acc: 0.9455\n",
            "Epoch 61 / 100 \n",
            " - time: 2.354149580001831 - sq_loss: 0.0007019426673650742 - tot_loss: 0.07329283505678177 - loss_class: 16596.05078125 - acc: 0.9465666666666667 - val_acc: 0.9453\n",
            "Epoch 62 / 100 \n",
            " - time: 2.3469791412353516 - sq_loss: 0.0006792690139263869 - tot_loss: 0.07567360042594373 - loss_class: 16580.92578125 - acc: 0.94685 - val_acc: 0.9458\n",
            "Epoch 63 / 100 \n",
            " - time: 2.3563568592071533 - sq_loss: 0.0006577799562364818 - tot_loss: 0.07806637850590051 - loss_class: 16569.08203125 - acc: 0.94735 - val_acc: 0.9461\n",
            "Epoch 64 / 100 \n",
            " - time: 2.3557279109954834 - sq_loss: 0.0006373767275363207 - tot_loss: 0.08045398541726173 - loss_class: 16554.12109375 - acc: 0.9478 - val_acc: 0.9465\n",
            "Epoch 65 / 100 \n",
            " - time: 2.3487799167633057 - sq_loss: 0.0006179697811603546 - tot_loss: 0.08284987658262254 - loss_class: 16542.783203125 - acc: 0.9480166666666666 - val_acc: 0.9467\n",
            "Epoch 66 / 100 \n",
            " - time: 2.3583874702453613 - sq_loss: 0.0005994900595396758 - tot_loss: 0.0852374381851405 - loss_class: 16528.7265625 - acc: 0.9483666666666667 - val_acc: 0.9471\n",
            "Epoch 67 / 100 \n",
            " - time: 2.3503146171569824 - sq_loss: 0.0005818743724375963 - tot_loss: 0.08763158884830773 - loss_class: 16518.15234375 - acc: 0.9487833333333333 - val_acc: 0.9473\n",
            "Epoch 68 / 100 \n",
            " - time: 2.3534603118896484 - sq_loss: 0.0005650595296174288 - tot_loss: 0.09001422696746886 - loss_class: 16504.046875 - acc: 0.949 - val_acc: 0.9471\n",
            "Epoch 69 / 100 \n",
            " - time: 2.3544185161590576 - sq_loss: 0.0005489953793585301 - tot_loss: 0.09240073068067432 - loss_class: 16494.05078125 - acc: 0.9493833333333334 - val_acc: 0.9479\n",
            "Epoch 70 / 100 \n",
            " - time: 2.3524417877197266 - sq_loss: 0.0005336272064596414 - tot_loss: 0.09477207497693599 - loss_class: 16481.0546875 - acc: 0.94965 - val_acc: 0.9482\n",
            "Epoch 71 / 100 \n",
            " - time: 2.3531692028045654 - sq_loss: 0.000518918875604868 - tot_loss: 0.09714300753548742 - loss_class: 16471.0 - acc: 0.9498666666666666 - val_acc: 0.9481\n",
            "Epoch 72 / 100 \n",
            " - time: 2.358030080795288 - sq_loss: 0.0005048279650509358 - tot_loss: 0.09950577775016428 - loss_class: 16458.50390625 - acc: 0.9502 - val_acc: 0.9484\n",
            "Epoch 73 / 100 \n",
            " - time: 2.3494272232055664 - sq_loss: 0.0004913145676255226 - tot_loss: 0.101859237998724 - loss_class: 16448.484375 - acc: 0.9505166666666667 - val_acc: 0.9485\n",
            "Epoch 74 / 100 \n",
            " - time: 2.3585262298583984 - sq_loss: 0.00047834762372076515 - tot_loss: 0.10419849199242891 - loss_class: 16437.603515625 - acc: 0.9508833333333333 - val_acc: 0.9486\n",
            "Epoch 75 / 100 \n",
            " - time: 2.3609249591827393 - sq_loss: 0.00046589849516749383 - tot_loss: 0.10652983719483018 - loss_class: 16427.47265625 - acc: 0.9510666666666666 - val_acc: 0.9486\n",
            "Epoch 76 / 100 \n",
            " - time: 2.3503048419952393 - sq_loss: 0.00045393966138362885 - tot_loss: 0.10884621739387512 - loss_class: 16416.9140625 - acc: 0.9512333333333334 - val_acc: 0.9486\n",
            "Epoch 77 / 100 \n",
            " - time: 2.3533406257629395 - sq_loss: 0.00044244346208870415 - tot_loss: 0.11114889760501684 - loss_class: 16407.1953125 - acc: 0.9513666666666667 - val_acc: 0.9487\n",
            "Epoch 78 / 100 \n",
            " - time: 2.353410243988037 - sq_loss: 0.00043138656765222554 - tot_loss: 0.11343945730477573 - loss_class: 16396.775390625 - acc: 0.9515833333333333 - val_acc: 0.9487\n",
            "Epoch 79 / 100 \n",
            " - time: 2.3638415336608887 - sq_loss: 0.00042074164375662806 - tot_loss: 0.11571960290893912 - loss_class: 16388.40625 - acc: 0.9518333333333333 - val_acc: 0.9489\n",
            "Epoch 80 / 100 \n",
            " - time: 2.3525502681732178 - sq_loss: 0.00041049201972782613 - tot_loss: 0.11797427837736905 - loss_class: 16378.22265625 - acc: 0.9521 - val_acc: 0.9489\n",
            "Epoch 81 / 100 \n",
            " - time: 2.3506011962890625 - sq_loss: 0.0004006190225481987 - tot_loss: 0.1202200410887599 - loss_class: 16369.4931640625 - acc: 0.9522 - val_acc: 0.9492\n",
            "Epoch 82 / 100 \n",
            " - time: 2.3542542457580566 - sq_loss: 0.0003911075182259083 - tot_loss: 0.12244496801868082 - loss_class: 16359.3427734375 - acc: 0.9524166666666667 - val_acc: 0.9493\n",
            "Epoch 83 / 100 \n",
            " - time: 2.348924398422241 - sq_loss: 0.0003819353645667434 - tot_loss: 0.12466141504701227 - loss_class: 16352.080078125 - acc: 0.9525833333333333 - val_acc: 0.9497\n",
            "Epoch 84 / 100 \n",
            " - time: 2.3508713245391846 - sq_loss: 0.00037308819591999056 - tot_loss: 0.1268515631556511 - loss_class: 16341.607421875 - acc: 0.9528 - val_acc: 0.9498\n",
            "Epoch 85 / 100 \n",
            " - time: 2.3637428283691406 - sq_loss: 0.0003645505988970399 - tot_loss: 0.12902684027794747 - loss_class: 16335.2685546875 - acc: 0.953 - val_acc: 0.9498\n",
            "Epoch 86 / 100 \n",
            " - time: 2.3562793731689453 - sq_loss: 0.00035630930215120317 - tot_loss: 0.13118189629167318 - loss_class: 16324.6494140625 - acc: 0.9531666666666667 - val_acc: 0.9502\n",
            "Epoch 87 / 100 \n",
            " - time: 2.354830503463745 - sq_loss: 0.00034834714606404307 - tot_loss: 0.13332283580675722 - loss_class: 16318.9990234375 - acc: 0.9533666666666667 - val_acc: 0.9501\n",
            "Epoch 88 / 100 \n",
            " - time: 2.3589982986450195 - sq_loss: 0.0003406522795557976 - tot_loss: 0.135438315756619 - loss_class: 16308.4521484375 - acc: 0.9535166666666667 - val_acc: 0.9503\n",
            "Epoch 89 / 100 \n",
            " - time: 2.3614308834075928 - sq_loss: 0.00033321699593216183 - tot_loss: 0.13754235266242176 - loss_class: 16303.35546875 - acc: 0.9537666666666667 - val_acc: 0.9505\n",
            "Epoch 90 / 100 \n",
            " - time: 2.3519697189331055 - sq_loss: 0.00032602725550532344 - tot_loss: 0.13962092949077487 - loss_class: 16293.09765625 - acc: 0.95385 - val_acc: 0.9506\n",
            "Epoch 91 / 100 \n",
            " - time: 2.3512113094329834 - sq_loss: 0.00031907313968986277 - tot_loss: 0.14168740452732892 - loss_class: 16287.865234375 - acc: 0.9541333333333334 - val_acc: 0.9508\n",
            "Epoch 92 / 100 \n",
            " - time: 2.3644490242004395 - sq_loss: 0.0003123434027656913 - tot_loss: 0.14372824260499328 - loss_class: 16278.486328125 - acc: 0.9541833333333334 - val_acc: 0.9509\n",
            "Epoch 93 / 100 \n",
            " - time: 2.361562490463257 - sq_loss: 0.00030582991894334555 - tot_loss: 0.14575552379246803 - loss_class: 16273.7900390625 - acc: 0.9544333333333334 - val_acc: 0.9511\n",
            "Epoch 94 / 100 \n",
            " - time: 2.3529040813446045 - sq_loss: 0.00029952183831483126 - tot_loss: 0.14775337458122523 - loss_class: 16263.55859375 - acc: 0.9545666666666667 - val_acc: 0.9511\n",
            "Epoch 95 / 100 \n",
            " - time: 2.363555669784546 - sq_loss: 0.0002934129908680916 - tot_loss: 0.1497425278648734 - loss_class: 16260.3037109375 - acc: 0.9548 - val_acc: 0.9512\n",
            "Epoch 96 / 100 \n",
            " - time: 2.3579893112182617 - sq_loss: 0.0002874930389225483 - tot_loss: 0.15170162143185736 - loss_class: 16249.755859375 - acc: 0.9548833333333333 - val_acc: 0.9514\n",
            "Epoch 97 / 100 \n",
            " - time: 2.347597599029541 - sq_loss: 0.00028175578918308023 - tot_loss: 0.1536516589811072 - loss_class: 16246.94921875 - acc: 0.95495 - val_acc: 0.9514\n",
            "Epoch 98 / 100 \n",
            " - time: 2.345649003982544 - sq_loss: 0.00027619556058198217 - tot_loss: 0.1555738735245541 - loss_class: 16236.623046875 - acc: 0.95515 - val_acc: 0.9516\n",
            "Epoch 99 / 100 \n",
            " - time: 2.360081434249878 - sq_loss: 0.0002708034357056022 - tot_loss: 0.15748375297989697 - loss_class: 16233.982421875 - acc: 0.9552666666666667 - val_acc: 0.9516\n",
            "Epoch 100 / 100 \n",
            " - time: 2.3590261936187744 - sq_loss: 0.00026557259261608126 - tot_loss: 0.15935855247080327 - loss_class: 16223.748046875 - acc: 0.95535 - val_acc: 0.9515\n",
            "The total time spent is: 234.14597535133362 s\n",
            "\n",
            "\n",
            "\n",
            "Early stopping accuracy: 0.9516\n"
          ]
        }
      ]
    }
  ]
}