{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Block_coordinate_descent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjmJ5IsZInkj",
        "outputId": "f455b802-ef96-4b58-f31c-0bb0e573f94b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 1.11.0+cu113\n",
            "Torchvision Version: 0.12.0+cu113\n",
            "GPU is available? True\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "\n",
        "from utilities import *\n",
        "\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "print(\"Torchvision Version:\", torchvision.__version__)\n",
        "print(\"GPU is available?\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "DT-roPj1Isd7"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imported datasets\n",
        "For the testing and comparison of our algorithms we will use the following datasets:\n",
        "\n",
        "1. MNIST\n",
        "2. FashionMNIST\n",
        "3. CIFAR10"
      ],
      "metadata": {
        "id": "N_1aD5CdJCfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0,), (1,))])\n",
        "\n",
        "# change the flag to choose the dataset to work with\n",
        "dataset_flag = 1\n",
        "\n",
        "if dataset_flag ==0:\n",
        "  trainset = datasets.MNIST('../data', train=True, download=True, transform=ts)\n",
        "  testset = datasets.MNIST(root='../data', train=False, download=True, transform=ts)\n",
        "elif dataset_flag ==1:\n",
        "  trainset = datasets.FashionMNIST('../data', train=True, download=True, transform=ts)\n",
        "  testset = datasets.FashionMNIST(root='../data', train=False, download=True, transform=ts)\n",
        "else:\n",
        "  trainset = datasets.CIFAR10('../data', train=True, download=True, transform=ts)\n",
        "  testset = datasets.CIFAR10(root='../data', train=False, download=True, transform=ts)"
      ],
      "metadata": {
        "id": "FCpRsZlPIuew"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "boston = datasets.load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "x_train = torch.from_numpy(x_train).to(device = device)\n",
        "x_test = torch.from_numpy(x_test).to(device = device)\n",
        "y_train = torch.from_numpy(y_train).to(device = device)\n",
        "y_test = torch.from_numpy(y_test).to(device = device)\n",
        "\n",
        "x_train = x_train.T.float()\n",
        "x_test = x_test.T.float()\n",
        "\n",
        "N = x_train.shape[1]\n",
        "N_test = x_test.shape[1]\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "-oRYcSE2I27L",
        "outputId": "ec6969c2-04ff-43e6-b138-b06dfbefdd8d"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom sklearn import datasets\\nfrom sklearn.model_selection import train_test_split\\n\\nboston = datasets.load_boston()\\nX = boston.data\\ny = boston.target\\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\\n\\nx_train = torch.from_numpy(x_train).to(device = device)\\nx_test = torch.from_numpy(x_test).to(device = device)\\ny_train = torch.from_numpy(y_train).to(device = device)\\ny_test = torch.from_numpy(y_test).to(device = device)\\n\\nx_train = x_train.T.float()\\nx_test = x_test.T.float()\\n\\nN = x_train.shape[1]\\nN_test = x_test.shape[1]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset preprocessing"
      ],
      "metadata": {
        "id": "dtekIhnJJJhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_test, y_test, y_train_one_hot, y_test_one_hot= load_dataset(trainset, testset, 10)\n",
        "\n",
        "# We move to GPU\n",
        "x_train = x_train.to(device = device)\n",
        "x_test = x_test.to(device = device)\n",
        "y_train = y_train.to(device = device)\n",
        "y_test = y_test.to(device = device)\n",
        "y_train_one_hot = y_train_one_hot.to(device = device)\n",
        "y_test_one_hot = y_test_one_hot.to(device = device)"
      ],
      "metadata": {
        "id": "GJ9kefhFI6vI"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture initialization\n",
        "\n",
        "For the MultiLayerPerceptron we have the parameters **input_size** , **hidden_size**,**output_size** corresponding to the size of the input layer, the hidden layer and the output layer, respectively.\n",
        "\n",
        "The MLP only has 3 layers like https://github.com/timlautk/BCD-for-DNNs-PyTorch/blob/master/bcd_dnn_mlp_mnist.ipynb as a starting point.\n",
        "\n",
        "Also we use ReLU currently for the same reason."
      ],
      "metadata": {
        "id": "w7M3uSc8JRdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = x_train.shape[0]\n",
        "hidden_size = 1500\n",
        "output_size = 10"
      ],
      "metadata": {
        "id": "QiyJP8y2I81R"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "Note: Fix it so that it moves everything to device in the following function and that it does the label sample split here"
      ],
      "metadata": {
        "id": "QAm_re0mJbOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_v_js(U1, U2, W, b, rho, gamma):\n",
        "    \"\"\"\n",
        "    The function updates the V_js parameters during the training phase\n",
        "    \n",
        "    :param U1: The U parameter on the same level of V that we are updating\n",
        "    :param U2: The U parameter which is in the next level of the V that we are updating\n",
        "    :param W: The W parameter which is in the next level of the V that we are updating\n",
        "    :param b: The b parameter which is in the next level of the V that we are updating\n",
        "    :param rho: The constant rho parameter which is in the next level of the V that we are updating\n",
        "    :param gamma: The constant gamma parameter which is in the next level of the V that we are updating\n",
        "    :return: The updated V\n",
        "    \"\"\"\n",
        "    _, d = W.size()\n",
        "    I = torch.eye(d, device=device)\n",
        "    U1 = nn.ReLU()(U1)\n",
        "    _, col_U2 = U2.size()\n",
        "    Vstar = torch.mm(torch.inverse(rho * (torch.mm(torch.t(W), W)) + gamma * I),\n",
        "                     rho * torch.mm(torch.t(W), U2 - b.repeat(1, col_U2)) + gamma * U1)\n",
        "    return Vstar"
      ],
      "metadata": {
        "id": "78BFLCPNJbss"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_wb_js(U, V, W, b, alpha, rho):\n",
        "    \"\"\"\n",
        "    The function updates the W and b parameters during the training phase\n",
        "    \n",
        "    :param U: The U in the current level of W and b\n",
        "    :param V: The V in the previous level with respect to the W that we are updating\n",
        "    :param W: The current W that we have to update\n",
        "    :param b: The current b that we have to update\n",
        "    :param alpha: The alpha constant of the updates\n",
        "    :param rho: The rho constant of the updates\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    d, N = V.size()\n",
        "    I = torch.eye(d, device=device)\n",
        "    _, col_U = U.size()\n",
        "    Wstar = torch.mm(alpha * W + rho * torch.mm(U - b.repeat(1, col_U), torch.t(V)),\n",
        "                     torch.inverse(alpha * I + rho * (torch.mm(V, torch.t(V)))))\n",
        "    bstar = (alpha * b + rho * torch.sum(U - torch.mm(W, V), dim=1).reshape(b.size())) / (rho * N + alpha)\n",
        "    return Wstar, bstar"
      ],
      "metadata": {
        "id": "rPdLFBQ_Jf0R"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_prox(a, b, gamma, d, N):\n",
        "    \"\"\"\n",
        "    The function compute the solution to the relu proximal update problem\n",
        "    \n",
        "    :param a: the a in the closed formula of the linearized update\n",
        "    :param b: the b in the closed formula of the linearized update\n",
        "    :param gamma: The constant used in the update\n",
        "    :param d: the dimension of the current layer\n",
        "    :param N: The number of samples\n",
        "    :return: The obtained solution of the prox update\n",
        "    \"\"\"\n",
        "    val = torch.empty(d, N, device=device)\n",
        "    x = (a + gamma * b) / (1 + gamma)\n",
        "    y = torch.min(b, torch.zeros(d, N, device=device))\n",
        "    val = torch.where(a + gamma * b < 0, y, torch.zeros(d, N, device=device))\n",
        "    val = torch.where(\n",
        "        ((a + gamma * b >= 0) & (b >= 0)) | ((a * (gamma - np.sqrt(gamma * (gamma + 1))) <= gamma * b) & (b < 0)), x,\n",
        "        val)\n",
        "    val = torch.where((-a <= gamma * b) & (gamma * b <= a * (gamma - np.sqrt(gamma * (gamma + 1)))), b, val)\n",
        "    return val\n"
      ],
      "metadata": {
        "id": "xD0dhzjdJiKg"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_pred(Ws,bs,input,N):\n",
        "  \"\"\"\n",
        "  The function is used to make the predictions based on the best found parameters\n",
        "  :param Ws: The weight matrices\n",
        "  :param bs: the bias vectors\n",
        "  :return pred, prob\n",
        "  \"\"\"\n",
        "  a1_train = input\n",
        "  for i in range(0,len(Ws)-1):\n",
        "    a1_train = nn.ReLU()(torch.addmm(bs[i].repeat(1, N), Ws[i], a1_train))\n",
        "  pred = torch.argmax(torch.addmm(bs[len(Ws)-1].repeat(1, N), Ws[len(Ws)-1], a1_train), dim=0)\n",
        "  output_last = torch.addmm(bs[len(Ws)-1].repeat(1, N), Ws[len(Ws)-1], a1_train)\n",
        "  prob = torch.exp(output_last)/torch.sum(torch.exp(output_last),dim=0)\n",
        "  return pred, prob"
      ],
      "metadata": {
        "id": "5C2ka5vzJlCm"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import cross_entropy\n",
        "cross_entropy = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "-Lq1gjZ4Zgfo"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The function requires at least 1 hidden layer otherwise it need some rewrriting\n",
        "def execute_training(layers, input_size, hidden_size, output_size, train_set, val_set, \n",
        "                     train_labels, val_labels, use_gradient,\n",
        "                     niter = 100, gamma = 1, alpha = 5):\n",
        "  \"\"\"\n",
        "  The function takes the following arguements and produces a list of weights and biases with which \n",
        "  you can use the make_pred function to get a list of predictions\n",
        "  :param layers: The total number of layers of the network\n",
        "  :param input_size: The total size of the input layer\n",
        "  :param hidden_size: The size of the hidden layer\n",
        "  :param output_size: The size of the output layer (usefull for multiclass classification)\n",
        "  :param train_set: The training set\n",
        "  :param val_set: The validation set\n",
        "  :param train_labels: The training labels\n",
        "  :param val labels: The validation labels\n",
        "  :param use_gradient: True if the first update of V is carried out without linearization but using the gradient\n",
        "  :param niter: The default number of epochs to train the network\n",
        "  :param gamma: The gamma parameter of the algorithm\n",
        "  :param alpha: The alpha parameter of the algorithm\n",
        "  :return Ws,bs: Returns two lists that go in order from the input to the output layer of the weights and the biases of each layer\n",
        "  \"\"\"\n",
        "\n",
        "  N = len(train_labels)\n",
        "  N_test = len(val_labels)\n",
        "\n",
        "  # weight initialization (we replicate pytorch weight initialization)\n",
        "\n",
        "  std = math.sqrt(1/input_size)\n",
        "  W = torch.FloatTensor(hidden_size, input_size).uniform_(-std, std)\n",
        "  b = torch.FloatTensor(hidden_size, 1).uniform_(-std, std)\n",
        "\n",
        "  b = b.to(device = device)\n",
        "  W = W.to(device = device)\n",
        "\n",
        "  U = torch.addmm(b.repeat(1, N), W, x_train) # equivalent to W1@x_train+b1.repeat(1,N)\n",
        "  V = nn.ReLU()(U)\n",
        "\n",
        "  Ws = [W]\n",
        "  bs = [b]\n",
        "  Us = [U]\n",
        "  Vs = [V]\n",
        "\n",
        "  for i in range(1,layers-1):\n",
        "    std = math.sqrt(1/hidden_size)\n",
        "    W = torch.FloatTensor(hidden_size, hidden_size).uniform_(-std, std)\n",
        "    b = torch.FloatTensor(hidden_size, 1).uniform_(-std, std)\n",
        "    b = b.to(device = device)\n",
        "    W = W.to(device = device)\n",
        "    U = torch.addmm(b.repeat(1, N), W, Vs[-1])\n",
        "    V = nn.ReLU()(U)\n",
        "    Ws.append(W)\n",
        "    bs.append(b)\n",
        "    Us.append(U)\n",
        "    Vs.append(V)\n",
        "  \n",
        "  std = math.sqrt(1/hidden_size)\n",
        "  W = torch.FloatTensor(output_size, hidden_size).uniform_(-std, std)\n",
        "  b = torch.FloatTensor(output_size, 1).uniform_(-std, std)\n",
        "\n",
        "  # we move them to GPU\n",
        "  b = b.to(device = device)\n",
        "  W = W.to(device = device)\n",
        "  U = torch.addmm(b.repeat(1, N), W, Vs[-1])\n",
        "  V = U\n",
        "  Ws.append(W)\n",
        "  bs.append(b)\n",
        "  Us.append(U)\n",
        "  Vs.append(V)\n",
        "  \n",
        "  # constant initialization\n",
        "\n",
        "  gamma1 = gamma2 = gamma3 = gamma4 = gamma\n",
        "\n",
        "  rho = gamma\n",
        "  rho1 = rho2 = rho3 = rho4 = rho\n",
        "\n",
        "  alpha1 = alpha2 = alpha3 = alpha4 = alpha5 = alpha6 = alpha7 \\\n",
        "  = alpha8 = alpha9 = alpha10 = alpha\n",
        "\n",
        "  # vector of performance initialization\n",
        "\n",
        "  loss1 = np.empty(niter)\n",
        "  loss2 = np.empty(niter)\n",
        "  loss_class = np.empty(niter)\n",
        "  accuracy_train = np.empty(niter)\n",
        "  accuracy_test = np.empty(niter)\n",
        "  time1 = np.empty(niter)\n",
        "\n",
        "  print('Train on', N, 'samples, validate on', N_test, 'samples')\n",
        "  for k in range(niter):\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # update V3\n",
        "    if use_gradient == True:\n",
        "      if (k == 1):\n",
        "        Vs[-1] = (y_train_one_hot + gamma3*Us[-1] + alpha1*Vs[-1])/(1+ gamma3 + alpha1)\n",
        "      else:\n",
        "        for i in range(250):\n",
        "          Vs[-1] = Vs[-1] - (torch.exp(Vs[-1])/torch.sum(torch.exp(Vs[-1]),dim=0)-y_train_one_hot) * 0.01/(i+1)\n",
        "    else:\n",
        "      Vs[-1] = (y_train_one_hot + gamma3*Us[-1] + alpha1*Vs[-1])/(1+ gamma3 + alpha1)\n",
        "\n",
        "    # update U3 \n",
        "    Us[-1] = (gamma3*Vs[-1] + rho3*(torch.mm(Ws[-1],Vs[-2]) + bs[-1].repeat(1,N)))/(gamma3 + rho3)\n",
        "\n",
        "    # update W3 and b3\n",
        "    W, b = update_wb_js(Us[-1],Vs[-2],Ws[-1],bs[-1],alpha1, rho3)\n",
        "    Ws[-1] = W\n",
        "    bs[-1] = b\n",
        "\n",
        "    for i in range(len(Vs)-2,0,-1):\n",
        "      Vs[i] = update_v_js(Us[i],Us[i+1],Ws[i+1],bs[i+1],rho3,gamma2)\n",
        "      Us[i] = relu_prox(Vs[i],(rho2*torch.addmm(bs[i].repeat(1,N), Ws[i], Vs[i-1]) +\n",
        "                               alpha2*Us[i])/(rho2 + alpha2),(rho2 + alpha2)/gamma2, hidden_size, N)\n",
        "      W,b = update_wb_js(Us[i],Vs[i-1],Ws[i],bs[i],alpha3,rho2)\n",
        "      Ws[i] = W\n",
        "      bs[i]= b\n",
        "    \n",
        "    # update V1\n",
        "    Vs[0] = update_v_js(Us[0],Us[1],Ws[1],bs[1],rho2,gamma1)\n",
        "        \n",
        "    # update U1\n",
        "    Us[0] = relu_prox(Vs[0],(rho1*torch.addmm(bs[0].repeat(1,N), Ws[0], x_train) +\n",
        "                             alpha7*Us[0])/(rho1 + alpha7),(rho1 + alpha7)/gamma1, hidden_size, N)\n",
        "    \n",
        "    # update W1 and b1\n",
        "    W, b = update_wb_js(Us[0],x_train,Ws[0],bs[0],alpha8,rho1)\n",
        "    Ws[0] = W\n",
        "    bs[0] = b\n",
        "\n",
        "    #a1_train = nn.ReLU()(torch.addmm(b1.repeat(1, N), W1, x_train))\n",
        "    #a1_train = x_train\n",
        "    #for i in range(len(Vs)-1,0,-1):\n",
        "    #  a1_train = nn.ReLU()(torch.addmm(bs[i].repeat(1, N), Ws[i], a1_train))\n",
        "    #pred = torch.argmax(torch.addmm(bs[0].repeat(1, N), Ws[0], a1_train), dim=0)\n",
        "    pred,_ = make_pred(Ws,bs,x_train,N)\n",
        "\n",
        "    #a1_test = x_test\n",
        "    #a1_test = nn.ReLU()(torch.addmm(b1.repeat(1, N_test), W1, x_test))\n",
        "    #for i in range(len(Vs)-1,0,-1):\n",
        "    #  a1_test = nn.ReLU()(torch.addmm(bs[i].repeat(1, N_test), Ws[i], a1_test))\n",
        "    #pred_test = torch.argmax(torch.addmm(bs[0].repeat(1, N_test), Ws[0], a1_test), dim=0)\n",
        "    pred_test, prob_test = make_pred(Ws,bs,x_test,N_test) \n",
        "\n",
        "    \n",
        "    loss_class[k] = torch.sum(- y_test_one_hot * torch.log(prob_test))\n",
        "\n",
        "    loss1[k] = gamma/2*torch.pow(torch.dist(Vs[-1],y_train_one_hot,2),2).cpu().numpy()\n",
        "    loss2[k] = loss1[k] + gamma/2 * torch.pow(torch.dist(torch.addmm(bs[0].repeat(1,N), Ws[0], x_train),Us[0],2),2).cpu().numpy()\n",
        "\n",
        "    for i in range(1,layers):\n",
        "      loss2[k] = loss2[k] + gamma/2 * torch.pow(torch.dist(torch.addmm(bs[i].repeat(1,N), Ws[i], Vs[i-1]),Us[i],2),2).cpu().numpy()\n",
        "\n",
        "    #loss2[k] = loss1[k] + rho1/2*torch.pow(torch.dist(torch.addmm(b1.repeat(1,N), W1, x_train),U1,2),2).cpu().numpy() \\\n",
        "    #+rho2/2*torch.pow(torch.dist(torch.addmm(b2.repeat(1,N), W2, V1),U2,2),2).cpu().numpy() \\\n",
        "    #+rho3/2*torch.pow(torch.dist(torch.addmm(b3.repeat(1,N), W3, V2),U3,2),2).cpu().numpy()\n",
        "        \n",
        "    # compute training accuracy\n",
        "    correct_train = pred == train_labels\n",
        "    accuracy_train[k] = np.mean(correct_train.cpu().numpy())\n",
        "        \n",
        "    # compute validation accuracy\n",
        "    correct_test = pred_test == val_labels\n",
        "    accuracy_test[k] = np.mean(correct_test.cpu().numpy())\n",
        "        \n",
        "    # compute training time\n",
        "    stop = time.time()\n",
        "    duration = stop - start\n",
        "    time1[k] = duration\n",
        "        \n",
        "    # print results\n",
        "    print('Epoch', k + 1, '/', niter, '\\n', \n",
        "          '-', 'time:', time1[k], '-', 'sq_loss:', loss1[k], '-', 'tot_loss:',\n",
        "          loss2[k], '-', 'loss_class:', loss_class[k], '-', 'acc:',\n",
        "          accuracy_train[k], '-', 'val_acc:', accuracy_test[k])\n",
        "    \n",
        "  print('The total time spent is:', np.sum(time1), 's')\n",
        "  return loss2,loss_class,accuracy_test"
      ],
      "metadata": {
        "id": "lJSDpRPRJ0xb"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss2, loss_class , accuracy_test = execute_training(3, input_size, hidden_size, output_size, x_train, x_test, y_train, y_test,\n",
        "                                         True, niter = 100, gamma = 1, alpha = 5)"
      ],
      "metadata": {
        "id": "PLkWZMwqJ1vH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eff90ffc-f59b-465d-80a4-008e2d2a8c48"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1 / 100 \n",
            " - time: 1.254204273223877 - sq_loss: 27133.44140625 - tot_loss: 27137.89398983121 - loss_class: 22896.875 - acc: 0.20288333333333333 - val_acc: 0.2019\n",
            "Epoch 2 / 100 \n",
            " - time: 1.2348954677581787 - sq_loss: 20125.6875 - tot_loss: 20153.516769926995 - loss_class: 22402.408203125 - acc: 0.5325833333333333 - val_acc: 0.5221\n",
            "Epoch 3 / 100 \n",
            " - time: 1.2764136791229248 - sq_loss: 17598.59375 - tot_loss: 17610.66461057961 - loss_class: 21959.951171875 - acc: 0.7324333333333334 - val_acc: 0.7146\n",
            "Epoch 4 / 100 \n",
            " - time: 1.2688531875610352 - sq_loss: 15279.66796875 - tot_loss: 15287.164010394365 - loss_class: 21551.84765625 - acc: 0.8118166666666666 - val_acc: 0.7983\n",
            "Epoch 5 / 100 \n",
            " - time: 1.275266170501709 - sq_loss: 13165.2744140625 - tot_loss: 13172.195207450539 - loss_class: 21162.244140625 - acc: 0.84335 - val_acc: 0.8285\n",
            "Epoch 6 / 100 \n",
            " - time: 1.2673799991607666 - sq_loss: 11251.59765625 - tot_loss: 11257.422478109598 - loss_class: 20790.33984375 - acc: 0.8567333333333333 - val_acc: 0.8398\n",
            "Epoch 7 / 100 \n",
            " - time: 1.2764477729797363 - sq_loss: 9534.6591796875 - tot_loss: 9540.293713644147 - loss_class: 20424.30078125 - acc: 0.8633833333333333 - val_acc: 0.8443\n",
            "Epoch 8 / 100 \n",
            " - time: 1.2762067317962646 - sq_loss: 8010.30419921875 - tot_loss: 8016.153746411204 - loss_class: 20068.498046875 - acc: 0.8676 - val_acc: 0.8489\n",
            "Epoch 9 / 100 \n",
            " - time: 1.2857871055603027 - sq_loss: 6674.2197265625 - tot_loss: 6680.465238451958 - loss_class: 19717.96484375 - acc: 0.8702 - val_acc: 0.8511\n",
            "Epoch 10 / 100 \n",
            " - time: 1.2783880233764648 - sq_loss: 5521.93505859375 - tot_loss: 5528.724398806691 - loss_class: 19372.27734375 - acc: 0.8724166666666666 - val_acc: 0.8524\n",
            "Epoch 11 / 100 \n",
            " - time: 1.279768705368042 - sq_loss: 4548.83837890625 - tot_loss: 4555.8660966306925 - loss_class: 19035.888671875 - acc: 0.8739 - val_acc: 0.8538\n",
            "Epoch 12 / 100 \n",
            " - time: 1.3012399673461914 - sq_loss: 3750.178955078125 - tot_loss: 3757.9814442545176 - loss_class: 18705.353515625 - acc: 0.8752666666666666 - val_acc: 0.8547\n",
            "Epoch 13 / 100 \n",
            " - time: 1.2783653736114502 - sq_loss: 3121.08056640625 - tot_loss: 3128.9475698918104 - loss_class: 18375.02734375 - acc: 0.8768 - val_acc: 0.8548\n",
            "Epoch 14 / 100 \n",
            " - time: 1.300041675567627 - sq_loss: 2656.55615234375 - tot_loss: 2664.745300889015 - loss_class: 18056.02734375 - acc: 0.8778 - val_acc: 0.8555\n",
            "Epoch 15 / 100 \n",
            " - time: 1.2994458675384521 - sq_loss: 2351.517822265625 - tot_loss: 2359.901353120804 - loss_class: 17741.04296875 - acc: 0.8789166666666667 - val_acc: 0.8556\n",
            "Epoch 16 / 100 \n",
            " - time: 1.2918870449066162 - sq_loss: 2200.792724609375 - tot_loss: 2209.6048315763474 - loss_class: 17434.345703125 - acc: 0.8799333333333333 - val_acc: 0.857\n",
            "Epoch 17 / 100 \n",
            " - time: 1.296433687210083 - sq_loss: 2199.1357421875 - tot_loss: 2207.8431389331818 - loss_class: 17128.302734375 - acc: 0.8813333333333333 - val_acc: 0.8577\n",
            "Epoch 18 / 100 \n",
            " - time: 1.298219919204712 - sq_loss: 2341.24853515625 - tot_loss: 2350.0201506614685 - loss_class: 16832.87109375 - acc: 0.8819666666666667 - val_acc: 0.8582\n",
            "Epoch 19 / 100 \n",
            " - time: 1.2984647750854492 - sq_loss: 2621.788818359375 - tot_loss: 2630.47244066 - loss_class: 16542.306640625 - acc: 0.88265 - val_acc: 0.859\n",
            "Epoch 20 / 100 \n",
            " - time: 1.3015472888946533 - sq_loss: 3035.39453125 - tot_loss: 3044.030793994665 - loss_class: 16256.732421875 - acc: 0.88335 - val_acc: 0.8595\n",
            "Epoch 21 / 100 \n",
            " - time: 1.29429292678833 - sq_loss: 3576.690185546875 - tot_loss: 3585.25158649683 - loss_class: 15979.26171875 - acc: 0.8840166666666667 - val_acc: 0.8597\n",
            "Epoch 22 / 100 \n",
            " - time: 1.2990856170654297 - sq_loss: 4240.31201171875 - tot_loss: 4248.678810983896 - loss_class: 15708.259765625 - acc: 0.8845 - val_acc: 0.8598\n",
            "Epoch 23 / 100 \n",
            " - time: 1.3002734184265137 - sq_loss: 5020.9140625 - tot_loss: 5028.959044545889 - loss_class: 15445.603515625 - acc: 0.88515 - val_acc: 0.8605\n",
            "Epoch 24 / 100 \n",
            " - time: 1.3018996715545654 - sq_loss: 5913.1943359375 - tot_loss: 5921.00682798028 - loss_class: 15187.080078125 - acc: 0.8856166666666667 - val_acc: 0.8607\n",
            "Epoch 25 / 100 \n",
            " - time: 1.3011283874511719 - sq_loss: 6911.89599609375 - tot_loss: 6919.511129140854 - loss_class: 14934.7177734375 - acc: 0.8861833333333333 - val_acc: 0.8609\n",
            "Epoch 26 / 100 \n",
            " - time: 1.2993566989898682 - sq_loss: 8011.83251953125 - tot_loss: 8019.359738975763 - loss_class: 14687.6875 - acc: 0.8866166666666667 - val_acc: 0.8615\n",
            "Epoch 27 / 100 \n",
            " - time: 1.2981374263763428 - sq_loss: 9207.89453125 - tot_loss: 9215.212274804711 - loss_class: 14451.6484375 - acc: 0.88705 - val_acc: 0.8617\n",
            "Epoch 28 / 100 \n",
            " - time: 1.302429437637329 - sq_loss: 10495.0634765625 - tot_loss: 10502.088554486632 - loss_class: 14216.2041015625 - acc: 0.8873333333333333 - val_acc: 0.8621\n",
            "Epoch 29 / 100 \n",
            " - time: 1.306595802307129 - sq_loss: 11868.423828125 - tot_loss: 11875.260475710034 - loss_class: 13989.3740234375 - acc: 0.8877166666666667 - val_acc: 0.8627\n",
            "Epoch 30 / 100 \n",
            " - time: 1.2961771488189697 - sq_loss: 13323.1640625 - tot_loss: 13329.651075959206 - loss_class: 13772.658203125 - acc: 0.8879833333333333 - val_acc: 0.8625\n",
            "Epoch 31 / 100 \n",
            " - time: 1.2992000579833984 - sq_loss: 14854.6025390625 - tot_loss: 14860.862364903092 - loss_class: 13559.001953125 - acc: 0.8882166666666667 - val_acc: 0.8625\n",
            "Epoch 32 / 100 \n",
            " - time: 1.2993769645690918 - sq_loss: 16458.177734375 - tot_loss: 16464.193843625486 - loss_class: 13351.1865234375 - acc: 0.8884666666666666 - val_acc: 0.8625\n",
            "Epoch 33 / 100 \n",
            " - time: 1.2997524738311768 - sq_loss: 18129.46875 - tot_loss: 18135.3183978945 - loss_class: 13152.1865234375 - acc: 0.8886666666666667 - val_acc: 0.8627\n",
            "Epoch 34 / 100 \n",
            " - time: 1.3051669597625732 - sq_loss: 19864.189453125 - tot_loss: 19870.017821766436 - loss_class: 12953.4111328125 - acc: 0.8887666666666667 - val_acc: 0.863\n",
            "Epoch 35 / 100 \n",
            " - time: 1.2847485542297363 - sq_loss: 21658.19921875 - tot_loss: 21663.797684788704 - loss_class: 12768.771484375 - acc: 0.88915 - val_acc: 0.8628\n",
            "Epoch 36 / 100 \n",
            " - time: 1.3017289638519287 - sq_loss: 23507.505859375 - tot_loss: 23512.83693421632 - loss_class: 12583.048828125 - acc: 0.8893833333333333 - val_acc: 0.8633\n",
            "Epoch 37 / 100 \n",
            " - time: 1.2992982864379883 - sq_loss: 25408.263671875 - tot_loss: 25413.41325453669 - loss_class: 12401.515625 - acc: 0.8895 - val_acc: 0.8631\n",
            "Epoch 38 / 100 \n",
            " - time: 1.309915542602539 - sq_loss: 27356.78515625 - tot_loss: 27361.73599383235 - loss_class: 12233.951171875 - acc: 0.8898166666666667 - val_acc: 0.8636\n",
            "Epoch 39 / 100 \n",
            " - time: 1.2944257259368896 - sq_loss: 29349.525390625 - tot_loss: 29354.371456578374 - loss_class: 12063.66015625 - acc: 0.88995 - val_acc: 0.8635\n",
            "Epoch 40 / 100 \n",
            " - time: 1.3164820671081543 - sq_loss: 31383.095703125 - tot_loss: 31388.011433765292 - loss_class: 11900.4609375 - acc: 0.88985 - val_acc: 0.8636\n",
            "Epoch 41 / 100 \n",
            " - time: 1.3089871406555176 - sq_loss: 33454.2578125 - tot_loss: 33458.84856143594 - loss_class: 11747.853515625 - acc: 0.89 - val_acc: 0.8632\n",
            "Epoch 42 / 100 \n",
            " - time: 1.3111419677734375 - sq_loss: 35559.921875 - tot_loss: 35564.61528398469 - loss_class: 11594.3251953125 - acc: 0.8899833333333333 - val_acc: 0.8632\n",
            "Epoch 43 / 100 \n",
            " - time: 1.3146910667419434 - sq_loss: 37697.15234375 - tot_loss: 37701.45811044797 - loss_class: 11447.982421875 - acc: 0.8901 - val_acc: 0.8629\n",
            "Epoch 44 / 100 \n",
            " - time: 1.3145062923431396 - sq_loss: 39863.1328125 - tot_loss: 39867.477877061814 - loss_class: 11302.060546875 - acc: 0.8902166666666667 - val_acc: 0.8631\n",
            "Epoch 45 / 100 \n",
            " - time: 1.3194942474365234 - sq_loss: 42055.22265625 - tot_loss: 42059.285928908736 - loss_class: 11164.88671875 - acc: 0.89035 - val_acc: 0.863\n",
            "Epoch 46 / 100 \n",
            " - time: 1.3222124576568604 - sq_loss: 44270.88671875 - tot_loss: 44275.88421243057 - loss_class: 11040.146484375 - acc: 0.8904166666666666 - val_acc: 0.8627\n",
            "Epoch 47 / 100 \n",
            " - time: 1.3172237873077393 - sq_loss: 46507.734375 - tot_loss: 46511.69917374849 - loss_class: 10903.36328125 - acc: 0.8905333333333333 - val_acc: 0.8631\n",
            "Epoch 48 / 100 \n",
            " - time: 1.3095664978027344 - sq_loss: 48763.52734375 - tot_loss: 48767.20283225179 - loss_class: 10776.8974609375 - acc: 0.8904 - val_acc: 0.8628\n",
            "Epoch 49 / 100 \n",
            " - time: 1.3150148391723633 - sq_loss: 51036.109375 - tot_loss: 51039.74003196135 - loss_class: 10653.603515625 - acc: 0.8905333333333333 - val_acc: 0.8629\n",
            "Epoch 50 / 100 \n",
            " - time: 1.3203246593475342 - sq_loss: 53323.48828125 - tot_loss: 53326.993524603546 - loss_class: 10538.2919921875 - acc: 0.8906666666666667 - val_acc: 0.8628\n",
            "Epoch 51 / 100 \n",
            " - time: 1.318798542022705 - sq_loss: 55623.74609375 - tot_loss: 55627.218941856176 - loss_class: 10426.5390625 - acc: 0.8907833333333334 - val_acc: 0.8624\n",
            "Epoch 52 / 100 \n",
            " - time: 1.3184690475463867 - sq_loss: 57935.1015625 - tot_loss: 57938.69862269238 - loss_class: 10316.6728515625 - acc: 0.8910166666666667 - val_acc: 0.862\n",
            "Epoch 53 / 100 \n",
            " - time: 1.3237380981445312 - sq_loss: 60255.87109375 - tot_loss: 60259.30855012685 - loss_class: 10203.08203125 - acc: 0.891 - val_acc: 0.8617\n",
            "Epoch 54 / 100 \n",
            " - time: 1.3249664306640625 - sq_loss: 62584.49609375 - tot_loss: 62587.64076381177 - loss_class: 10100.8896484375 - acc: 0.89085 - val_acc: 0.8618\n",
            "Epoch 55 / 100 \n",
            " - time: 1.319044589996338 - sq_loss: 64919.4765625 - tot_loss: 64922.49267964065 - loss_class: 10001.15234375 - acc: 0.8909333333333334 - val_acc: 0.8618\n",
            "Epoch 56 / 100 \n",
            " - time: 1.3153290748596191 - sq_loss: 67259.4296875 - tot_loss: 67262.4574177973 - loss_class: 9905.576171875 - acc: 0.8910166666666667 - val_acc: 0.8617\n",
            "Epoch 57 / 100 \n",
            " - time: 1.3249752521514893 - sq_loss: 69603.0625 - tot_loss: 69605.9172780998 - loss_class: 9808.46484375 - acc: 0.8909166666666667 - val_acc: 0.8613\n",
            "Epoch 58 / 100 \n",
            " - time: 1.3181421756744385 - sq_loss: 71949.1484375 - tot_loss: 71952.11688368209 - loss_class: 9719.5966796875 - acc: 0.8911166666666667 - val_acc: 0.861\n",
            "Epoch 59 / 100 \n",
            " - time: 1.3258306980133057 - sq_loss: 74296.5625 - tot_loss: 74299.99737439118 - loss_class: 9622.8486328125 - acc: 0.891 - val_acc: 0.8613\n",
            "Epoch 60 / 100 \n",
            " - time: 1.3284459114074707 - sq_loss: 76644.234375 - tot_loss: 76647.53519511409 - loss_class: 9547.2080078125 - acc: 0.8911166666666667 - val_acc: 0.8614\n",
            "Epoch 61 / 100 \n",
            " - time: 1.3280527591705322 - sq_loss: 78991.203125 - tot_loss: 78993.91627022438 - loss_class: 9457.2607421875 - acc: 0.89095 - val_acc: 0.8615\n",
            "Epoch 62 / 100 \n",
            " - time: 1.3321588039398193 - sq_loss: 81336.5078125 - tot_loss: 81339.18891068734 - loss_class: 9379.4375 - acc: 0.8909833333333333 - val_acc: 0.8611\n",
            "Epoch 63 / 100 \n",
            " - time: 1.3351035118103027 - sq_loss: 83679.3203125 - tot_loss: 83681.78891225904 - loss_class: 9296.53515625 - acc: 0.8911333333333333 - val_acc: 0.8607\n",
            "Epoch 64 / 100 \n",
            " - time: 1.3380343914031982 - sq_loss: 86018.84375 - tot_loss: 86021.31545021571 - loss_class: 9217.953125 - acc: 0.8910666666666667 - val_acc: 0.8607\n",
            "Epoch 65 / 100 \n",
            " - time: 1.335113286972046 - sq_loss: 88354.328125 - tot_loss: 88356.6963503547 - loss_class: 9145.1875 - acc: 0.8911166666666667 - val_acc: 0.8605\n",
            "Epoch 66 / 100 \n",
            " - time: 1.332146167755127 - sq_loss: 90685.109375 - tot_loss: 90687.93696919829 - loss_class: 9081.689453125 - acc: 0.8909833333333333 - val_acc: 0.8598\n",
            "Epoch 67 / 100 \n",
            " - time: 1.3398768901824951 - sq_loss: 93010.5390625 - tot_loss: 93012.93537840433 - loss_class: 9002.6796875 - acc: 0.891 - val_acc: 0.8598\n",
            "Epoch 68 / 100 \n",
            " - time: 1.3375670909881592 - sq_loss: 95330.0078125 - tot_loss: 95332.17989788577 - loss_class: 8936.7236328125 - acc: 0.891 - val_acc: 0.8597\n",
            "Epoch 69 / 100 \n",
            " - time: 1.3304643630981445 - sq_loss: 97643.03125 - tot_loss: 97645.14216259029 - loss_class: 8871.2275390625 - acc: 0.8909333333333334 - val_acc: 0.8592\n",
            "Epoch 70 / 100 \n",
            " - time: 1.3297040462493896 - sq_loss: 99949.0546875 - tot_loss: 99951.13258416764 - loss_class: 8802.498046875 - acc: 0.8909333333333334 - val_acc: 0.8596\n",
            "Epoch 71 / 100 \n",
            " - time: 1.336857557296753 - sq_loss: 102247.671875 - tot_loss: 102249.75543121435 - loss_class: 8743.634765625 - acc: 0.8909 - val_acc: 0.8593\n",
            "Epoch 72 / 100 \n",
            " - time: 1.3432433605194092 - sq_loss: 104538.421875 - tot_loss: 104540.4096981166 - loss_class: 8677.2998046875 - acc: 0.89095 - val_acc: 0.8593\n",
            "Epoch 73 / 100 \n",
            " - time: 1.3416931629180908 - sq_loss: 106820.9296875 - tot_loss: 106823.03113653883 - loss_class: 8621.8583984375 - acc: 0.8910833333333333 - val_acc: 0.8592\n",
            "Epoch 74 / 100 \n",
            " - time: 1.3284525871276855 - sq_loss: 109094.84375 - tot_loss: 109096.74743673019 - loss_class: 8559.1259765625 - acc: 0.8910333333333333 - val_acc: 0.8593\n",
            "Epoch 75 / 100 \n",
            " - time: 1.3481531143188477 - sq_loss: 111359.8359375 - tot_loss: 111361.72797673848 - loss_class: 8503.2470703125 - acc: 0.8909666666666667 - val_acc: 0.8592\n",
            "Epoch 76 / 100 \n",
            " - time: 1.339250087738037 - sq_loss: 113615.6328125 - tot_loss: 113617.59539707005 - loss_class: 8445.302734375 - acc: 0.8909833333333333 - val_acc: 0.8593\n",
            "Epoch 77 / 100 \n",
            " - time: 1.3360316753387451 - sq_loss: 115861.9140625 - tot_loss: 115863.73439696245 - loss_class: 8395.904296875 - acc: 0.8909 - val_acc: 0.8593\n",
            "Epoch 78 / 100 \n",
            " - time: 1.3439979553222656 - sq_loss: 118098.5078125 - tot_loss: 118100.53285626415 - loss_class: 8340.287109375 - acc: 0.89095 - val_acc: 0.8595\n",
            "Epoch 79 / 100 \n",
            " - time: 1.344146728515625 - sq_loss: 120325.140625 - tot_loss: 120327.0008968953 - loss_class: 8294.50390625 - acc: 0.8909166666666667 - val_acc: 0.8589\n",
            "Epoch 80 / 100 \n",
            " - time: 1.3384790420532227 - sq_loss: 122541.640625 - tot_loss: 122543.39875287656 - loss_class: 8241.287109375 - acc: 0.8908 - val_acc: 0.8589\n",
            "Epoch 81 / 100 \n",
            " - time: 1.3225276470184326 - sq_loss: 124747.8359375 - tot_loss: 124749.5712793069 - loss_class: 8193.63671875 - acc: 0.8908666666666667 - val_acc: 0.8589\n",
            "Epoch 82 / 100 \n",
            " - time: 1.336451768875122 - sq_loss: 126943.578125 - tot_loss: 126945.36011356954 - loss_class: 8148.2919921875 - acc: 0.8908833333333334 - val_acc: 0.8589\n",
            "Epoch 83 / 100 \n",
            " - time: 1.340369462966919 - sq_loss: 129128.6953125 - tot_loss: 129130.53403156996 - loss_class: 8102.1494140625 - acc: 0.8907833333333334 - val_acc: 0.859\n",
            "Epoch 84 / 100 \n",
            " - time: 1.337522268295288 - sq_loss: 131303.078125 - tot_loss: 131304.99097356014 - loss_class: 8057.6708984375 - acc: 0.8908666666666667 - val_acc: 0.8591\n",
            "Epoch 85 / 100 \n",
            " - time: 1.3348126411437988 - sq_loss: 133466.65625 - tot_loss: 133468.3580438057 - loss_class: 8010.1826171875 - acc: 0.8909166666666667 - val_acc: 0.8592\n",
            "Epoch 86 / 100 \n",
            " - time: 1.3432414531707764 - sq_loss: 135619.265625 - tot_loss: 135620.8930988675 - loss_class: 7965.99658203125 - acc: 0.8908666666666667 - val_acc: 0.8591\n",
            "Epoch 87 / 100 \n",
            " - time: 1.3496131896972656 - sq_loss: 137760.921875 - tot_loss: 137762.72891195863 - loss_class: 7927.94873046875 - acc: 0.8908 - val_acc: 0.8593\n",
            "Epoch 88 / 100 \n",
            " - time: 1.3459599018096924 - sq_loss: 139891.46875 - tot_loss: 139893.05224491097 - loss_class: 7883.736328125 - acc: 0.8907666666666667 - val_acc: 0.8594\n",
            "Epoch 89 / 100 \n",
            " - time: 1.3380818367004395 - sq_loss: 142010.890625 - tot_loss: 142012.45378680434 - loss_class: 7845.7353515625 - acc: 0.8908166666666667 - val_acc: 0.8594\n",
            "Epoch 90 / 100 \n",
            " - time: 1.3403692245483398 - sq_loss: 144119.1875 - tot_loss: 144120.9731375086 - loss_class: 7801.92138671875 - acc: 0.8906666666666667 - val_acc: 0.8593\n",
            "Epoch 91 / 100 \n",
            " - time: 1.339219093322754 - sq_loss: 146216.234375 - tot_loss: 146217.82862560684 - loss_class: 7767.927734375 - acc: 0.8905166666666666 - val_acc: 0.8591\n",
            "Epoch 92 / 100 \n",
            " - time: 1.3537471294403076 - sq_loss: 148302.109375 - tot_loss: 148303.57384436205 - loss_class: 7728.2021484375 - acc: 0.8906 - val_acc: 0.8594\n",
            "Epoch 93 / 100 \n",
            " - time: 1.344442367553711 - sq_loss: 150376.75 - tot_loss: 150378.15420338232 - loss_class: 7691.7353515625 - acc: 0.89065 - val_acc: 0.8593\n",
            "Epoch 94 / 100 \n",
            " - time: 1.3369622230529785 - sq_loss: 152440.15625 - tot_loss: 152441.60098131094 - loss_class: 7655.787109375 - acc: 0.8905333333333333 - val_acc: 0.8592\n",
            "Epoch 95 / 100 \n",
            " - time: 1.3491785526275635 - sq_loss: 154492.296875 - tot_loss: 154493.6848794762 - loss_class: 7616.4228515625 - acc: 0.8905166666666666 - val_acc: 0.8594\n",
            "Epoch 96 / 100 \n",
            " - time: 1.344041347503662 - sq_loss: 156533.234375 - tot_loss: 156534.5983177889 - loss_class: 7583.94775390625 - acc: 0.8905666666666666 - val_acc: 0.8591\n",
            "Epoch 97 / 100 \n",
            " - time: 1.343900203704834 - sq_loss: 158562.953125 - tot_loss: 158564.33397201914 - loss_class: 7550.396484375 - acc: 0.8904833333333333 - val_acc: 0.859\n",
            "Epoch 98 / 100 \n",
            " - time: 1.3371899127960205 - sq_loss: 160581.515625 - tot_loss: 160583.1813372788 - loss_class: 7521.07861328125 - acc: 0.8905666666666666 - val_acc: 0.8592\n",
            "Epoch 99 / 100 \n",
            " - time: 1.3439693450927734 - sq_loss: 162588.875 - tot_loss: 162590.79623970296 - loss_class: 7480.96240234375 - acc: 0.8904666666666666 - val_acc: 0.8595\n",
            "Epoch 100 / 100 \n",
            " - time: 1.3360569477081299 - sq_loss: 164585.078125 - tot_loss: 164586.540365607 - loss_class: 7453.716796875 - acc: 0.89065 - val_acc: 0.8596\n",
            "The total time spent is: 131.5758147239685 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_one_hot.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERkwXe1KeDhV",
        "outputId": "4538497e-e2fb-4894-8aed-18a005af9cf1"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 10000])"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyis of the results"
      ],
      "metadata": {
        "id": "0JYv4fCSKAgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## We plot the train losses\n",
        "\n",
        "plot_train_losses(loss1.shape[0], loss1, 'Coordinate_descent')"
      ],
      "metadata": {
        "id": "lar28JQyJ5zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## We plot the test accuracy\n",
        "\n",
        "plot_test_accuracy(accuracy_test.shape[0], accuracy_test, 'Coordinate_descent')\n"
      ],
      "metadata": {
        "id": "hmBh9-j3KFVn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}