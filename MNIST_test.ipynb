{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f585aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports and Utilities\n",
    "\n",
    "!pip install --upgrade --force-reinstall --quiet git+https://github.com/ZIB-IOL/StochasticFrankWolfe.git@arXiv-2010.07243v2\n",
    "!pip install --quiet barbar\n",
    "\n",
    "from barbar import Bar\n",
    "import math\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from constraints.constraints import *\n",
    "from SFW import *\n",
    "\n",
    "class Utilities:\n",
    "\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def categorical_accuracy(y_true, output, topk=1):\n",
    "        \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "        prediction = output.topk(topk, dim=1, largest=True, sorted=False).indices.t()\n",
    "        n_labels = float(len(y_true))\n",
    "        return prediction.eq(y_true.expand_as(prediction)).sum().item() / n_labels\n",
    "\n",
    "class RetractionLR(torch.optim.lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Retracts the learning rate as follows. Two running averages are kept, one of length n_close, one of n_far. Adjust\n",
    "    the learning_rate depending on the relation of far_average and close_average. Decrease by 1-retraction_factor.\n",
    "    Increase by 1/(1 - retraction_factor*growth_factor)\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, retraction_factor=0.3, n_close=5, n_far=10, lowerBound=1e-5, upperBound=1, growth_factor=0.2, last_epoch=-1):\n",
    "        self.retraction_factor = retraction_factor\n",
    "        self.n_close = n_close\n",
    "        self.n_far = n_far\n",
    "        self.lowerBound = lowerBound\n",
    "        self.upperBound = upperBound\n",
    "        self.growth_factor = growth_factor\n",
    "\n",
    "        assert (0 <= self.retraction_factor < 1), \"Retraction factor must be in [0, 1[.\"\n",
    "        assert (0 <= self.lowerBound < self.upperBound <= 1), \"Bounds must be in [0, 1]\"\n",
    "        assert (0 < self.growth_factor <= 1), \"Growth factor must be in ]0, 1]\"\n",
    "\n",
    "        self.closeAverage = RunningAverage(self.n_close)\n",
    "        self.farAverage = RunningAverage(self.n_far)\n",
    "\n",
    "        super(RetractionLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def update_averages(self, loss):\n",
    "        self.closeAverage(loss)\n",
    "        self.farAverage(loss)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        factor = 1\n",
    "        if self.farAverage.is_complete() and self.closeAverage.is_complete():\n",
    "            if self.closeAverage.result() > self.farAverage.result():\n",
    "                # Decrease the learning rate\n",
    "                factor = 1 - self.retraction_factor\n",
    "            elif self.farAverage.result() > self.closeAverage.result():\n",
    "                # Increase the learning rate\n",
    "                factor = 1./(1 - self.retraction_factor*self.growth_factor)\n",
    "\n",
    "        return [max(self.lowerBound, min(factor * group['lr'], self.upperBound)) for group in self.optimizer.param_groups]\n",
    "\n",
    "class RunningAverage(object):\n",
    "    \"\"\"Tracks the running average of n numbers\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.avg = 0\n",
    "        self.entries = []\n",
    "\n",
    "    def result(self):\n",
    "        return self.avg\n",
    "\n",
    "    def get_count(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def is_complete(self):\n",
    "        return len(self.entries) == self.n\n",
    "\n",
    "    def __call__(self, val):\n",
    "        if len(self.entries) == self.n:\n",
    "            l = self.entries.pop(0)\n",
    "            self.sum -= l\n",
    "        self.entries.append(val)\n",
    "        self.sum += val\n",
    "        self.avg = self.sum / len(self.entries)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.avg)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def result(self):\n",
    "        return self.avg\n",
    "\n",
    "    def __call__(self, val, n=1):\n",
    "        \"\"\"val is an average over n samples. To compute the overall average, add val*n to sum and increase count by n\"\"\"\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.avg)\n",
    "\n",
    "means = {\n",
    "    'cifar10': (0.4914, 0.4822, 0.4465),\n",
    "    'cifar100': (0.5071, 0.4867, 0.4408),\n",
    "    'imagenet': (0.485, 0.456, 0.406),\n",
    "}\n",
    "\n",
    "stds = {\n",
    "    'cifar10': (0.2023, 0.1994, 0.2010),\n",
    "    'cifar100': (0.2675, 0.2565, 0.2761),\n",
    "    'imagenet': (0.229, 0.224, 0.225),\n",
    "}\n",
    "\n",
    "\n",
    "datasetDict = {  # Links dataset names to actual torch datasets\n",
    "    'cifar10': getattr(torchvision.datasets, 'CIFAR10'),\n",
    "    'cifar100': getattr(torchvision.datasets, 'CIFAR100'),\n",
    "    'imagenet': getattr(torchvision.datasets, 'ImageNet'),\n",
    "}\n",
    "\n",
    "# Note: previously, these were dependent on the model name, now they are on the dataset name only\n",
    "trainTransformDict = {  # Links dataset names to train dataset transformers\n",
    "    'cifar10': transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=means['cifar10'], std=stds['cifar10']), ]),\n",
    "    'cifar100': transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=means['cifar100'], std=stds['cifar100']), ]),\n",
    "    'imagenet': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=means['imagenet'], std=stds['imagenet']),]),\n",
    "}\n",
    "testTransformDict = {  # Links dataset names to test dataset transformers\n",
    "    'cifar10': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=means['cifar10'], std=stds['cifar10']), ]),\n",
    "    'cifar100': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=means['cifar100'], std=stds['cifar100']), ]),\n",
    "    'imagenet': transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=means['imagenet'], std=stds['imagenet']),]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06f8ee36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#@title Select dataset and model\n",
    "#@markdown While the code also supports also the ImageNet-dataset, only CIFAR-10 and CIFAR-100 are selectable options here since ImageNet is not publicly accessible. Four different common DNN architectures can be selected:\n",
    "#@markdown - [DenseNet-121](https://arxiv.org/pdf/1608.06993.pdf)\n",
    "#@markdown - [WideResNet-28x10](https://arxiv.org/pdf/1605.07146v2.pdf)\n",
    "#@markdown - [GoogLeNet](https://arxiv.org/pdf/1409.4842v1.pdf)\n",
    "#@markdown - [ResNext50](https://arxiv.org/pdf/1611.05431.pdf)\n",
    "\n",
    "# select hyperparameters\n",
    "dataset_name = 'cifar10' #@param ['cifar10', 'cifar100']\n",
    "model_type = 'DenseNet' #@param ['DenseNet', 'WideResNet', 'GoogLeNet', 'ResNeXt']\n",
    "\n",
    "root = f\"{dataset_name}-dataset\"\n",
    "trainData = datasetDict[dataset_name](root=root, train=True, download=True,\n",
    "                                            transform=trainTransformDict[dataset_name])\n",
    "testData = datasetDict[dataset_name](root=root, train=False,\n",
    "                                        transform=testTransformDict[dataset_name])\n",
    "\n",
    "# initialize model\n",
    "if model_type == 'DenseNet':\n",
    "    model = torchvision.models.densenet121(pretrained=False)\n",
    "elif model_type == 'WideResNet':\n",
    "    class WideResNet(nn.Module):\n",
    "        def __init__(self, depth=28, widen_factor=10, dropout_rate=0.3, num_classes=10):\n",
    "            super().__init__()\n",
    "            self.in_planes = 16\n",
    "\n",
    "            assert ((depth-4)%6 ==0), 'Wide-resnet depth should be 6n+4'\n",
    "            n = (depth-4)/6\n",
    "            k = widen_factor\n",
    "\n",
    "            nStages = [16, 16*k, 32*k, 64*k]\n",
    "\n",
    "            class wide_basic(nn.Module):\n",
    "                def __init__(self, in_planes, planes, dropout_rate, stride=1):\n",
    "                    super(wide_basic, self).__init__()\n",
    "                    self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "                    self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, bias=True)\n",
    "                    self.dropout = nn.Dropout(p=dropout_rate)\n",
    "                    self.bn2 = nn.BatchNorm2d(planes)\n",
    "                    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "\n",
    "                    self.shortcut = nn.Sequential()\n",
    "                    if stride != 1 or in_planes != planes:\n",
    "                        self.shortcut = nn.Sequential(\n",
    "                            nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=True),\n",
    "                        )\n",
    "\n",
    "                def forward(self, x):\n",
    "                    out = self.dropout(self.conv1(F.relu(self.bn1(x))))\n",
    "                    out = self.conv2(F.relu(self.bn2(out)))\n",
    "                    out += self.shortcut(x)\n",
    "\n",
    "                    return out\n",
    "\n",
    "            self.conv1 = self.conv3x3(3,nStages[0])\n",
    "            self.layer1 = self._wide_layer(wide_basic, nStages[1], n, dropout_rate, stride=1)\n",
    "            self.layer2 = self._wide_layer(wide_basic, nStages[2], n, dropout_rate, stride=2)\n",
    "            self.layer3 = self._wide_layer(wide_basic, nStages[3], n, dropout_rate, stride=2)\n",
    "            self.bn1 = nn.BatchNorm2d(nStages[3], momentum=0.9)\n",
    "            self.linear = nn.Linear(nStages[3], num_classes)\n",
    "\n",
    "        def conv3x3(self, in_planes, out_planes, stride=1):\n",
    "            return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "\n",
    "        def _wide_layer(self, block, planes, num_blocks, dropout_rate, stride):\n",
    "            strides = [stride] + [1]*(int(num_blocks)-1)\n",
    "            layers = []\n",
    "\n",
    "            for stride in strides:\n",
    "                layers.append(block(self.in_planes, planes, dropout_rate, stride))\n",
    "                self.in_planes = planes\n",
    "\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.conv1(x)\n",
    "            out = self.layer1(out)\n",
    "            out = self.layer2(out)\n",
    "            out = self.layer3(out)\n",
    "            out = F.relu(self.bn1(out))\n",
    "            out = F.avg_pool2d(out, 8)\n",
    "            out = out.view(out.size(0), -1)\n",
    "            out = self.linear(out)\n",
    "\n",
    "            return out\n",
    "    model = WideResNet(num_classes=10 if dataset_name == 'cifar10' else 100)\n",
    "elif model_type == 'ResNeXt':\n",
    "    model = torchvision.models.resnext50_32x4d(pretrained=False)\n",
    "elif model_type == 'GoogLeNet':\n",
    "    class GoogleNet(torch.nn.Module):\n",
    "        def __init__(self, num_class=100):\n",
    "            super().__init__()\n",
    "\n",
    "            class Inception(torch.nn.Module):\n",
    "                def __init__(self, input_channels, n1x1, n3x3_reduce, n3x3, n5x5_reduce, n5x5, pool_proj):\n",
    "                    super().__init__()\n",
    "\n",
    "                    # 1x1conv branch\n",
    "                    self.b1 = nn.Sequential(\n",
    "                        nn.Conv2d(input_channels, n1x1, kernel_size=1),\n",
    "                        nn.BatchNorm2d(n1x1),\n",
    "                        nn.ReLU(inplace=True)\n",
    "                    )\n",
    "\n",
    "                    # 1x1conv -> 3x3conv branch\n",
    "                    self.b2 = nn.Sequential(\n",
    "                        nn.Conv2d(input_channels, n3x3_reduce, kernel_size=1),\n",
    "                        nn.BatchNorm2d(n3x3_reduce),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.Conv2d(n3x3_reduce, n3x3, kernel_size=3, padding=1),\n",
    "                        nn.BatchNorm2d(n3x3),\n",
    "                        nn.ReLU(inplace=True)\n",
    "                    )\n",
    "\n",
    "                    # 1x1conv -> 5x5conv branch\n",
    "                    # we use 2 3x3 conv filters stacked instead\n",
    "                    # of 1 5x5 filters to obtain the same receptive\n",
    "                    # field with fewer parameters\n",
    "                    self.b3 = nn.Sequential(\n",
    "                        nn.Conv2d(input_channels, n5x5_reduce, kernel_size=1),\n",
    "                        nn.BatchNorm2d(n5x5_reduce),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.Conv2d(n5x5_reduce, n5x5, kernel_size=3, padding=1),\n",
    "                        nn.BatchNorm2d(n5x5, n5x5),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n",
    "                        nn.BatchNorm2d(n5x5),\n",
    "                        nn.ReLU(inplace=True)\n",
    "                    )\n",
    "\n",
    "                    # 3x3pooling -> 1x1conv\n",
    "                    # same conv\n",
    "                    self.b4 = nn.Sequential(\n",
    "                        nn.MaxPool2d(3, stride=1, padding=1),\n",
    "                        nn.Conv2d(input_channels, pool_proj, kernel_size=1),\n",
    "                        nn.BatchNorm2d(pool_proj),\n",
    "                        nn.ReLU(inplace=True)\n",
    "                    )\n",
    "\n",
    "                def forward(self, x):\n",
    "                    return torch.cat([self.b1(x), self.b2(x), self.b3(x), self.b4(x)], dim=1)\n",
    "\n",
    "\n",
    "            self.prelayer = nn.Sequential(\n",
    "                nn.Conv2d(3, 192, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(192),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "            #although we only use 1 conv layer as prelayer,\n",
    "            #we still use name a3, b3.......\n",
    "            self.a3 = Inception(192, 64, 96, 128, 16, 32, 32)\n",
    "            self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "\n",
    "            #\"\"\"In general, an Inception network is a network consisting of\n",
    "            #modules of the above type stacked upon each other, with occasional\n",
    "            #max-pooling layers with stride 2 to halve the resolution of the\n",
    "            #grid\"\"\"\n",
    "            self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "            self.a4 = Inception(480, 192, 96, 208, 16, 48, 64)\n",
    "            self.b4 = Inception(512, 160, 112, 224, 24, 64, 64)\n",
    "            self.c4 = Inception(512, 128, 128, 256, 24, 64, 64)\n",
    "            self.d4 = Inception(512, 112, 144, 288, 32, 64, 64)\n",
    "            self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "\n",
    "            self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "            self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "            #input feature size: 8*8*1024\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.dropout = nn.Dropout2d(p=0.4)\n",
    "            self.linear = nn.Linear(1024, num_class)\n",
    "\n",
    "        def forward(self, x):\n",
    "            output = self.prelayer(x)\n",
    "            output = self.a3(output)\n",
    "            output = self.b3(output)\n",
    "\n",
    "            output = self.maxpool(output)\n",
    "\n",
    "            output = self.a4(output)\n",
    "            output = self.b4(output)\n",
    "            output = self.c4(output)\n",
    "            output = self.d4(output)\n",
    "            output = self.e4(output)\n",
    "\n",
    "            output = self.maxpool(output)\n",
    "\n",
    "            output = self.a5(output)\n",
    "            output = self.b5(output)\n",
    "\n",
    "            #\"\"\"It was found that a move from fully connected layers to\n",
    "            #average pooling improved the top-1 accuracy by about 0.6%,\n",
    "            #however the use of dropout remained essential even after\n",
    "            #removing the fully connected layers.\"\"\"\n",
    "            output = self.avgpool(output)\n",
    "            output = self.dropout(output)\n",
    "            output = output.view(output.size()[0], -1)\n",
    "            output = self.linear(output)\n",
    "\n",
    "            return output\n",
    "    model = GoogleNet(num_class=10 if dataset_name == 'cifar10' else 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e628bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Choosing Lp-Norm constraints\n",
    "#@markdown The following cell allows you to set Lp-norm constraints for the chosen network. For exact parameters both for the constraints and the optimizer see the last cell of this notebook.\n",
    "ord =  \"2\" #@param [1, 2, 5, 'inf']\n",
    "ord = float(ord)\n",
    "value = 10 #@param {type:\"number\"}\n",
    "mode = 'initialization' #@param ['initialization', 'radius', 'diameter']\n",
    "\n",
    "assert value > 0\n",
    "\n",
    "# Select constraints\n",
    "constraints = create_lp_constraints(model, ord=ord, value=value, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48b0d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Configuring the Frank-Wolfe Algorithm\n",
    "#@markdown Choose momentum and learning rate rescaling, see Section 3.1 of [arXiv:2010.07243](https://arxiv.org/pdf/2010.07243.pdf).\n",
    "momentum = 0.9 #@param {type:\"number\"}\n",
    "rescale = 'gradient' #@param ['gradient', 'diameter', 'None']\n",
    "rescale = None if rescale == 'None' else rescale\n",
    "\n",
    "#@markdown Choose a learning rate for SFW. You can activate the learning rate scheduler which automatically multiplies the current learning rate by `lr_decrease_factor` every `lr_step_size epochs`\n",
    "learning_rate = 0.1 #@param {type:\"number\"}\n",
    "lr_scheduler_active = True #@param {type:\"boolean\"}\n",
    "lr_decrease_factor = 0.1 #@param {type:\"number\"}\n",
    "lr_step_size = 60 #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown You can also enable retraction of the learning rate, i.e., if enabled the learning rate is increased and decreased automatically depending on the two moving averages of different length of the train loss over the epochs.\n",
    "retraction = True #@param {type:\"boolean\"}\n",
    "\n",
    "assert learning_rate > 0\n",
    "assert 0 <= momentum <= 1\n",
    "assert lr_decrease_factor > 0\n",
    "assert lr_step_size > 0\n",
    "\n",
    "\n",
    "# Select optimizer\n",
    "optimizer = SFW(params=model.parameters(), learning_rate=learning_rate, momentum=momentum, rescale=rescale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed44afd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 0/200\n",
      "Evaluation of train data:\n",
      "50000/50000: [===============================>] - ETA 0.6sss\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.3ss\n",
      "\n",
      " Finished epoch 0/200: Train Loss 6.907172891845703 | Test Loss 6.907180438232422 | Train Acc 0.0 | Test Acc 0.0\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "Training:\n",
      "50000/50000: [===============================>] - ETA 0.8sss\n",
      "Evaluation of test data:\n",
      "10000/10000: [===============================>] - ETA 0.2ss\n",
      "\n",
      " Finished epoch 1/200: Train Loss 1.6976631566619873 | Test Loss 1.3969239004135132 | Train Acc 0.41482 | Test Acc 0.5022\n",
      "\n",
      "\n",
      "Epoch 2/200\n",
      "Training:\n",
      " 1280/50000: [>...............................] - ETA 792.4s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 54>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     64\u001b[0m x_input, y_target \u001b[38;5;241m=\u001b[39m x_input\u001b[38;5;241m.\u001b[39mto(device), y_target\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Move to CUDA if possible\u001b[39;00m\n\u001b[0;32m     65\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the gradient buffers\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_criterion(output, y_target)\n\u001b[0;32m     68\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torchvision\\models\\densenet.py:215\u001b[0m, in \u001b[0;36mDenseNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 215\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(features, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    217\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(out, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torchvision\\models\\densenet.py:124\u001b[0m, in \u001b[0;36m_DenseBlock.forward\u001b[1;34m(self, init_features)\u001b[0m\n\u001b[0;32m    122\u001b[0m features \u001b[38;5;241m=\u001b[39m [init_features]\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 124\u001b[0m     new_features \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(new_features)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(features, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torchvision\\models\\densenet.py:90\u001b[0m, in \u001b[0;36m_DenseLayer.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     88\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_checkpoint_bottleneck(prev_features)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 90\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m new_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(bottleneck_output)))\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_rate \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torchvision\\models\\densenet.py:51\u001b[0m, in \u001b[0;36m_DenseLayer.bn_function\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbn_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: List[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     50\u001b[0m     concated_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(inputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcated_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: T484\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bottleneck_output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CS439\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    441\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    442\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@title Training the network\n",
    "#@markdown Choose the number of epochs and the size of each batch for training.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "nepochs = 200 #@param {type:\"integer\"}\n",
    "batch_size =  64#@param {type:\"integer\"}\n",
    "\n",
    "make_feasible(model, constraints)\n",
    "\n",
    "# define the loss object\n",
    "loss_criterion = torch.nn.CrossEntropyLoss().to(device=device)\n",
    "model = model.to(device=device)\n",
    "\n",
    "# Loaders\n",
    "trainLoader = torch.utils.data.DataLoader(trainData, batch_size=batch_size, shuffle=True,\n",
    "                        pin_memory=torch.cuda.is_available(), num_workers=2)\n",
    "testLoader = torch.utils.data.DataLoader(testData, batch_size=batch_size, shuffle=False,\n",
    "                        pin_memory=torch.cuda.is_available(), num_workers=2)\n",
    "\n",
    "# initialize some necessary metrics objects\n",
    "train_loss, train_accuracy = AverageMeter(), AverageMeter()\n",
    "test_loss, test_accuracy = AverageMeter(), AverageMeter()\n",
    "\n",
    "if lr_scheduler_active:\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=lr_step_size, gamma=lr_decrease_factor)\n",
    "\n",
    "if retraction:\n",
    "    retractionScheduler = RetractionLR(optimizer=optimizer)\n",
    "\n",
    "# function to reset metrics\n",
    "def reset_metrics():\n",
    "    train_loss.reset()\n",
    "    train_accuracy.reset()\n",
    "\n",
    "    test_loss.reset()\n",
    "    test_accuracy.reset()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(data='train'):\n",
    "    if data == 'train':\n",
    "        loader = trainLoader\n",
    "        mean_loss, mean_accuracy = train_loss, train_accuracy\n",
    "    elif data == 'test':\n",
    "        loader = testLoader\n",
    "        mean_loss, mean_accuracy = test_loss, test_accuracy\n",
    "\n",
    "    sys.stdout.write(f\"Evaluation of {data} data:\\n\")\n",
    "    for x_input, y_target in Bar(loader):\n",
    "        x_input, y_target = x_input.to(device), y_target.to(device)  # Move to CUDA if possible\n",
    "        output = model.eval()(x_input)\n",
    "        loss = loss_criterion(output, y_target)\n",
    "        mean_loss(loss.item(), len(y_target))\n",
    "        mean_accuracy(Utilities.categorical_accuracy(y_true=y_target, output=output), len(y_target))\n",
    "\n",
    "for epoch in range(nepochs + 1):\n",
    "    reset_metrics()\n",
    "    sys.stdout.write(f\"\\n\\nEpoch {epoch}/{nepochs}\\n\")\n",
    "    if epoch == 0:\n",
    "        # Just evaluate the model once to get the metrics\n",
    "        evaluate_model(data='train')\n",
    "    else:\n",
    "        # Train\n",
    "        sys.stdout.write(f\"Training:\\n\")\n",
    "        for x_input, y_target in Bar(trainLoader):\n",
    "            x_input, y_target = x_input.to(device), y_target.to(device)  # Move to CUDA if possible\n",
    "            optimizer.zero_grad()  # Zero the gradient buffers\n",
    "            output = model.train()(x_input)\n",
    "            loss = loss_criterion(output, y_target)\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step(constraints=constraints)\n",
    "            train_loss(loss.item(), len(y_target))\n",
    "            train_accuracy(Utilities.categorical_accuracy(y_true=y_target, output=output), len(y_target))\n",
    "        if lr_scheduler_active:\n",
    "            scheduler.step()\n",
    "        if retraction:\n",
    "            # Learning rate retraction\n",
    "            retractionScheduler.update_averages(train_loss.result())\n",
    "            retractionScheduler.step()\n",
    "\n",
    "    evaluate_model(data='test')\n",
    "    sys.stdout.write(f\"\\n Finished epoch {epoch}/{nepochs}: Train Loss {train_loss.result()} | Test Loss {test_loss.result()} | Train Acc {train_accuracy.result()} | Test Acc {test_accuracy.result()}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS439",
   "language": "python",
   "name": "cs439"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
