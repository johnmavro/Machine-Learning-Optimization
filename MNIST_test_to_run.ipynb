{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/Othercomputers/Il mio laptop/Machine-Learning-Optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX5R_9rnqw32",
        "outputId": "b3b55746-484b-43a7-ed82-c6170dec4af5"
      },
      "id": "DX5R_9rnqw32",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/Othercomputers/Il mio laptop/Machine-Learning-Optimization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dda1f696",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dda1f696",
        "outputId": "c787629f-db7f-432f-bfd5-c56ee4d2dac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting barbar\n",
            "  Downloading barbar-0.2.1-py3-none-any.whl (3.9 kB)\n",
            "Installing collected packages: barbar\n",
            "Successfully installed barbar-0.2.1\n"
          ]
        }
      ],
      "source": [
        "from Frank_Wolfe.utils.utils import *\n",
        "from Frank_Wolfe.SFW import *\n",
        "from Frank_Wolfe.constraints.constraints import *\n",
        "from Frank_Wolfe.architectures import *\n",
        "!pip install barbar\n",
        "from barbar import Bar\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "device = is_cuda_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45877559",
      "metadata": {
        "id": "45877559"
      },
      "outputs": [],
      "source": [
        "save_stats = True\n",
        "save_figs = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1db40586",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1db40586",
        "outputId": "09c94cd8-7f15-4b4a-8d05-5b693379c508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "#@title Choose dataset name and architecture of the network \n",
        "\n",
        "dataset_name = 'CIFAR10' #@param ['CIFAR10', 'CIFAR100']\n",
        "model_type = 'GoogLeNet' #@param ['DenseNet', 'WideResNet', 'GoogLeNet', 'ResNeXt']\n",
        "if model_type == 'GoogLeNet':\n",
        "  model = GoogleNet(num_class=10 if dataset_name == 'CIFAR10' else 100)\n",
        "elif model_type == 'DenseNet':\n",
        "  model = torchvision.models.densenet121(pretrained=False)\n",
        "elif model_type == 'ResNeXt':\n",
        "  model = torchvision.models.resnet101(pretrained=False)\n",
        "elif model_type == 'WideResNet':\n",
        "  model =  WideResNet(num_classes=10 if dataset_name == 'CIFAR10' else 100)\n",
        "else:\n",
        "  raise ValueError(\"Please, select an available architecture\")\n",
        "\n",
        "datasetDict = setDatasetAttributes(dataset_name)\n",
        "trainTransformDict, testTransformDict = setTrainAndTest(dataset_name)\n",
        "\n",
        "root = f\"{dataset_name}-dataset\"\n",
        "\n",
        "trainData = datasetDict['datasetDict'](root=root, train=True, download=True,\n",
        "                                            transform=trainTransformDict[dataset_name])\n",
        "testData = datasetDict['datasetDict'](root=root, train=False,\n",
        "                                        transform=testTransformDict[dataset_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee891cc5",
      "metadata": {
        "id": "ee891cc5"
      },
      "outputs": [],
      "source": [
        "#@title Choosing Lp-Norm constraints\n",
        "\n",
        "ord =  2 #@param [1, 2, 5, 'inf']\n",
        "ord = float(ord)\n",
        "value = 10 #@param {type:\"number\"}\n",
        "mode = 'initialization' #@param ['initialization', 'radius', 'diameter']\n",
        "\n",
        "assert value > 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6434095b",
      "metadata": {
        "id": "6434095b"
      },
      "outputs": [],
      "source": [
        "#@title Configuring the Frank-Wolfe Algorithm\n",
        "#@markdown Choose momentum and learning rate rescaling, see Section 3.1 of [arXiv:2010.07243](https://arxiv.org/pdf/2010.07243.pdf).\n",
        "momentum = 0.9 #@param {type:\"number\"}\n",
        "rescale = 'gradient' #@param ['gradient', 'diameter', 'None']\n",
        "rescale = None if rescale == 'None' else rescale\n",
        "\n",
        "#@markdown Choose a learning rate for SFW. You can activate the learning rate scheduler which automatically multiplies the current learning rate by `lr_decrease_factor` every `lr_step_size epochs`\n",
        "learning_rate = 0.1 #@param {type:\"number\"}\n",
        "lr_scheduler_active = True #@param {type:\"boolean\"}\n",
        "lr_decrease_factor = 0.1 #@param {type:\"number\"}\n",
        "lr_step_size = 60 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown You can also enable retraction of the learning rate, i.e., if enabled the learning rate is increased and decreased automatically depending on the two moving averages of different length of the train loss over the epochs.\n",
        "retraction = True #@param {type:\"boolean\"}\n",
        "\n",
        "assert learning_rate > 0\n",
        "assert 0 <= momentum <= 1\n",
        "assert lr_decrease_factor > 0\n",
        "assert lr_step_size > 0\n",
        "\n",
        "# Select optimizer\n",
        "optimizer = SFW(params=model.parameters(), learning_rate=learning_rate, momentum=momentum, rescale=rescale)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "different_norms_dict_other = {}"
      ],
      "metadata": {
        "id": "cvnvnK30y7GF"
      },
      "id": "cvnvnK30y7GF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fe3337b7",
      "metadata": {
        "id": "fe3337b7"
      },
      "source": [
        "### We analyse now the dependence of the performance on the choice of the norm for the constraint, for a fixed $value$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "epochs_times = []\n",
        "\n",
        "nepochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "# check cuda availability\n",
        "device = is_cuda_available()\n",
        "\n",
        "constraints = create_lp_constraints(model, ord=ord, value=value, mode=mode)\n",
        "\n",
        "make_feasible(model, constraints)\n",
        "\n",
        "# define the loss object\n",
        "loss_criterion = torch.nn.CrossEntropyLoss().to(device=device)\n",
        "model = model.to(device=device)\n",
        "\n",
        "# Loaders\n",
        "trainLoader = torch.utils.data.DataLoader(trainData, batch_size=batch_size, shuffle=True,\n",
        "                                      pin_memory=torch.cuda.is_available(), num_workers=2)\n",
        "testLoader = torch.utils.data.DataLoader(testData, batch_size=batch_size, shuffle=False,\n",
        "                                      pin_memory=torch.cuda.is_available(), num_workers=2)\n",
        "\n",
        "# initialize some necessary metrics objects\n",
        "train_loss, train_accuracy = AverageMeter(), AverageMeter()\n",
        "test_loss, test_accuracy = AverageMeter(), AverageMeter()\n",
        "\n",
        "if lr_scheduler_active:\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=lr_step_size,\n",
        "                                                gamma=lr_decrease_factor)\n",
        "\n",
        "if retraction:\n",
        "    retractionScheduler = RetractionLR(optimizer=optimizer)\n",
        "\n",
        "# function to reset metrics\n",
        "def reset_metrics():\n",
        "    train_loss.reset()\n",
        "    train_accuracy.reset()\n",
        "    test_loss.reset()\n",
        "    test_accuracy.reset()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(data=\"train\"):\n",
        "    if data == \"train\":\n",
        "        loader = trainLoader\n",
        "        mean_loss, mean_accuracy = train_loss, train_accuracy\n",
        "    elif data == \"test\":\n",
        "        loader = testLoader\n",
        "        mean_loss, mean_accuracy = test_loss, test_accuracy\n",
        "\n",
        "    sys.stdout.write(f\"Evaluation of {data} data:\\n\")\n",
        "    for x_input, y_target in Bar(loader):\n",
        "        x_input, y_target = x_input.to(device), y_target.to(device)  # Move to CUDA if possible\n",
        "        output = model.eval()(x_input)\n",
        "        loss = loss_criterion(output, y_target)\n",
        "        mean_loss(loss.item(), len(y_target))\n",
        "        mean_accuracy(Utilities.categorical_accuracy(y_true=y_target, output=output), len(y_target))\n",
        "\n",
        "\n",
        "for epoch in range(nepochs + 1):\n",
        "    start = time.time()\n",
        "    reset_metrics()\n",
        "    sys.stdout.write(f\"\\n\\nEpoch {epoch}/{nepochs}\\n\")\n",
        "    if epoch == 0:\n",
        "        # Just evaluate the model once to get the metrics\n",
        "        evaluate_model(data='train')\n",
        "    else:\n",
        "        # Train\n",
        "        sys.stdout.write(f\"Training:\\n\")\n",
        "        for x_input, y_target in Bar(trainLoader):\n",
        "            x_input, y_target = x_input.to(device), y_target.to(device)  # Move to CUDA if possible\n",
        "            optimizer.zero_grad()  # Zero the gradient buffers\n",
        "            output = model.train()(x_input)\n",
        "            loss = loss_criterion(output, y_target)\n",
        "            loss.backward()  # Backpropagation\n",
        "            optimizer.step(constraints=constraints)\n",
        "            train_loss(loss.item(), len(y_target))\n",
        "            train_accuracy(Utilities.categorical_accuracy(y_true=y_target, output=output), len(y_target))\n",
        "\n",
        "        if lr_scheduler_active:\n",
        "            scheduler.step()\n",
        "        if retraction:\n",
        "            # Learning rate retraction\n",
        "            retractionScheduler.update_averages(train_loss.result())\n",
        "            retractionScheduler.step()\n",
        "\n",
        "    evaluate_model(data='test')\n",
        "    sys.stdout.write(f\"\\n Finished epoch {epoch}/{nepochs}: Train Loss {train_loss.result()} | Test Loss {test_loss.result()} | Train Acc {train_accuracy.result()} | Test Acc {test_accuracy.result()}\\n\")\n",
        "\n",
        "    train_losses.append(train_loss.result())\n",
        "    train_accuracies.append(train_accuracy.result())\n",
        "    test_losses.append(test_loss.result())\n",
        "    test_accuracies.append(test_accuracy.result())\n",
        "\n",
        "\n",
        "    elapsed_time = time.time()-start\n",
        "    sys.stdout.write(f\"Time elapsed for the current epoch {elapsed_time}\")\n",
        "    epochs_times.append(elapsed_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYBPt-kWqblh",
        "outputId": "f7ea0d4d-9a1a-4db9-a111-08297f3ae50b"
      },
      "id": "VYBPt-kWqblh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 0/10\n",
            "Evaluation of train data:\n",
            "50000/50000: [===============================>] - ETA 0.3s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.3s\n",
            "\n",
            " Finished epoch 0/10: Train Loss 2.3027064724731447 | Test Loss 2.302706488800049 | Train Acc 0.1 | Test Acc 0.1\n",
            "Time elapsed for the current epoch 160.03240823745728\n",
            "\n",
            "Epoch 1/10\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.9s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.3s\n",
            "\n",
            " Finished epoch 1/10: Train Loss 1.314090299320221 | Test Loss 1.117825666809082 | Train Acc 0.5187 | Test Acc 0.5981\n",
            "Time elapsed for the current epoch 613.5114524364471\n",
            "\n",
            "Epoch 2/10\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.9s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.2s\n",
            "\n",
            " Finished epoch 2/10: Train Loss 0.8299031141281128 | Test Loss 0.8585513844490051 | Train Acc 0.71022 | Test Acc 0.7144\n",
            "Time elapsed for the current epoch 616.481103181839\n",
            "\n",
            "Epoch 3/10\n",
            "Training:\n",
            " 3200/50000: [==>.............................] - ETA 619.3s"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "current_dict = {'epochs': nepochs, 'train_losses': train_losses, 'test_losses': test_losses,\n",
        "            'train_accuracies': train_accuracies, 'test_accuracies': test_accuracies, 'elapsed_time': elapsed_time}\n",
        "different_norms_dict_other.update({ord: current_dict})"
      ],
      "metadata": {
        "id": "2NBj9eKQxtIk"
      },
      "id": "2NBj9eKQxtIk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save everything onto file\n",
        "if save_stats: \n",
        "    output_folder = os.path.join(os.getcwd(), 'results')  # set the folder\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    fname = output_folder + '/stats_dict_different_norms_other_list_' + dataset_name + '_' + model_type + '.pkl'\n",
        "    with open(fname, 'wb') as handle:\n",
        "        pickle.dump(different_norms_dict_other, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "L1OL6gZUvg8_"
      },
      "id": "L1OL6gZUvg8_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "different_norms_dict_other"
      ],
      "metadata": {
        "id": "5orp5_ziAokP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd60ae21-bbbb-44a1-b372-c85d124cb6b4"
      },
      "id": "5orp5_ziAokP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {'elapsed_time': 606.9135293960571,\n",
              "  'epochs': 10,\n",
              "  'test_accuracies': [0.1,\n",
              "   0.6934,\n",
              "   0.7447,\n",
              "   0.8048,\n",
              "   0.8194,\n",
              "   0.8175,\n",
              "   0.7918,\n",
              "   0.828,\n",
              "   0.8265,\n",
              "   0.8203,\n",
              "   0.8568],\n",
              "  'test_losses': [2.3026767009735107,\n",
              "   0.8827920928955079,\n",
              "   0.73717989153862,\n",
              "   0.5900344198226929,\n",
              "   0.5351551820516587,\n",
              "   0.5244382320404053,\n",
              "   0.6094213529586792,\n",
              "   0.5032419197082519,\n",
              "   0.5103842622995377,\n",
              "   0.5290620605945587,\n",
              "   0.42982167949676514],\n",
              "  'train_accuracies': [0.1,\n",
              "   0.55534,\n",
              "   0.73572,\n",
              "   0.79454,\n",
              "   0.8255,\n",
              "   0.84164,\n",
              "   0.8514,\n",
              "   0.86056,\n",
              "   0.86748,\n",
              "   0.87194,\n",
              "   0.87668],\n",
              "  'train_losses': [2.302676717681885,\n",
              "   1.2216925020980836,\n",
              "   0.7586758876609803,\n",
              "   0.601818448047638,\n",
              "   0.5193995813941955,\n",
              "   0.46962812870025633,\n",
              "   0.4394497441673279,\n",
              "   0.41368210065841676,\n",
              "   0.3916200085258484,\n",
              "   0.3789891492748261,\n",
              "   0.3645921782207489]}}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "CS439",
      "language": "python",
      "name": "cs439"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "MNIST_test_to_run.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}