{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1f696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Frank_Wolfe.utils.utils import *\n",
    "from Frank_Wolfe.SFW import *\n",
    "from Frank_Wolfe.constraints.constraints import *\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "device = is_cuda_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45877559",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_stats = True\n",
    "save_figs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1914fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleNet(torch.nn.Module):\n",
    "        def __init__(self, num_class=100):\n",
    "            super().__init__()\n",
    "\n",
    "            class Inception(torch.nn.Module):\n",
    "                def __init__(self, input_channels, n1x1, n3x3_reduce, n3x3, n5x5_reduce, n5x5, pool_proj):\n",
    "                    super().__init__()\n",
    "\n",
    "                    # 1x1conv branch\n",
    "                    self.b1 = nn.Sequential(\n",
    "                        nn.Conv2d(input_channels, n1x1, kernel_size=1),\n",
    "                        nn.BatchNorm2d(n1x1),\n",
    "                        nn.ReLU(inplace=True)\n",
    "                    )\n",
    "\n",
    "                    # 1x1conv -> 3x3conv branch\n",
    "                    self.b2 = nn.Sequential(\n",
    "                        nn.Conv2d(input_channels, n3x3_reduce, kernel_size=1),\n",
    "                        nn.BatchNorm2d(n3x3_reduce),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.Conv2d(n3x3_reduce, n3x3, kernel_size=3, padding=1),\n",
    "                        nn.BatchNorm2d(n3x3),\n",
    "                        nn.ReLU(inplace=True)\n",
    "                    )\n",
    "\n",
    "                    # 1x1conv -> 5x5conv branch\n",
    "                    # we use 2 3x3 conv filters stacked instead\n",
    "                    # of 1 5x5 filters to obtain the same receptive\n",
    "                    # field with fewer parameters\n",
    "                    self.b3 = nn.Sequential(\n",
    "                        nn.Conv2d(input_channels, n5x5_reduce, kernel_size=1),\n",
    "                        nn.BatchNorm2d(n5x5_reduce),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.Conv2d(n5x5_reduce, n5x5, kernel_size=3, padding=1),\n",
    "                        nn.BatchNorm2d(n5x5, n5x5),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n",
    "                        nn.BatchNorm2d(n5x5),\n",
    "                        nn.ReLU(inplace=True)\n",
    "                    )\n",
    "\n",
    "                    # 3x3pooling -> 1x1conv\n",
    "                    # same conv\n",
    "                    self.b4 = nn.Sequential(\n",
    "                        nn.MaxPool2d(3, stride=1, padding=1),\n",
    "                        nn.Conv2d(input_channels, pool_proj, kernel_size=1),\n",
    "                        nn.BatchNorm2d(pool_proj),\n",
    "                        nn.ReLU(inplace=True)\n",
    "                    )\n",
    "\n",
    "                def forward(self, x):\n",
    "                    return torch.cat([self.b1(x), self.b2(x), self.b3(x), self.b4(x)], dim=1)\n",
    "\n",
    "\n",
    "            self.prelayer = nn.Sequential(\n",
    "                nn.Conv2d(3, 192, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(192),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "            #although we only use 1 conv layer as prelayer,\n",
    "            #we still use name a3, b3.......\n",
    "            self.a3 = Inception(192, 64, 96, 128, 16, 32, 32)\n",
    "            self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "\n",
    "            #\"\"\"In general, an Inception network is a network consisting of\n",
    "            #modules of the above type stacked upon each other, with occasional\n",
    "            #max-pooling layers with stride 2 to halve the resolution of the\n",
    "            #grid\"\"\"\n",
    "            self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "            self.a4 = Inception(480, 192, 96, 208, 16, 48, 64)\n",
    "            self.b4 = Inception(512, 160, 112, 224, 24, 64, 64)\n",
    "            self.c4 = Inception(512, 128, 128, 256, 24, 64, 64)\n",
    "            self.d4 = Inception(512, 112, 144, 288, 32, 64, 64)\n",
    "            self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "\n",
    "            self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "            self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "            #input feature size: 8*8*1024\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.dropout = nn.Dropout2d(p=0.4)\n",
    "            self.linear = nn.Linear(1024, num_class)\n",
    "\n",
    "        def forward(self, x):\n",
    "            output = self.prelayer(x)\n",
    "            output = self.a3(output)\n",
    "            output = self.b3(output)\n",
    "\n",
    "            output = self.maxpool(output)\n",
    "\n",
    "            output = self.a4(output)\n",
    "            output = self.b4(output)\n",
    "            output = self.c4(output)\n",
    "            output = self.d4(output)\n",
    "            output = self.e4(output)\n",
    "\n",
    "            output = self.maxpool(output)\n",
    "\n",
    "            output = self.a5(output)\n",
    "            output = self.b5(output)\n",
    "\n",
    "            #\"\"\"It was found that a move from fully connected layers to\n",
    "            #average pooling improved the top-1 accuracy by about 0.6%,\n",
    "            #however the use of dropout remained essential even after\n",
    "            #removing the fully connected layers.\"\"\"\n",
    "            output = self.avgpool(output)\n",
    "            output = self.dropout(output)\n",
    "            output = output.view(output.size()[0], -1)\n",
    "            output = self.linear(output)\n",
    "\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db40586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Select dataset and model\n",
    "#@markdown While the code also supports also the ImageNet-dataset, only CIFAR-10 and CIFAR-100 are selectable\n",
    "#@markdown - [DenseNet-121](https://arxiv.org/pdf/1608.06993.pdf)\n",
    "#@markdown - [WideResNet-28x10](https://arxiv.org/pdf/1605.07146v2.pdf)\n",
    "#@markdown - [GoogLeNet](https://arxiv.org/pdf/1409.4842v1.pdf)\n",
    "#@markdown - [ResNext50](https://arxiv.org/pdf/1611.05431.pdf)\n",
    "\n",
    "# select hyperparameters\n",
    "dataset_name = 'CIFAR10' #@param ['CIFAR10', 'CIFAR100']\n",
    "model_type = 'mlp' #@param ['mlp', DenseNet', 'WideResNet', 'GoogLeNet', 'ResNeXt']\n",
    "model = GoogleNet(num_class=10)\n",
    "datasetDict = setDatasetAttributes(dataset_name)\n",
    "trainTransformDict, testTransformDict = setTrainAndTest(dataset_name)\n",
    "\n",
    "root = f\"{dataset_name}-dataset\"\n",
    "\n",
    "trainData = datasetDict['datasetDict'](root=root, train=True, download=True,\n",
    "                                            transform=trainTransformDict[dataset_name])\n",
    "testData = datasetDict['datasetDict'](root=root, train=False,\n",
    "                                        transform=testTransformDict[dataset_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee891cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Choosing Lp-Norm constraints\n",
    "#@markdown The following cell allows you to set Lp-norm constraints for the chosen network. For exact parameters both for the constraints and the optimizer see the last cell of this notebook.\n",
    "ord =  \"2\" #@param [1, 2, 5, 'inf']\n",
    "ord = float(ord)\n",
    "value = 10 #@param {type:\"number\"}\n",
    "mode = 'initialization' #@param ['initialization', 'radius', 'diameter']\n",
    "\n",
    "assert value > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6434095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Configuring the Frank-Wolfe Algorithm\n",
    "#@markdown Choose momentum and learning rate rescaling, see Section 3.1 of [arXiv:2010.07243](https://arxiv.org/pdf/2010.07243.pdf).\n",
    "momentum = 0.9 #@param {type:\"number\"}\n",
    "rescale = 'gradient' #@param ['gradient', 'diameter', 'None']\n",
    "rescale = None if rescale == 'None' else rescale\n",
    "\n",
    "#@markdown Choose a learning rate for SFW. You can activate the learning rate scheduler which automatically multiplies the current learning rate by `lr_decrease_factor` every `lr_step_size epochs`\n",
    "learning_rate = 0.1 #@param {type:\"number\"}\n",
    "lr_scheduler_active = True #@param {type:\"boolean\"}\n",
    "lr_decrease_factor = 0.1 #@param {type:\"number\"}\n",
    "lr_step_size = 60 #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown You can also enable retraction of the learning rate, i.e., if enabled the learning rate is increased and decreased automatically depending on the two moving averages of different length of the train loss over the epochs.\n",
    "retraction = True #@param {type:\"boolean\"}\n",
    "\n",
    "assert learning_rate > 0\n",
    "assert 0 <= momentum <= 1\n",
    "assert lr_decrease_factor > 0\n",
    "assert lr_step_size > 0\n",
    "\n",
    "# Select optimizer\n",
    "optimizer = SFW(params=model.parameters(), learning_rate=learning_rate, momentum=momentum, rescale=rescale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3337b7",
   "metadata": {},
   "source": [
    "### We analyse now the dependence of the performance on the choice of the norm for the constraint, for a fixed $value$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c8adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 10\n",
    "batch_size = 64\n",
    "value = 10\n",
    "mode = 'initialization'\n",
    "orders = [\"1\", \"2\", \"inf\"]\n",
    "different_norms_dict = {}\n",
    "for ord in orders:\n",
    "    sys.stdout.write(f\"------------ Collecting results with L_{ord} constraints ---------\")\n",
    "    ord = float(ord)\n",
    "    constraints = create_lp_constraints(model, ord=ord, value=value, mode=mode)\n",
    "    train_losses, test_losses, train_accuracies, test_accuracies, elapsed_time = train_network(nepochs, batch_size,\n",
    "                                                                     model, constraints, trainData, testData, optimizer)\n",
    "    current_dict = {'epochs': nepochs, 'train_losses': train_losses, 'test_losses': test_losses,\n",
    "            'train_accuracies': train_accuracies, 'test_accuracies': test_accuracies, 'elapsed_time': elapsed_time}\n",
    "    different_norms_dict.update({ord: current_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cf623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save everything onto file\n",
    "if save_stats: \n",
    "    output_folder = os.path.join(os.getcwd(), 'results')  # set the folder\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    fname = output_folder + '/stats_dict_different_norms_list.pkl'\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pickle.dump(different_norms_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS439",
   "language": "python",
   "name": "cs439"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
