{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf28e293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from Frank_Wolfe.DFW import *\n",
    "from Frank_Wolfe.utils import *\n",
    "from Frank_Wolfe.MultiClassHingeLoss import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "834e829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_stats = True\n",
    "save_figs = True\n",
    "load = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5b0cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted_logits, reference):\n",
    "    \"\"\"\n",
    "    Compute the ratio of correctly predicted labels\n",
    "    :param predicted_logits: float32 tensor of shape (batch size, num classes)\n",
    "                        The logits predicted by the model\n",
    "    :param reference: int64 tensor of shape (batch_size) with the class number\n",
    "                        Ground-truth labels\n",
    "    :return: accuracy: float\n",
    "    \"\"\"\n",
    "\n",
    "    labels = torch.argmax(predicted_logits, 1)\n",
    "    correct_predictions = labels.eq(reference)\n",
    "    return correct_predictions.sum().float() / correct_predictions.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "385b8bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we download and normalize the datasets\n",
    "\n",
    "ts = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0,), (1,))])\n",
    "mnist_trainset = datasets.MNIST('../data', train=True, download=True, transform=ts)\n",
    "mnist_testset = datasets.MNIST(root='../data', train=False, download=True, transform=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e023e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "# we import the dataloaders\n",
    "\n",
    "dataset_test = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../data', train=False, download=True, transform=torchvision.transforms.ToTensor()), \n",
    "  batch_size=100,\n",
    "  shuffle=True\n",
    ")\n",
    "dataset_train = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../data', train=True, download=True, transform=torchvision.transforms.ToTensor()),\n",
    "  batch_size=batch_size,\n",
    "  shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "196b3a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "hidden_size = 1500\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e29b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture definition\n",
    "\n",
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "    \n",
    "    # fully connected neural network with 2 hidden layers with 1500 neurons each. We use ReLU activation functions\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MultiLayerPerceptron,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_in = nn.Linear(self.input_size,self.hidden_size,bias=True) # fully connected input_layer\n",
    "        self.fc_hid_1 = nn.Linear(self.hidden_size,self.hidden_size,bias=True) # fully connected hidden_layer_1\n",
    "        self.fc_hid_2 = nn.Linear(self.hidden_size,self.hidden_size,bias=True) # fully connected hidden_layer_2\n",
    "        self.fc_out = nn.Linear(self.hidden_size,self.output_size,bias=True)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, self.input_size)\n",
    "        x = self.relu(self.fc_in(x))\n",
    "        x = self.relu(self.fc_hid_1(x))\n",
    "        x = self.relu(self.fc_hid_2(x))\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7cd9664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model\n",
    "mlp = MultiLayerPerceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "405bc61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "criterion = MultiClassHingeLoss()\n",
    "\n",
    "optimizer = DFW(mlp.parameters(), eta=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b503191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset_train, dataset_test, optimizer, criterion, epochs, scheduler=None, lr_decrease=True):\n",
    "    \"\"\"\n",
    "    The function is used to train the neural network\n",
    "    :param model: <class '__main__.MultiLayerPerceptron'>\n",
    "                    The model we wish to train\n",
    "    :param dataset_train: <class 'torch.utils.data.dataloader.DataLoader'>\n",
    "                    The train pytorch dataloader\n",
    "    :param dataset_test: <class 'torch.utils.data.dataloader.DataLoader'>\n",
    "                    The test pytorch dataloader\n",
    "    :param optimizer: <class 'torch.optim.sgd.SGD'>\n",
    "                    The used pytorch optimizer\n",
    "    :param criterion: <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
    "                    The loss used during the training\n",
    "    :param epochs: int\n",
    "                    The number of epochs\n",
    "    :return: train losses, accuracies: lists of training losses and test accuracies respectively\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs): # loop over the dataset multiple times\n",
    "        epoch_loss = 0.0 \n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        n_steps = 0\n",
    "        for batch_x, batch_y in dataset_train:\n",
    "            n_steps = n_steps+1\n",
    "            # batch_x,batch_y = batch_x.to(device),batch_y.to(device)\n",
    "\n",
    "            #Get output and evaluate with loss function\n",
    "            predictions = model(batch_x)\n",
    "            loss = criterion(predictions,batch_y)\n",
    "            running_loss += loss.item() * len(batch_y)\n",
    "    \n",
    "            #Initialize optimizer\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            #Update the network\n",
    "            optimizer.step(lambda: float(loss), model, batch_x, batch_y)\n",
    "            \n",
    "        running_loss = running_loss / n_steps\n",
    "        train_losses.append(running_loss)\n",
    "        print(running_loss)\n",
    "        \n",
    "        #Test the quality on the test set\n",
    "        model.eval()\n",
    "        accuracies_test = []\n",
    "        \n",
    "        for batch_x, batch_y in dataset_test:\n",
    "            # batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            # Evaluate the network (forward pass)\n",
    "            prediction = model(batch_x)\n",
    "            accuracies_test.append(accuracy(prediction, batch_y))\n",
    "    \n",
    "        print(\"Epoch {} | Test accuracy: {:.5f}\".format(epoch, sum(accuracies_test).item()/len(accuracies_test)))\n",
    "        \n",
    "        accuracies.append(sum(accuracies_test).item()/len(accuracies_test))\n",
    "    return train_losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d9a0bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single step\n",
      "Single step\n",
      "Single step\n",
      "Single step\n",
      "Single step\n",
      "Single step\n",
      "Single step\n",
      "Single step\n",
      "Single step\n",
      "Single step\n",
      "More than one step\n",
      "Single step\n",
      "Single step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "1007.2094698747\n",
      "Epoch 0 | Test accuracy: 0.45760\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "Single step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "Single step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "Single step\n",
      "Single step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n",
      "More than one step\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#train using DFW\n",
    "train_losses, accuracies_test = train_model(mlp, dataset_train, dataset_test, optimizer, criterion, num_epochs)\n",
    "\n",
    "end = time.time()\n",
    "print('\\n')\n",
    "print('The total time spent is', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10784159",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(num_epochs), accuracies_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b2c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dict = {'epochs': num_epochs, 'train_losses': train_losses, 'test_accuracies': accuracies_test}\n",
    "results.update({'SGD': current_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e085029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save everything onto file\n",
    "if save_stats: \n",
    "    output_folder = os.path.join(os.getcwd(), 'results')  # set the folder\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    fname = output_folder + '/stats_dict_DFW.pkl'\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load:\n",
    "    output_folder = os.path.join(os.getcwd(), 'results')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    fname = output_folder + '/stats_dict_DFW.pkl'\n",
    "    with open(fname, 'rb') as handle:\n",
    "        stats_dict_DFW = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610892d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dict_DFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12377cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_DFW = stats_dict_DFW['DFW']['train_losses']\n",
    "train_losses_SGD = stats_dict_DFW['SGD']['train_losses']\n",
    "test_accuracies_DFW = stats_dict_DFW['DFW']['test_accuracies']\n",
    "test_accuracies_SGD = stats_dict_DFW['SGD']['test_accuracies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b6ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "num_epochs = 200\n",
    "plt.semilogx(np.arange(num_epochs), train_losses_DFW, label=\"DFW\")\n",
    "plt.semilogx(np.arange(num_epochs), train_losses_SGD, label=\"SGD\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Training loss for SGD and Deep Frank-Wolfe\")\n",
    "plt.savefig(\"losses.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "num_epochs = 200\n",
    "plt.plot(np.arange(num_epochs), test_accuracies_DFW, label=\"DFW\")\n",
    "plt.plot(np.arange(num_epochs), test_accuracies_SGD, label=\"SGD\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Test accuracy for SGD and Deep Frank-Wolfe\")\n",
    "plt.savefig(\"accuracies.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610290e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS439",
   "language": "python",
   "name": "cs439"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
