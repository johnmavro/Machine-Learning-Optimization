{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KDbRlCKv4U8",
        "outputId": "2920238a-95bb-453a-95c6-0138a804af3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 1.11.0+cu113\n",
            "Torchvision Version: 0.12.0+cu113\n",
            "GPU is available? True\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms, utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "from utilities import *\n",
        "\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "print(\"Torchvision Version:\", torchvision.__version__)\n",
        "print(\"GPU is available?\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "nYF1cnvPwthx"
      },
      "outputs": [],
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqvTMJAbw74Q"
      },
      "source": [
        "# Imported datasets\n",
        "For the testing and comparison of our algorithms we will use the following datasets:\n",
        "\n",
        "1. MNIST\n",
        "2. FashionMNIST\n",
        "3. CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Lv4ND2pMtC7V"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "icE91aqaw110"
      },
      "outputs": [],
      "source": [
        "ts = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0,), (1,))])\n",
        "mnist_trainset = datasets.MNIST('../data', train=True, download=True, transform=ts)\n",
        "mnist_testset = datasets.MNIST(root='../data', train=False, download=True, transform=ts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fmnist_trainset = datasets.FashionMNIST('../data', train=True, download=True, transform=ts)\n",
        "fmnist_testset = datasets.FashionMNIST(root='../data', train=False, download=True, transform=ts)"
      ],
      "metadata": {
        "id": "N-V66pgfBqGD"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "cifar10_trainset = datasets.CIFAR10('../data', train=True, download=True, transform=ts)\n",
        "cifar10_testset = datasets.CIFAR10(root='../data', train=False, download=True, transform=ts)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "XWvIF_EZIXLw",
        "outputId": "fe2f643d-bfb3-4606-f6e9-368636d05922"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ncifar10_trainset = datasets.CIFAR10('../data', train=True, download=True, transform=ts)\\ncifar10_testset = datasets.CIFAR10(root='../data', train=False, download=True, transform=ts)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "boston = datasets.load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "x_train = torch.from_numpy(x_train).to(device = device)\n",
        "x_test = torch.from_numpy(x_test).to(device = device)\n",
        "y_train = torch.from_numpy(y_train).to(device = device)\n",
        "y_test = torch.from_numpy(y_test).to(device = device)\n",
        "\n",
        "x_train = x_train.T.float()\n",
        "x_test = x_test.T.float()\n",
        "\n",
        "N = x_train.shape[1]\n",
        "N_test = x_test.shape[1]\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "zrwxO0BKb5gG",
        "outputId": "c7549f3e-9b63-4500-81e7-3711301b80ff"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom sklearn import datasets\\nfrom sklearn.model_selection import train_test_split\\n\\nboston = datasets.load_boston()\\nX = boston.data\\ny = boston.target\\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\\n\\nx_train = torch.from_numpy(x_train).to(device = device)\\nx_test = torch.from_numpy(x_test).to(device = device)\\ny_train = torch.from_numpy(y_train).to(device = device)\\ny_test = torch.from_numpy(y_test).to(device = device)\\n\\nx_train = x_train.T.float()\\nx_test = x_test.T.float()\\n\\nN = x_train.shape[1]\\nN_test = x_test.shape[1]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PEt-flsyEKM"
      },
      "source": [
        "# Train - test split\n",
        "\n",
        "Code taken from https://github.com/timlautk/BCD-for-DNNs-PyTorch/blob/master/bcd_dnn_mlp_mnist.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_test, y_test, y_train_one_hot, y_test_one_hot= load_dataset(fmnist_trainset, fmnist_testset, 10)\n",
        "\n",
        "# We move to GPU\n",
        "x_train = x_train.to(device = device)\n",
        "x_test = x_test.to(device = device)\n",
        "y_train = y_train.to(device = device)\n",
        "y_test = y_test.to(device = device)\n",
        "y_train_one_hot = y_train_one_hot.to(device = device)\n",
        "y_test_one_hot = y_test_one_hot.to(device = device)"
      ],
      "metadata": {
        "id": "74DaGSPsDS5X"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me_6E9_Hxsxw"
      },
      "source": [
        "# Architecture initialization\n",
        "\n",
        "For the MultiLayerPerceptron we have the parameters **input_size** , **hidden_size**,**output_size** corresponding to the size of the input layer, the hidden layer and the output layer, respectively.\n",
        "\n",
        "The MLP only has 3 layers like https://github.com/timlautk/BCD-for-DNNs-PyTorch/blob/master/bcd_dnn_mlp_mnist.ipynb as a starting point.\n",
        "\n",
        "Also we use ReLU currently for the same reason."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "hlRg4UWiezMt"
      },
      "outputs": [],
      "source": [
        "input_size = x_train.shape[0]\n",
        "hidden_size = 1500\n",
        "output_size = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrFu1jgT3kLz"
      },
      "source": [
        "# Training\n",
        "\n",
        "Note: Fix it so that it moves everything to device in the following function and that it does the label sample split here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "TFQBbrVor_zy"
      },
      "outputs": [],
      "source": [
        "def update_v_js(U1, U2, W, b, rho, gamma):\n",
        "    \"\"\"\n",
        "    The function updates the V_js parameters during the training phase\n",
        "    \n",
        "    :param U1: The U parameter on the same level of V that we are updating\n",
        "    :param U2: The U parameter which is in the next level of the V that we are updating\n",
        "    :param W: The W parameter which is in the next level of the V that we are updating\n",
        "    :param b: The b parameter which is in the next level of the V that we are updating\n",
        "    :param rho: The constant rho parameter which is in the next level of the V that we are updating\n",
        "    :param gamma: The constant gamma parameter which is in the next level of the V that we are updating\n",
        "    :return: The updated V\n",
        "    \"\"\"\n",
        "    _, d = W.size()\n",
        "    I = torch.eye(d, device=device)\n",
        "    U1 = nn.ReLU()(U1)\n",
        "    _, col_U2 = U2.size()\n",
        "    Vstar = torch.mm(torch.inverse(rho * (torch.mm(torch.t(W), W)) + gamma * I),\n",
        "                     rho * torch.mm(torch.t(W), U2 - b.repeat(1, col_U2)) + gamma * U1)\n",
        "    return Vstar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "A7sdqHftr_zz"
      },
      "outputs": [],
      "source": [
        "def update_wb_js(U, V, W, b, alpha, rho):\n",
        "    \"\"\"\n",
        "    The function updates the W and b parameters during the training phase\n",
        "    \n",
        "    :param U: The U in the current level of W and b\n",
        "    :param V: The V in the previous level with respect to the W that we are updating\n",
        "    :param W: The current W that we have to update\n",
        "    :param b: The current b that we have to update\n",
        "    :param alpha: The alpha constant of the updates\n",
        "    :param rho: The rho constant of the updates\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    d, N = V.size()\n",
        "    I = torch.eye(d, device=device)\n",
        "    _, col_U = U.size()\n",
        "    Wstar = torch.mm(alpha * W + rho * torch.mm(U - b.repeat(1, col_U), torch.t(V)),\n",
        "                     torch.inverse(alpha * I + rho * (torch.mm(V, torch.t(V)))))\n",
        "    bstar = (alpha * b + rho * torch.sum(U - torch.mm(W, V), dim=1).reshape(b.size())) / (rho * N + alpha)\n",
        "    return Wstar, bstar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "IX88WYSQr_z0"
      },
      "outputs": [],
      "source": [
        "def relu_prox(a, b, gamma, d, N):\n",
        "    \"\"\"\n",
        "    The function compute the solution to the relu proximal update problem\n",
        "    \n",
        "    :param a: the a in the closed formula of the linearized update\n",
        "    :param b: the b in the closed formula of the linearized update\n",
        "    :param gamma: The constant used in the update\n",
        "    :param d: the dimension of the current layer\n",
        "    :param N: The number of samples\n",
        "    :return: The obtained solution of the prox update\n",
        "    \"\"\"\n",
        "    val = torch.empty(d, N, device=device)\n",
        "    x = (a + gamma * b) / (1 + gamma)\n",
        "    y = torch.min(b, torch.zeros(d, N, device=device))\n",
        "    # torch.zeros(d,N, device=device)\n",
        "    val = torch.where(a + gamma * b < 0, y, torch.zeros(d, N, device=device))\n",
        "    val = torch.where(\n",
        "        ((a + gamma * b >= 0) & (b >= 0)) | ((a * (gamma - np.sqrt(gamma * (gamma + 1))) <= gamma * b) & (b < 0)), x,\n",
        "        val)\n",
        "    val = torch.where((-a <= gamma * b) & (gamma * b <= a * (gamma - np.sqrt(gamma * (gamma + 1)))), b, val)\n",
        "    return val\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(t):\n",
        "  \"\"\"\n",
        "  The function flatten the list removing the sublists in it\n",
        "  :param t: the list to flatten\n",
        "  \"\"\"\n",
        "  return [item for sublist in t for item in sublist]\n",
        "\n",
        "\n",
        "def random_select(min,max,nodes,dims):\n",
        "  \"\"\"\n",
        "  This function generates a set of indexes to chose which nodes of the layer we will update\n",
        "  :param min: the minimum index from which the nodes will be chosen\n",
        "  :param max: the maximum index from which the nodes will be chosen\n",
        "  :param nodes: the number of nodes that should be chosen\n",
        "  :param dims: the second dimension of the layer \n",
        "  :return layers,res: two lists that index together randomly selected elements in the layers\n",
        "  \"\"\"\n",
        "  res = []\n",
        "  layers = []\n",
        "  for i in range(dims):\n",
        "    res.append(random.sample(range(min,max),nodes))\n",
        "    layers.append([i]*nodes)\n",
        "  return flatten(layers),flatten(res)\n",
        "\n",
        "def rand(min,max,nodes):\n",
        "   return random.sample(range(min,max),nodes)"
      ],
      "metadata": {
        "id": "D6AomYZKmhcc"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def index_lists(l1,l2):\n",
        "  ret1 = flatten(list(map(lambda x : np.repeat(x,len(l2)).tolist(),l1)))\n",
        "  ret2 = np.tile(l2,len(l1)).tolist()\n",
        "  return ret1,ret2"
      ],
      "metadata": {
        "id": "UGuBYbDyt0EG"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_pred(Ws,bs,input,N):\n",
        "  \"\"\"\n",
        "  The function is used to make the predictions based on the best found parameters\n",
        "  :param Ws: The weight matrices\n",
        "  :param bs: the bias vectors\n",
        "  :return pred: the predictions\n",
        "  \"\"\"\n",
        "  a1_train = input\n",
        "  for i in range(0,len(Ws)-1):\n",
        "    a1_train = nn.ReLU()(torch.addmm(bs[i].repeat(1, N), Ws[i], a1_train))\n",
        "  pred = torch.argmax(torch.addmm(bs[len(Ws)-1].repeat(1, N), Ws[len(Ws)-1], a1_train), dim=0)\n",
        "  return pred"
      ],
      "metadata": {
        "id": "VkC8qOOkwZCD"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The function requires at least 1 hidden layer otherwise it need some rewrriting\n",
        "def execute_training(layers, input_size, hidden_size, output_size, train_set, val_set,train_labels, val_labels, niter = 100,\n",
        "                     gamma = 1, alpha = 5):\n",
        "  \"\"\"\n",
        "  The function takes the following arguements and produces a list of weights and biases with which \n",
        "  you can use the make_pred function to get a list of predictions\n",
        "  :param layers: The total number of layers of the network\n",
        "  :param input_size: The total size of the input layer\n",
        "  :param hidden_size: The size of the hidden layer\n",
        "  :param output_size: The size of the output layer (usefull for multiclass classification)\n",
        "  :param train_set: The training set\n",
        "  :param val_set: The validation set\n",
        "  :param train_labels: The training labels\n",
        "  :param test labels: The validation labels\n",
        "  :param niter: The default number of epochs to train the network\n",
        "  :param gamma: The gamma parameter of the algorithm\n",
        "  :param alpha: The alpha parameter of the algorithm\n",
        "  :return Ws,bs: Returns two lists that go in order from the input to the output layer of the weights and the biases of each layer\n",
        "  \"\"\"\n",
        "\n",
        "  N = len(train_labels)\n",
        "  N_test = len(val_labels)\n",
        "\n",
        "  # weight initialization (we replicate pytorch weight initialization)\n",
        "\n",
        "  std = math.sqrt(1/input_size)\n",
        "  W = torch.FloatTensor(hidden_size, input_size).uniform_(-std, std)\n",
        "  b = torch.FloatTensor(hidden_size, 1).uniform_(-std, std)\n",
        "\n",
        "  b = b.to(device = device)\n",
        "  W = W.to(device = device)\n",
        "\n",
        "  U = torch.addmm(b.repeat(1, N), W, x_train) # equivalent to W1@x_train+b1.repeat(1,N)\n",
        "  V = nn.ReLU()(U)\n",
        "\n",
        "  Ws = [W]\n",
        "  bs = [b]\n",
        "  Us = [U]\n",
        "  Vs = [V]\n",
        "\n",
        "  for i in range(1,layers-1):\n",
        "    std = math.sqrt(1/hidden_size)\n",
        "    W = torch.FloatTensor(hidden_size, hidden_size).uniform_(-std, std)\n",
        "    b = torch.FloatTensor(hidden_size, 1).uniform_(-std, std)\n",
        "    b = b.to(device = device)\n",
        "    W = W.to(device = device)\n",
        "    U = torch.addmm(b.repeat(1, N), W, Vs[-1])\n",
        "    V = nn.ReLU()(U)\n",
        "    Ws.append(W)\n",
        "    bs.append(b)\n",
        "    Us.append(U)\n",
        "    Vs.append(V)\n",
        "  \n",
        "  std = math.sqrt(1/hidden_size)\n",
        "  W = torch.FloatTensor(output_size, hidden_size).uniform_(-std, std)\n",
        "  b = torch.FloatTensor(output_size, 1).uniform_(-std, std)\n",
        "\n",
        "  # we move them to GPU\n",
        "  b = b.to(device = device)\n",
        "  W = W.to(device = device)\n",
        "  U = torch.addmm(b.repeat(1, N), W, Vs[-1])\n",
        "  V = U\n",
        "  Ws.append(W)\n",
        "  bs.append(b)\n",
        "  Us.append(U)\n",
        "  Vs.append(V)\n",
        "  \n",
        "  # constant initialization\n",
        "\n",
        "  gamma1 = gamma2 = gamma3 = gamma4 = gamma\n",
        "\n",
        "  rho = gamma\n",
        "  rho1 = rho2 = rho3 = rho4 = rho\n",
        "\n",
        "  alpha1 = alpha2 = alpha3 = alpha4 = alpha5 = alpha6 = alpha7 \\\n",
        "  = alpha8 = alpha9 = alpha10 = alpha\n",
        "\n",
        "  # vector of performance initialization\n",
        "\n",
        "  loss1 = np.empty(niter)\n",
        "  loss2 = np.empty(niter)\n",
        "  accuracy_train = np.empty(niter)\n",
        "  accuracy_test = np.empty(niter)\n",
        "  time1 = np.empty(niter)\n",
        "\n",
        "  print('Train on', N, 'samples, validate on', N_test, 'samples')\n",
        "  for k in range(niter):\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # update V3\n",
        "    Vs[-1] = (y_train_one_hot + gamma3*Us[-1] + alpha1*Vs[-1])/(1+ gamma3 + alpha1)\n",
        "    #Vs[-1] = (train_labels + gamma3*Us[-1] + alpha1*Vs[-1])/(1+ gamma3 + alpha1)\n",
        "\n",
        "    # update U3 \n",
        "    Us[-1] = (gamma3*Vs[-1] + rho3*(torch.mm(Ws[-1],Vs[-2]) + bs[-1].repeat(1,N)))/(gamma3 + rho3)\n",
        "\n",
        "    # update W3 and b3\n",
        "    W, b = update_wb_js(Us[-1],Vs[-2],Ws[-1],bs[-1],alpha1, rho3)\n",
        "    Ws[-1] = W\n",
        "    bs[-1] = b\n",
        "\n",
        "    for i in range(len(Vs)-2,0,-1):\n",
        "      Vs[i] = update_v_js(Us[i],Us[i+1],Ws[i+1],bs[i+1],rho3,gamma2)\n",
        "      Us[i] = relu_prox(Vs[i],(rho2*torch.addmm(bs[i].repeat(1,N), Ws[i], Vs[i-1]) +\n",
        "                               alpha2*Us[i])/(rho2 + alpha2),(rho2 + alpha2)/gamma2, hidden_size, N)\n",
        "      W,b = update_wb_js(Us[i],Vs[i-1],Ws[i],bs[i],alpha3,rho2)\n",
        "      Ws[i] = W\n",
        "      bs[i]= b\n",
        "    \n",
        "    # update V1\n",
        "    Vs[0] = update_v_js(Us[0],Us[1],Ws[1],bs[1],rho2,gamma1)\n",
        "        \n",
        "    # update U1\n",
        "    Us[0] = relu_prox(Vs[0],(rho1*torch.addmm(bs[0].repeat(1,N), Ws[0], x_train) +\n",
        "                             alpha7*Us[0])/(rho1 + alpha7),(rho1 + alpha7)/gamma1, hidden_size, N)\n",
        "    \n",
        "    # update W1 and b1\n",
        "    W, b = update_wb_js(Us[0],x_train,Ws[0],bs[0],alpha8,rho1)\n",
        "    Ws[0] = W\n",
        "    bs[0] = b\n",
        "\n",
        "    #a1_train = nn.ReLU()(torch.addmm(b1.repeat(1, N), W1, x_train))\n",
        "    #a1_train = x_train\n",
        "    #for i in range(len(Vs)-1,0,-1):\n",
        "    #  a1_train = nn.ReLU()(torch.addmm(bs[i].repeat(1, N), Ws[i], a1_train))\n",
        "    #pred = torch.argmax(torch.addmm(bs[0].repeat(1, N), Ws[0], a1_train), dim=0)\n",
        "    pred = make_pred(Ws,bs,x_train,N)\n",
        "\n",
        "    #a1_test = x_test\n",
        "    #a1_test = nn.ReLU()(torch.addmm(b1.repeat(1, N_test), W1, x_test))\n",
        "    #for i in range(len(Vs)-1,0,-1):\n",
        "    #  a1_test = nn.ReLU()(torch.addmm(bs[i].repeat(1, N_test), Ws[i], a1_test))\n",
        "    #pred_test = torch.argmax(torch.addmm(bs[0].repeat(1, N_test), Ws[0], a1_test), dim=0)\n",
        "    pred_test = make_pred(Ws,bs,x_test,N_test) \n",
        "\n",
        "    loss1[k] = gamma/2*torch.pow(torch.dist(Vs[-1],y_train_one_hot,2),2).cpu().numpy()\n",
        "    loss2[k] = loss1[k] + gamma/2 * torch.pow(torch.dist(torch.addmm(bs[0].repeat(1,N), Ws[0], x_train),Us[0],2),2).cpu().numpy()\n",
        "\n",
        "    for i in range(1,layers):\n",
        "      loss2[k] = loss2[k] + gamma/2 * torch.pow(torch.dist(torch.addmm(bs[i].repeat(1,N), Ws[i], Vs[i-1]),Us[i],2),2).cpu().numpy()\n",
        "\n",
        "    #loss2[k] = loss1[k] + rho1/2*torch.pow(torch.dist(torch.addmm(b1.repeat(1,N), W1, x_train),U1,2),2).cpu().numpy() \\\n",
        "    #+rho2/2*torch.pow(torch.dist(torch.addmm(b2.repeat(1,N), W2, V1),U2,2),2).cpu().numpy() \\\n",
        "    #+rho3/2*torch.pow(torch.dist(torch.addmm(b3.repeat(1,N), W3, V2),U3,2),2).cpu().numpy()\n",
        "        \n",
        "    # compute training accuracy\n",
        "    correct_train = pred == train_labels\n",
        "    accuracy_train[k] = np.mean(correct_train.cpu().numpy())\n",
        "        \n",
        "    # compute validation accuracy\n",
        "    correct_test = pred_test == val_labels\n",
        "    accuracy_test[k] = np.mean(correct_test.cpu().numpy())\n",
        "        \n",
        "    # compute training time\n",
        "    stop = time.time()\n",
        "    duration = stop - start\n",
        "    time1[k] = duration\n",
        "        \n",
        "    # print results\n",
        "    print('Epoch', k + 1, '/', niter, '\\n', \n",
        "          '-', 'time:', time1[k], '-', 'sq_loss:', loss1[k], '-', 'tot_loss:', loss2[k], \n",
        "          '-', 'acc:', accuracy_train[k], '-', 'val_acc:', accuracy_test[k])\n",
        "  print('The total time spent is:', np.sum(time1), 's')\n",
        "  return Ws, bs"
      ],
      "metadata": {
        "id": "EKrkhXmv0ckR"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ , _ = execute_training(3, input_size, hidden_size, output_size, x_train, x_test, y_train, y_test, niter = 25, gamma = 1, alpha = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "H_I6sn-bA_jS",
        "outputId": "932f44df-5937-4de6-f436-d47bc3c65464"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1 / 25 \n",
            " - time: 2.5452237129211426 - sq_loss: 22844.775390625 - tot_loss: 22878.96977776289 - acc: 0.4882666666666667 - val_acc: 0.4781\n",
            "Epoch 2 / 25 \n",
            " - time: 2.6436941623687744 - sq_loss: 17253.373046875 - tot_loss: 17285.43397408724 - acc: 0.76845 - val_acc: 0.7515\n",
            "Epoch 3 / 25 \n",
            " - time: 2.6263837814331055 - sq_loss: 13219.8076171875 - tot_loss: 13240.917934536934 - acc: 0.8368166666666667 - val_acc: 0.8177\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-ad995b94d232>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecute_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mniter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-55-f14b96bb3ef6>\u001b[0m in \u001b[0;36mexecute_training\u001b[0;34m(layers, input_size, hidden_size, output_size, train_set, val_set, train_labels, val_labels, niter, gamma, alpha)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m# update W1 and b1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_wb_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mWs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrho1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0mWs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mbs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-c5079f584a6b>\u001b[0m in \u001b[0;36mupdate_wb_js\u001b[0;34m(U, V, W, b, alpha, rho)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_U\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     Wstar = torch.mm(alpha * W + rho * torch.mm(U - b.repeat(1, col_U), torch.t(V)),\n\u001b[0;32m---> 17\u001b[0;31m                      torch.inverse(alpha * I + rho * (torch.mm(V, torch.t(V)))))\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mbstar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrho\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mWstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbstar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropout"
      ],
      "metadata": {
        "id": "j0CP_P12vk7G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HQgkqHxr_zv"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(32)\n",
        "d0 = input_size\n",
        "d1 = d2 = hidden_size\n",
        "d3 = output_size \n",
        "N = len(y_train)\n",
        "N_test = len(y_test)\n",
        "# The layers are: input + 2 hidden + output\n",
        "\n",
        "# we represent the weigths of each layer as matrices with d_{i-1} columns and d_{i} rows\n",
        "\n",
        "# Weight initialization (we replicate pytorch initialization)\n",
        "std_1 = math.sqrt(1/d0)\n",
        "W1 = torch.FloatTensor(d1, d0).uniform_(-std_1, std_1)\n",
        "b1 = torch.FloatTensor(d1, 1).uniform_(-std_1, std_1)\n",
        "\n",
        "# we move them to GPU\n",
        "b1 = b1.to('cuda:0')\n",
        "W1 = W1.to('cuda:0')\n",
        "\n",
        "\n",
        "std_2 = math.sqrt(1/d1)\n",
        "W2 = torch.FloatTensor(d2, d1).uniform_(-std_2, std_2)\n",
        "b2 = torch.FloatTensor(d2, 1).uniform_(-std_2, std_2)\n",
        "\n",
        "# we move them to GPU\n",
        "b2 = b2.to('cuda:0')\n",
        "W2 = W2.to('cuda:0')\n",
        "\n",
        "std_3 = math.sqrt(1/d2)\n",
        "W3 = torch.FloatTensor(d3, d2).uniform_(-std_3, std_3)\n",
        "b3 = torch.FloatTensor(d3, 1).uniform_(-std_3, std_3)\n",
        "\n",
        "# we move them to GPU\n",
        "b3 = b3.to('cuda:0')\n",
        "W3 = W3.to('cuda:0')\n",
        "\n",
        "\n",
        "U1 = torch.addmm(b1.repeat(1, N), W1, x_train) # equivalent to W1@x_train+b1.repeat(1,N)\n",
        "V1 = nn.ReLU()(U1)\n",
        "U2 = torch.addmm(b2.repeat(1, N), W2, V1)\n",
        "V2 = nn.ReLU()(U2)\n",
        "U3 = torch.addmm(b3.repeat(1, N), W3, V2)\n",
        "V3 = U3\n",
        "\n",
        "# constant initializations\n",
        "gamma = 1\n",
        "gamma1 = gamma2 = gamma3 = gamma4 = gamma\n",
        "\n",
        "rho = gamma\n",
        "rho1 = rho2 = rho3 = rho4 = rho\n",
        "\n",
        "\n",
        "alpha = 5\n",
        "alpha1 = alpha2 = alpha3 = alpha4 = alpha5 = alpha6 = alpha7 \\\n",
        "= alpha8 = alpha9 = alpha10 = alpha\n",
        "\n",
        "# initialization of the vectors of losses and accuracies\n",
        "niter = 500\n",
        "loss1 = np.empty(niter)\n",
        "loss2 = np.empty(niter)\n",
        "accuracy_train = np.empty(niter)\n",
        "accuracy_test = np.empty(niter)\n",
        "time1 = np.empty(niter)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we perform one-hot encoding on the y-> y_train_one_hot will be a tensor of dimension 10*training_samples, for each sample there\n",
        "# will be a one in the row corresponding to the class.\n",
        "\n",
        "# Iterations\n",
        "print('Train on', N, 'samples, validate on', N_test, 'samples')\n",
        "for k in range(niter):\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  #select_the nodes for the third layer\n",
        "  nodes_up3 = rand(0,10,5)\n",
        "  nodes_up2 = rand(0,1500,750)\n",
        "  nodes_up1 = rand(0,1500,750)\n",
        "  V3_sample = V3[nodes_up3]\n",
        "  U3_sample = U3[nodes_up3]\n",
        "  W3_sample = torch.transpose(torch.transpose(W3[nodes_up3],0,1)[nodes_up2],0,1)\n",
        "  b3_sample = b3[nodes_up3]\n",
        "  V2_sample = V2[nodes_up2]\n",
        "  U2_sample = U2[nodes_up2]\n",
        "  W2_sample = torch.transpose(torch.transpose(W2[nodes_up2],0,1)[nodes_up1],0,1)\n",
        "  b2_sample = b2[nodes_up2]\n",
        "  V1_sample = V1[nodes_up1]\n",
        "  U1_sample = U1[nodes_up1]\n",
        "  W1_sample = W1[nodes_up1]\n",
        "  b1_sample = b1[nodes_up1]\n",
        "  # update V3\n",
        "  V3_sample = (y_train_one_hot[nodes_up3] + gamma3*U3_sample + alpha1*V3_sample)/(1+ gamma3 + alpha1)\n",
        "      \n",
        "  # update U3 \n",
        "  U3_sample = (gamma3*V3_sample + rho3*(torch.mm(W3_sample,V2_sample) + b3_sample.repeat(1,N)))/(gamma3 + rho3)\n",
        "\n",
        "  # update W3 and b3\n",
        "  W3_sample, b3_sample = updateWb_js(U3_sample,V2_sample,W3_sample,b3_sample,alpha1,rho3)\n",
        "  \n",
        "  # update V2\n",
        "  V2_sample = updateV_js(U2_sample,U3_sample,W3_sample,b3_sample,rho3,gamma2)\n",
        "      \n",
        "  # update U2\n",
        "  U2_sample = relu_prox(V2_sample,(rho2*torch.addmm(b2_sample.repeat(1,N), W2_sample, V1_sample) + alpha2*U2_sample)/(rho2 + alpha2),(rho2 + alpha2)/gamma2,750,N)\n",
        "      \n",
        "  # update W2 and b2\n",
        "  W2_sample, b2_sample = updateWb_js(U2_sample,V1_sample,W2_sample,b2_sample,alpha3,rho2)\n",
        "  \n",
        "  # update V1\n",
        "  V1_sample = updateV_js(U1_sample,U2_sample,W2_sample,b2_sample,rho2,gamma1)\n",
        "      \n",
        "  # update U1\n",
        "  U1_sample = relu_prox(V1_sample,(rho1*torch.addmm(b1_sample.repeat(1,N), W1_sample, x_train) + alpha7*U1_sample)/(rho1 + alpha7),(rho1 + alpha7)/gamma1,750,N)\n",
        "  \n",
        "  # update W1 and b1\n",
        "  W1_sample, b1_sample = updateWb_js(U1_sample,x_train,W1_sample,b1_sample,alpha8,rho1)\n",
        "\n",
        "  print(list(zip(nodes_up3,nodes_up2)))\n",
        "  #print(W3[].shape)\n",
        "  V3[nodes_up3] = V3_sample\n",
        "  U3[nodes_up3] = U3_sample\n",
        "  W3[index_lists(nodes_up3,nodes_up2)] = torch.reshape(W3_sample,(750*5,))\n",
        "  b3[nodes_up3] = b3_sample\n",
        "  V2[nodes_up2] = V2_sample\n",
        "  U2[nodes_up2] = U2_sample\n",
        "  W2[index_lists(nodes_up2,nodes_up1)] = torch.reshape(W2_sample,(750*750,))\n",
        "  b2[nodes_up2] = b2_sample\n",
        "  V1[nodes_up1] = V1_sample\n",
        "  U1[nodes_up1] = V1_sample\n",
        "  W1[nodes_up1] = W1_sample\n",
        "  b1[nodes_up1] = b1_sample\n",
        "\n",
        "  a1_train = nn.ReLU()(torch.addmm(b1_sample.repeat(1, N), W1_sample, x_train))\n",
        "  a2_train = nn.ReLU()(torch.addmm(b2_sample.repeat(1, N), W2_sample, a1_train))\n",
        "  pred = torch.argmax(torch.addmm(b3_sample.repeat(1, N), W3_sample, a2_train), dim=0)\n",
        "\n",
        "  a1_test = nn.ReLU()(torch.addmm(b1_sample.repeat(1, N_test), W1_sample, x_test))\n",
        "  a2_test = nn.ReLU()(torch.addmm(b2_sample.repeat(1, N_test), W2_sample, a1_test))\n",
        "  pred_test = torch.argmax(torch.addmm(b3_sample.repeat(1, N_test), W3_sample, a2_test), dim=0)\n",
        "      \n",
        "  loss1[k] = gamma3/2*torch.pow(torch.dist(V3,y_train_one_hot,2),2).cpu().numpy()\n",
        "  loss2[k] = loss1[k] + rho1/2*torch.pow(torch.dist(torch.addmm(b1.repeat(1,N), W1, x_train),U1,2),2).cpu().numpy() \\\n",
        "  +rho2/2*torch.pow(torch.dist(torch.addmm(b2.repeat(1,N), W2, V1),U2,2),2).cpu().numpy() \\\n",
        "  +rho3/2*torch.pow(torch.dist(torch.addmm(b3.repeat(1,N), W3, V2),U3,2),2).cpu().numpy()\n",
        "      \n",
        "  # compute training accuracy\n",
        "  correct_train = pred == y_train\n",
        "  accuracy_train[k] = np.mean(correct_train.cpu().numpy())\n",
        "      \n",
        "  # compute validation accuracy\n",
        "  correct_test = pred_test == y_test\n",
        "  accuracy_test[k] = np.mean(correct_test.cpu().numpy())\n",
        "      \n",
        "  # compute training time\n",
        "  stop = time.time()\n",
        "  duration = stop - start\n",
        "  time1[k] = duration\n",
        "      \n",
        "  # print results\n",
        "  print('Epoch', k + 1, '/', niter, '\\n', \n",
        "        '-', 'time:', time1[k], '-', 'sq_loss:', loss1[k], '-', 'tot_loss:', loss2[k], \n",
        "        '-', 'acc:', accuracy_train[k], '-', 'val_acc:', accuracy_test[k])"
      ],
      "metadata": {
        "id": "MM7vUi3xpSai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## We plot the train losses\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('train losses')\n",
        "plt.plot(np.arange(0,loss1.shape[0]), loss1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eFw-MbOat5p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## We plot the test accuracy\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('test accuracy')\n",
        "plt.plot(np.arange(0, accuracy_test.shape[0]), accuracy_test)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2gF9-R3-uB3c"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Drop_and_parametric_run.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}