{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KDbRlCKv4U8",
        "outputId": "624ad4fb-481e-4f48-fac3-b853ab0ed031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 1.11.0+cu113\n",
            "Torchvision Version: 0.12.0+cu113\n",
            "GPU is available? True\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms, utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "print(\"Torchvision Version:\", torchvision.__version__)\n",
        "print(\"GPU is available?\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nYF1cnvPwthx"
      },
      "outputs": [],
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqvTMJAbw74Q"
      },
      "source": [
        "# Imported datasets\n",
        "For the testing and comparison of our algorithms we will use the following datasets:\n",
        "\n",
        "1. MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "icE91aqaw110"
      },
      "outputs": [],
      "source": [
        "ts = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0,), (1,))])\n",
        "mnist_trainset = datasets.MNIST('../data', train=True, download=True, transform=ts)\n",
        "mnist_testset = datasets.MNIST(root='../data', train=False, download=True, transform=ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PEt-flsyEKM"
      },
      "source": [
        "# Train - test split\n",
        "\n",
        "Code taken from https://github.com/timlautk/BCD-for-DNNs-PyTorch/blob/master/bcd_dnn_mlp_mnist.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hVoD2jFeg-69"
      },
      "outputs": [],
      "source": [
        "#train-set initialization\n",
        "x_d0 = mnist_trainset[0][0].size()[0]\n",
        "x_d1 = mnist_trainset[0][0].size()[1]\n",
        "x_d2 = mnist_trainset[0][0].size()[2]\n",
        "N = x_d3 = len(mnist_trainset)\n",
        "K = 10\n",
        "x_train = torch.empty((N,x_d0*x_d1*x_d2), device=device)\n",
        "y_train = torch.empty(N, dtype=torch.long)\n",
        "for i in range(N): \n",
        "    x_train[i,:] = torch.reshape(mnist_trainset[i][0], (1, x_d0*x_d1*x_d2))\n",
        "    y_train[i] = mnist_trainset[i][1]\n",
        "x_train = torch.t(x_train)\n",
        "#y_one_hot = torch.zeros(N, K).scatter_(1, torch.reshape(y_train, (N, 1)), 1)\n",
        "#y_one_hot = torch.t(y_one_hot).to(device=device)\n",
        "y_train = y_train.to(device=device)\n",
        "\n",
        "#test-set initialization\n",
        "N_test = x_d3_test = len(mnist_testset)\n",
        "x_test = torch.empty((N_test,x_d0*x_d1*x_d2), device=device)\n",
        "y_test = torch.empty(N_test, dtype=torch.long)\n",
        "for i in range(N_test): \n",
        "    x_test[i,:] = torch.reshape(mnist_testset[i][0], (1, x_d0*x_d1*x_d2))\n",
        "    y_test[i] = mnist_testset[i][1]\n",
        "x_test = torch.t(x_test)\n",
        "#y_test_one_hot = torch.zeros(N_test, K).scatter_(1, torch.reshape(y_test, (N_test, 1)), 1)\n",
        "#y_test_one_hot = torch.t(y_test_one_hot).to(device=device)\n",
        "y_test = y_test.to(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me_6E9_Hxsxw"
      },
      "source": [
        "# Architecture initialization\n",
        "\n",
        "For the MultiLayerPerceptron we have the parameters **input_size** , **hidden_size**,**output_size** corresponding to the size of the input layer, the hidden layer and the output layer, respectively.\n",
        "\n",
        "The MLP only has 3 layers like https://github.com/timlautk/BCD-for-DNNs-PyTorch/blob/master/bcd_dnn_mlp_mnist.ipynb as a starting point.\n",
        "\n",
        "Also we use ReLU currently for the same reason."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hlRg4UWiezMt"
      },
      "outputs": [],
      "source": [
        "input_size = 28*28\n",
        "hidden_size = 1500\n",
        "output_size = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0HQgkqHxr_zv"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(32)\n",
        "d0 = input_size\n",
        "d1 = d2 = 1500\n",
        "d3 = output_size \n",
        "\n",
        "# The layers are: input + 2 hidden + output\n",
        "\n",
        "# we represent the weigths of each layer as matrices with d_{i-1} columns and d_{i} rows\n",
        "\n",
        "# Weight initialization (we replicate pytorch initialization)\n",
        "std_1 = math.sqrt(1/d0)\n",
        "W1 = torch.FloatTensor(d1, d0).uniform_(-std_1, std_1)\n",
        "b1 = torch.FloatTensor(d1, 1).uniform_(-std_1, std_1)\n",
        "\n",
        "# we move them to GPU\n",
        "b1 = b1.to('cuda:0')\n",
        "W1 = W1.to('cuda:0')\n",
        "\n",
        "\n",
        "std_2 = math.sqrt(1/d1)\n",
        "W2 = torch.FloatTensor(d2, d1).uniform_(-std_2, std_2)\n",
        "b2 = torch.FloatTensor(d2, 1).uniform_(-std_2, std_2)\n",
        "\n",
        "# we move them to GPU\n",
        "b2 = b2.to('cuda:0')\n",
        "W2 = W2.to('cuda:0')\n",
        "\n",
        "std_3 = math.sqrt(1/d2)\n",
        "W3 = torch.FloatTensor(d3, d2).uniform_(-std_3, std_3)\n",
        "b3 = torch.FloatTensor(d3, 1).uniform_(-std_3, std_3)\n",
        "\n",
        "# we move them to GPU\n",
        "b3 = b3.to('cuda:0')\n",
        "W3 = W3.to('cuda:0')\n",
        "\n",
        "\n",
        "U1 = torch.addmm(b1.repeat(1, N), W1, x_train) # equivalent to W1@x_train+b1.repeat(1,N)\n",
        "V1 = nn.ReLU()(U1)\n",
        "U2 = torch.addmm(b2.repeat(1, N), W2, V1)\n",
        "V2 = nn.ReLU()(U2)\n",
        "U3 = torch.addmm(b3.repeat(1, N), W3, V2)\n",
        "V3 = U3\n",
        "\n",
        "# constant initializations\n",
        "gamma = 1\n",
        "gamma1 = gamma2 = gamma3 = gamma4 = gamma\n",
        "\n",
        "rho = gamma\n",
        "rho1 = rho2 = rho3 = rho4 = rho\n",
        "\n",
        "\n",
        "alpha = 5\n",
        "alpha1 = alpha2 = alpha3 = alpha4 = alpha5 = alpha6 = alpha7 \\\n",
        "= alpha8 = alpha9 = alpha10 = alpha\n",
        "\n",
        "# initialization of the vectors of losses and accuracies\n",
        "niter = 500\n",
        "loss1 = np.empty(niter)\n",
        "loss2 = np.empty(niter)\n",
        "accuracy_train = np.empty(niter)\n",
        "accuracy_test = np.empty(niter)\n",
        "time1 = np.empty(niter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TFQBbrVor_zy"
      },
      "outputs": [],
      "source": [
        "def updateV_js(U1,U2,W,b,rho,gamma): \n",
        "    _, d = W.size()\n",
        "    I = torch.eye(d, device=device)\n",
        "    U1 = nn.ReLU()(U1)\n",
        "    _, col_U2 = U2.size()\n",
        "    Vstar = torch.mm(torch.inverse(rho*(torch.mm(torch.t(W),W))+gamma*I), rho*torch.mm(torch.t(W),U2-b.repeat(1,col_U2))+gamma*U1)\n",
        "    return Vstar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "A7sdqHftr_zz"
      },
      "outputs": [],
      "source": [
        "def updateWb_js(U, V, W, b, alpha, rho): \n",
        "    d,N = V.size()\n",
        "    I = torch.eye(d, device=device)\n",
        "    _, col_U = U.size()\n",
        "    Wstar = torch.mm(alpha*W+rho*torch.mm(U-b.repeat(1,col_U),torch.t(V)),torch.inverse(alpha*I+rho*(torch.mm(V,torch.t(V)))))\n",
        "    bstar = (alpha*b+rho*torch.sum(U-torch.mm(W,V), dim=1).reshape(b.size()))/(rho*N+alpha)\n",
        "    return Wstar, bstar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IX88WYSQr_z0"
      },
      "outputs": [],
      "source": [
        "def relu_prox(a, b, gamma, d, N):\n",
        "    val = torch.empty(d,N, device=device)\n",
        "    x = (a+gamma*b)/(1+gamma)\n",
        "    y = torch.min(b,torch.zeros(d,N, device=device))\n",
        "    # torch.zeros(d,N, device=device)\n",
        "    val = torch.where(a+gamma*b < 0, y, torch.zeros(d,N, device=device))\n",
        "    val = torch.where(((a+gamma*b >= 0) & (b >=0)) | ((a*(gamma-np.sqrt(gamma*(gamma+1))) <= gamma*b) & (b < 0)), x, val)\n",
        "    val = torch.where((-a <= gamma*b) & (gamma*b <= a*(gamma-np.sqrt(gamma*(gamma+1)))), b, val)\n",
        "    return val"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]\n",
        "\n",
        "def random_select(min,max,nodes,dims):\n",
        "  \"\"\"\n",
        "  This function generates a set of indexes to chose which nodes of the layer we will update\n",
        "  :param min: the minimum index from which the nodes will be chosen\n",
        "  :param max: the maximum index from which the nodes will be chosen\n",
        "  :param nodes: the number of nodes that should be chosen\n",
        "  :param dims: the second dimension of the layer \n",
        "  :return layers,res: two lists that index together randomly selected elements in the layers\n",
        "  \"\"\"\n",
        "  res = []\n",
        "  layers = []\n",
        "  for i in range(dims):\n",
        "    res.append(random.sample(range(min,max),nodes))\n",
        "    layers.append([i]*nodes)\n",
        "  return flatten(layers),flatten(res)\n",
        "\n",
        "def rand(min,max,nodes):\n",
        "   return random.sample(range(min,max),nodes)"
      ],
      "metadata": {
        "id": "D6AomYZKmhcc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = 1000\n",
        "layers = 10"
      ],
      "metadata": {
        "id": "MGiDJi6XO_-I"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10,1,-1):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rO6sMPkFSPw",
        "outputId": "22af94e0-41e9-4e6f-b304-f709dddea671"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def index_lists(l1,l2):\n",
        "  ret1 = flatten(list(map(lambda x : np.repeat(x,len(l2)).tolist(),l1)))\n",
        "  ret2 = np.tile(l2,len(l1)).tolist()\n",
        "  return ret1,ret2"
      ],
      "metadata": {
        "id": "UGuBYbDyt0EG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we perform one-hot encoding on the y-> y_one_hot will be a tensor of dimension 10*training_samples, for each sample there\n",
        "# will be a one in the row corresponding to the class.\n",
        "\n",
        "y_one_hot = torch.zeros(N, K).to(device = device).scatter_(1, torch.reshape(y_train,(N,1)), 1)\n",
        "y_one_hot = torch.t(y_one_hot).to(device=device)\n",
        "\n",
        "y_test_one_hot = torch.zeros(N_test, K).to(device = device).scatter_(1, torch.reshape(y_test,(N_test,1)), 1)\n",
        "y_test_one_hot = torch.t(y_test_one_hot).to(device=device)\n",
        "\n",
        "Vs = [V3,V2,V1]\n",
        "Us = [U3,U2,U1]\n",
        "Ws = [W3,W2,W1]\n",
        "bs = [b3,b2,b1]\n",
        "# Iterations\n",
        "print('Train on', N, 'samples, validate on', N_test, 'samples')\n",
        "for k in range(niter):\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  # update V3\n",
        "  Vs[0] = (y_one_hot + gamma3*Us[0] + alpha1*Vs[0])/(1+ gamma3 + alpha1)\n",
        "  \n",
        "  # update U3 \n",
        "  Us[0] = (gamma3*Vs[0] + rho3*(torch.mm(Ws[0],Vs[1]) + bs[0].repeat(1,N)))/(gamma3 + rho3)\n",
        "\n",
        "  # update W3 and b3\n",
        "  W, b = updateWb_js(Us[0],Vs[1],Ws[0],bs[0],alpha1,rho3)\n",
        "  Ws[0] = W\n",
        "  bs[0] = b\n",
        "\n",
        "  for i in range(1,len(Vs)-1):\n",
        "    Vs[i] = updateV_js(Us[i],Us[i-1],Ws[i-1],bs[i-1],rho3,gamma2)\n",
        "    Us[i] = relu_prox(Vs[i],(rho2*torch.addmm(bs[i].repeat(1,N), Ws[i], Vs[i+1]) + alpha2*Us[i])/(rho2 + alpha2),(rho2 + alpha2)/gamma2,1500,N)\n",
        "    W,b = updateWb_js(Us[i],Vs[i+1],Ws[i],bs[i],alpha3,rho2)\n",
        "    Ws[i] = W\n",
        "    bs[i]= b\n",
        "  \n",
        "  # update V1\n",
        "  Vs[len(Vs)-1] = updateV_js(Us[len(Vs)-1],Us[len(Vs)-2],Ws[len(Vs)-2],bs[len(Vs)-2],rho2,gamma1)\n",
        "      \n",
        "  # update U1\n",
        "  Us[len(Us)-1] = relu_prox(Vs[len(Vs)-1],(rho1*torch.addmm(bs[len(Vs)-1].repeat(1,N), Ws[len(Vs)-1], x_train) + alpha7*Us[len(Us)-1])/(rho1 + alpha7),(rho1 + alpha7)/gamma1,1500,N)\n",
        "  \n",
        "  # update W1 and b1\n",
        "  W, b = updateWb_js(Us[len(Us)-1],x_train,Ws[len(Vs)-1],bs[len(Vs)-1],alpha8,rho1)\n",
        "\n",
        "  #a1_train = nn.ReLU()(torch.addmm(b1.repeat(1, N), W1, x_train))\n",
        "  a1_train = x_train\n",
        "  for i in range(len(Vs)-1,0,-1):\n",
        "    a1_train = nn.ReLU()(torch.addmm(bs[i].repeat(1, N), Ws[i], a1_train))\n",
        "  pred = torch.argmax(torch.addmm(bs[0].repeat(1, N), Ws[0], a1_train), dim=0)\n",
        "\n",
        "  a1_test = x_test\n",
        "  #a1_test = nn.ReLU()(torch.addmm(b1.repeat(1, N_test), W1, x_test))\n",
        "  for i in range(len(Vs)-1,0,-1):\n",
        "    a1_test = nn.ReLU()(torch.addmm(bs[i].repeat(1, N_test), Ws[i], a1_test))\n",
        "  pred_test = torch.argmax(torch.addmm(bs[0].repeat(1, N_test), Ws[0], a1_test), dim=0)\n",
        "      \n",
        "  loss1[k] = gamma3/2*torch.pow(torch.dist(V3,y_one_hot,2),2).cpu().numpy()\n",
        "  loss2[k] = loss1[k] + rho1/2*torch.pow(torch.dist(torch.addmm(b1.repeat(1,N), W1, x_train),U1,2),2).cpu().numpy() \\\n",
        "  +rho2/2*torch.pow(torch.dist(torch.addmm(b2.repeat(1,N), W2, V1),U2,2),2).cpu().numpy() \\\n",
        "  +rho3/2*torch.pow(torch.dist(torch.addmm(b3.repeat(1,N), W3, V2),U3,2),2).cpu().numpy()\n",
        "      \n",
        "  # compute training accuracy\n",
        "  correct_train = pred == y_train\n",
        "  accuracy_train[k] = np.mean(correct_train.cpu().numpy())\n",
        "      \n",
        "  # compute validation accuracy\n",
        "  correct_test = pred_test == y_test\n",
        "  accuracy_test[k] = np.mean(correct_test.cpu().numpy())\n",
        "      \n",
        "  # compute training time\n",
        "  stop = time.time()\n",
        "  duration = stop - start\n",
        "  time1[k] = duration\n",
        "      \n",
        "  # print results\n",
        "  print('Epoch', k + 1, '/', niter, '\\n', \n",
        "        '-', 'time:', time1[k], '-', 'sq_loss:', loss1[k], '-', 'tot_loss:', loss2[k], \n",
        "        '-', 'acc:', accuracy_train[k], '-', 'val_acc:', accuracy_test[k])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ljhcSwQF6EBF",
        "outputId": "7f02c8ef-dc42-41f4-89a6-aba159ad71d4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1 / 500 \n",
            " - time: 1.2501146793365479 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.5315 - val_acc: 0.5444\n",
            "Epoch 2 / 500 \n",
            " - time: 1.1871728897094727 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.89885 - val_acc: 0.897\n",
            "Epoch 3 / 500 \n",
            " - time: 1.1952650547027588 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9394833333333333 - val_acc: 0.9379\n",
            "Epoch 4 / 500 \n",
            " - time: 1.1846301555633545 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9478 - val_acc: 0.9454\n",
            "Epoch 5 / 500 \n",
            " - time: 1.2054591178894043 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.95085 - val_acc: 0.9476\n",
            "Epoch 6 / 500 \n",
            " - time: 1.1969456672668457 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9531666666666667 - val_acc: 0.9492\n",
            "Epoch 7 / 500 \n",
            " - time: 1.187875747680664 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9546666666666667 - val_acc: 0.9501\n",
            "Epoch 8 / 500 \n",
            " - time: 1.2003638744354248 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9558166666666666 - val_acc: 0.9503\n",
            "Epoch 9 / 500 \n",
            " - time: 1.2072429656982422 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9565333333333333 - val_acc: 0.9512\n",
            "Epoch 10 / 500 \n",
            " - time: 1.2049033641815186 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9573833333333334 - val_acc: 0.9514\n",
            "Epoch 11 / 500 \n",
            " - time: 1.193906545639038 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9581 - val_acc: 0.9515\n",
            "Epoch 12 / 500 \n",
            " - time: 1.199713945388794 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9586833333333333 - val_acc: 0.9517\n",
            "Epoch 13 / 500 \n",
            " - time: 1.2048425674438477 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9593666666666667 - val_acc: 0.9518\n",
            "Epoch 14 / 500 \n",
            " - time: 1.2088983058929443 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9599666666666666 - val_acc: 0.9521\n",
            "Epoch 15 / 500 \n",
            " - time: 1.2040767669677734 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9605333333333334 - val_acc: 0.9524\n",
            "Epoch 16 / 500 \n",
            " - time: 1.2192392349243164 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9609333333333333 - val_acc: 0.9528\n",
            "Epoch 17 / 500 \n",
            " - time: 1.2078802585601807 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9613833333333334 - val_acc: 0.9535\n",
            "Epoch 18 / 500 \n",
            " - time: 1.2270503044128418 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9618333333333333 - val_acc: 0.9541\n",
            "Epoch 19 / 500 \n",
            " - time: 1.2182435989379883 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9624333333333334 - val_acc: 0.9543\n",
            "Epoch 20 / 500 \n",
            " - time: 1.2091808319091797 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9630333333333333 - val_acc: 0.9546\n",
            "Epoch 21 / 500 \n",
            " - time: 1.2150835990905762 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9632666666666667 - val_acc: 0.9552\n",
            "Epoch 22 / 500 \n",
            " - time: 1.2210240364074707 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9637166666666667 - val_acc: 0.9548\n",
            "Epoch 23 / 500 \n",
            " - time: 1.2209458351135254 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9641666666666666 - val_acc: 0.9549\n",
            "Epoch 24 / 500 \n",
            " - time: 1.2161128520965576 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9646166666666667 - val_acc: 0.9552\n",
            "Epoch 25 / 500 \n",
            " - time: 1.2227637767791748 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9651166666666666 - val_acc: 0.9552\n",
            "Epoch 26 / 500 \n",
            " - time: 1.231522560119629 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9654833333333334 - val_acc: 0.9555\n",
            "Epoch 27 / 500 \n",
            " - time: 1.2232553958892822 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9658333333333333 - val_acc: 0.9557\n",
            "Epoch 28 / 500 \n",
            " - time: 1.2299113273620605 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9661666666666666 - val_acc: 0.9559\n",
            "Epoch 29 / 500 \n",
            " - time: 1.2254362106323242 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9663833333333334 - val_acc: 0.9561\n",
            "Epoch 30 / 500 \n",
            " - time: 1.238501787185669 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9667833333333333 - val_acc: 0.956\n",
            "Epoch 31 / 500 \n",
            " - time: 1.231999397277832 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9670166666666666 - val_acc: 0.9561\n",
            "Epoch 32 / 500 \n",
            " - time: 1.2212471961975098 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9672666666666667 - val_acc: 0.9565\n",
            "Epoch 33 / 500 \n",
            " - time: 1.2485861778259277 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9673833333333334 - val_acc: 0.9567\n",
            "Epoch 34 / 500 \n",
            " - time: 1.2309560775756836 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9675833333333334 - val_acc: 0.9567\n",
            "Epoch 35 / 500 \n",
            " - time: 1.2259876728057861 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9677833333333333 - val_acc: 0.9568\n",
            "Epoch 36 / 500 \n",
            " - time: 1.2266852855682373 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9679166666666666 - val_acc: 0.9571\n",
            "Epoch 37 / 500 \n",
            " - time: 1.2380821704864502 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9680166666666666 - val_acc: 0.9572\n",
            "Epoch 38 / 500 \n",
            " - time: 1.2364418506622314 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9680333333333333 - val_acc: 0.9573\n",
            "Epoch 39 / 500 \n",
            " - time: 1.235468864440918 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9680666666666666 - val_acc: 0.9576\n",
            "Epoch 40 / 500 \n",
            " - time: 1.2369883060455322 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9682833333333334 - val_acc: 0.9576\n",
            "Epoch 41 / 500 \n",
            " - time: 1.2366344928741455 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9685666666666667 - val_acc: 0.9574\n",
            "Epoch 42 / 500 \n",
            " - time: 1.2360167503356934 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9686833333333333 - val_acc: 0.9575\n",
            "Epoch 43 / 500 \n",
            " - time: 1.2366881370544434 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.96875 - val_acc: 0.9574\n",
            "Epoch 44 / 500 \n",
            " - time: 1.2378885746002197 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9688666666666667 - val_acc: 0.9576\n",
            "Epoch 45 / 500 \n",
            " - time: 1.2368676662445068 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9690333333333333 - val_acc: 0.9575\n",
            "Epoch 46 / 500 \n",
            " - time: 1.2415235042572021 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9690833333333333 - val_acc: 0.9574\n",
            "Epoch 47 / 500 \n",
            " - time: 1.2544143199920654 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9692 - val_acc: 0.9576\n",
            "Epoch 48 / 500 \n",
            " - time: 1.2391505241394043 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9694 - val_acc: 0.9575\n",
            "Epoch 49 / 500 \n",
            " - time: 1.2537992000579834 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9694833333333334 - val_acc: 0.9576\n",
            "Epoch 50 / 500 \n",
            " - time: 1.2397868633270264 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.96955 - val_acc: 0.9575\n",
            "Epoch 51 / 500 \n",
            " - time: 1.2533106803894043 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9696666666666667 - val_acc: 0.9576\n",
            "Epoch 52 / 500 \n",
            " - time: 1.2570836544036865 - sq_loss: 31310.359375 - tot_loss: 31310.359375 - acc: 0.9697666666666667 - val_acc: 0.9577\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-59d153bf585f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;31m# update W3 and b3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdateWb_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mWs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrho3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m   \u001b[0mWs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mbs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-1d3145c7082a>\u001b[0m in \u001b[0;36mupdateWb_js\u001b[0;34m(U, V, W, b, alpha, rho)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_U\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mWstar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol_U\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mbstar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mWstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbstar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we perform one-hot encoding on the y-> y_one_hot will be a tensor of dimension 10*training_samples, for each sample there\n",
        "# will be a one in the row corresponding to the class.\n",
        "\n",
        "y_one_hot = torch.zeros(N, K).to(device = device).scatter_(1, torch.reshape(y_train,(N,1)), 1)\n",
        "y_one_hot = torch.t(y_one_hot).to(device=device)\n",
        "\n",
        "y_test_one_hot = torch.zeros(N_test, K).to(device = device).scatter_(1, torch.reshape(y_test,(N_test,1)), 1)\n",
        "y_test_one_hot = torch.t(y_test_one_hot).to(device=device)\n",
        "\n",
        "# Iterations\n",
        "print('Train on', N, 'samples, validate on', N_test, 'samples')\n",
        "for k in range(niter):\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  #select_the nodes for the third layer\n",
        "  nodes_up3 = rand(0,10,5)\n",
        "  nodes_up2 = rand(0,1500,750)\n",
        "  nodes_up1 = rand(0,1500,750)\n",
        "  V3_sample = V3[nodes_up3]\n",
        "  U3_sample = U3[nodes_up3]\n",
        "  W3_sample = torch.transpose(torch.transpose(W3[nodes_up3],0,1)[nodes_up2],0,1)\n",
        "  b3_sample = b3[nodes_up3]\n",
        "  V2_sample = V2[nodes_up2]\n",
        "  U2_sample = U2[nodes_up2]\n",
        "  W2_sample = torch.transpose(torch.transpose(W2[nodes_up2],0,1)[nodes_up1],0,1)\n",
        "  b2_sample = b2[nodes_up2]\n",
        "  V1_sample = V1[nodes_up1]\n",
        "  U1_sample = U1[nodes_up1]\n",
        "  W1_sample = W1[nodes_up1]\n",
        "  b1_sample = b1[nodes_up1]\n",
        "  # update V3\n",
        "  V3_sample = (y_one_hot[nodes_up3] + gamma3*U3_sample + alpha1*V3_sample)/(1+ gamma3 + alpha1)\n",
        "      \n",
        "  # update U3 \n",
        "  U3_sample = (gamma3*V3_sample + rho3*(torch.mm(W3_sample,V2_sample) + b3_sample.repeat(1,N)))/(gamma3 + rho3)\n",
        "\n",
        "  # update W3 and b3\n",
        "  W3_sample, b3_sample = updateWb_js(U3_sample,V2_sample,W3_sample,b3_sample,alpha1,rho3)\n",
        "  \n",
        "  # update V2\n",
        "  V2_sample = updateV_js(U2_sample,U3_sample,W3_sample,b3_sample,rho3,gamma2)\n",
        "      \n",
        "  # update U2\n",
        "  U2_sample = relu_prox(V2_sample,(rho2*torch.addmm(b2_sample.repeat(1,N), W2_sample, V1_sample) + alpha2*U2_sample)/(rho2 + alpha2),(rho2 + alpha2)/gamma2,750,N)\n",
        "      \n",
        "  # update W2 and b2\n",
        "  W2_sample, b2_sample = updateWb_js(U2_sample,V1_sample,W2_sample,b2_sample,alpha3,rho2)\n",
        "  \n",
        "  # update V1\n",
        "  V1_sample = updateV_js(U1_sample,U2_sample,W2_sample,b2_sample,rho2,gamma1)\n",
        "      \n",
        "  # update U1\n",
        "  U1_sample = relu_prox(V1_sample,(rho1*torch.addmm(b1_sample.repeat(1,N), W1_sample, x_train) + alpha7*U1_sample)/(rho1 + alpha7),(rho1 + alpha7)/gamma1,750,N)\n",
        "  \n",
        "  # update W1 and b1\n",
        "  W1_sample, b1_sample = updateWb_js(U1_sample,x_train,W1_sample,b1_sample,alpha8,rho1)\n",
        "\n",
        "  print(list(zip(nodes_up3,nodes_up2)))\n",
        "  #print(W3[].shape)\n",
        "  V3[nodes_up3] = V3_sample\n",
        "  U3[nodes_up3] = U3_sample\n",
        "  W3[index_lists(nodes_up3,nodes_up2)] = torch.reshape(W3_sample,(750*5,))\n",
        "  b3[nodes_up3] = b3_sample\n",
        "  V2[nodes_up2] = V2_sample\n",
        "  U2[nodes_up2] = U2_sample\n",
        "  W2[index_lists(nodes_up2,nodes_up1)] = torch.reshape(W2_sample,(750*750,))\n",
        "  b2[nodes_up2] = b2_sample\n",
        "  V1[nodes_up1] = V1_sample\n",
        "  U1[nodes_up1] = V1_sample\n",
        "  W1[nodes_up1] = W1_sample\n",
        "  b1[nodes_up1] = b1_sample\n",
        "\n",
        "  a1_train = nn.ReLU()(torch.addmm(b1_sample.repeat(1, N), W1_sample, x_train))\n",
        "  a2_train = nn.ReLU()(torch.addmm(b2_sample.repeat(1, N), W2_sample, a1_train))\n",
        "  pred = torch.argmax(torch.addmm(b3_sample.repeat(1, N), W3_sample, a2_train), dim=0)\n",
        "\n",
        "  a1_test = nn.ReLU()(torch.addmm(b1_sample.repeat(1, N_test), W1_sample, x_test))\n",
        "  a2_test = nn.ReLU()(torch.addmm(b2_sample.repeat(1, N_test), W2_sample, a1_test))\n",
        "  pred_test = torch.argmax(torch.addmm(b3_sample.repeat(1, N_test), W3_sample, a2_test), dim=0)\n",
        "      \n",
        "  loss1[k] = gamma3/2*torch.pow(torch.dist(V3,y_one_hot,2),2).cpu().numpy()\n",
        "  loss2[k] = loss1[k] + rho1/2*torch.pow(torch.dist(torch.addmm(b1.repeat(1,N), W1, x_train),U1,2),2).cpu().numpy() \\\n",
        "  +rho2/2*torch.pow(torch.dist(torch.addmm(b2.repeat(1,N), W2, V1),U2,2),2).cpu().numpy() \\\n",
        "  +rho3/2*torch.pow(torch.dist(torch.addmm(b3.repeat(1,N), W3, V2),U3,2),2).cpu().numpy()\n",
        "      \n",
        "  # compute training accuracy\n",
        "  correct_train = pred == y_train\n",
        "  accuracy_train[k] = np.mean(correct_train.cpu().numpy())\n",
        "      \n",
        "  # compute validation accuracy\n",
        "  correct_test = pred_test == y_test\n",
        "  accuracy_test[k] = np.mean(correct_test.cpu().numpy())\n",
        "      \n",
        "  # compute training time\n",
        "  stop = time.time()\n",
        "  duration = stop - start\n",
        "  time1[k] = duration\n",
        "      \n",
        "  # print results\n",
        "  print('Epoch', k + 1, '/', niter, '\\n', \n",
        "        '-', 'time:', time1[k], '-', 'sq_loss:', loss1[k], '-', 'tot_loss:', loss2[k], \n",
        "        '-', 'acc:', accuracy_train[k], '-', 'val_acc:', accuracy_test[k])"
      ],
      "metadata": {
        "id": "MM7vUi3xpSai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrFu1jgT3kLz"
      },
      "source": [
        "# Training\n",
        "\n",
        "Note: Fix it so that it moves everything to device in the following function and that it does the label sample split here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## We plot the train losses\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('train losses')\n",
        "plt.plot(np.arange(0,loss1.shape[0]), loss1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eFw-MbOat5p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## We plot the test accuracy\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('test accuracy')\n",
        "plt.plot(np.arange(0, accuracy_test.shape[0]), accuracy_test)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2gF9-R3-uB3c"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Block_coordinate_descent.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}