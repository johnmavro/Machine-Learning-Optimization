{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DFW.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A17RyO6oElc",
        "outputId": "7d03bb43-4af0-4238-db0a-93ac8c868a38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/Othercomputers/Il mio laptop/Machine-Learning-Optimization_new\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/Othercomputers/Il mio laptop/Machine-Learning-Optimization_new"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import and utilities \n",
        "\n",
        "from Frank_Wolfe.utils.utils import *\n",
        "from Frank_Wolfe.DFW import *\n",
        "from Frank_Wolfe.constraints.constraints import *\n",
        "from Frank_Wolfe.architectures import *\n",
        "from Frank_Wolfe.MultiClassHingeLoss import *\n",
        "!pip install barbar\n",
        "from barbar import Bar\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "device = \"cpu\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC7BWlWnoFBa",
        "outputId": "c0041085-0149-4266-f403-33cfa53b0c07",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting barbar\n",
            "  Downloading barbar-0.2.1-py3-none-any.whl (3.9 kB)\n",
            "Installing collected packages: barbar\n",
            "Successfully installed barbar-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_stats = True\n",
        "save_figs = True\n",
        "load = False"
      ],
      "metadata": {
        "id": "gMj3YVkWoM9x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose dataset name and architecture of the network \n",
        "\n",
        "dataset_name = 'CIFAR10' #@param ['CIFAR10', 'CIFAR100']\n",
        "model_type = 'DenseNet' #@param ['DenseNet', 'WideResNet', 'GoogLeNet', 'ResNeXt']\n",
        "if model_type == 'GoogLeNet':\n",
        "  model = GoogleNet(num_class=10 if dataset_name == 'CIFAR10' else 100)\n",
        "elif model_type == 'DenseNet':\n",
        "  model = torchvision.models.densenet121(pretrained=False)\n",
        "elif model_type == 'ResNeXt':\n",
        "  model = torchvision.models.resnet101(pretrained=False)\n",
        "elif model_type == 'WideResNet':\n",
        "  model =  WideResNet(num_classes=10 if dataset_name == 'CIFAR10' else 100)\n",
        "else:\n",
        "  raise ValueError(\"Please, select an available architecture\")\n",
        "\n",
        "datasetDict = setDatasetAttributes(dataset_name)\n",
        "trainTransformDict, testTransformDict = setTrainAndTest(dataset_name)\n",
        "\n",
        "root = f\"{dataset_name}-dataset\"\n",
        "\n",
        "trainData = datasetDict['datasetDict'](root=root, train=True, download=True,\n",
        "                                            transform=trainTransformDict[dataset_name])\n",
        "testData = datasetDict['datasetDict'](root=root, train=False,\n",
        "                                        transform=testTransformDict[dataset_name])\n",
        "\n",
        "model = model.to(device=\"cuda:0\")\n",
        "\n",
        "# define the loss object\n",
        "loss_criterion = MultiClassHingeLoss().to(device=\"cuda:0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf6SJcqjoUSA",
        "outputId": "05646495-ff4d-4ad7-8f76-11292574ff34",
        "cellView": "form"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose optimizer and parameters \n",
        "\n",
        "optimizer_name = \"SGD\" #@param  ['DFW', 'Adam', 'AdaGrad', 'SGD with momentum', 'SGD']\n",
        "momentum = 0.9 #@param {type:\"number\"}\n",
        "lr = 0.01 #@param {type:\"number\"}\n",
        "prox_steps =  1#@param{type:\"integer\"}\n",
        "eta =  1#@param {type:\"number\"}\n",
        "beta_1 = 0.9 #@param {type:\"number\"}\n",
        "beta_2 = 0.99 #@param {type:\"number\"}\n",
        "weight_decay = 0.01 #@param {type:\"number\"}\n",
        "\n",
        "if optimizer_name == \"DFW\":\n",
        "  optimizer = DFW(params=model.parameters(), eta=eta, momentum=momentum, \n",
        "                  prox_steps=prox_steps)\n",
        "  assert eta > 0\n",
        "elif optimizer_name == \"SGD\" or optimizer_name == \"SGD with momentum\":\n",
        "  optimizer = torch.optim.SGD(params=model.parameters(), lr=lr,\n",
        "                              momentum=momentum)\n",
        "  assert lr > 0\n",
        "  assert 0 <= momentum <= 1\n",
        "elif optimizer_name == \"Adam\":\n",
        "  optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, \n",
        "                               betas=(beta_1, beta_2), weight_decay=weight_decay)\n",
        "elif optimizer_name == \"AdaGrad\":\n",
        "  optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, weight_decay=weight_decay)"
      ],
      "metadata": {
        "id": "C6IgJdqGoWkV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train the network  \n",
        "\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "epochs_times = []\n",
        "\n",
        "nepochs = 10 #@param {type:\"integer\"}\n",
        "batch_size =   128#@param {type:\"integer\"}\n",
        "verbose = 0 #@param [0, 1]\n",
        "\n",
        "# Loaders\n",
        "trainLoader = torch.utils.data.DataLoader(trainData, batch_size=batch_size, shuffle=True,\n",
        "                                      pin_memory=torch.cuda.is_available(), num_workers=2)\n",
        "testLoader = torch.utils.data.DataLoader(testData, batch_size=batch_size, shuffle=False,\n",
        "                                      pin_memory=torch.cuda.is_available(), num_workers=2)\n",
        "\n",
        "# initialize some necessary metrics objects\n",
        "train_loss, train_accuracy = AverageMeter(), AverageMeter()\n",
        "test_loss, test_accuracy = AverageMeter(), AverageMeter()\n",
        "\n",
        "# function to reset metrics\n",
        "def reset_metrics():\n",
        "    train_loss.reset()\n",
        "    train_accuracy.reset()\n",
        "    test_loss.reset()\n",
        "    test_accuracy.reset()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(data=\"train\"):\n",
        "    if data == \"train\":\n",
        "        loader = trainLoader\n",
        "        mean_loss, mean_accuracy = train_loss, train_accuracy\n",
        "    elif data == \"test\":\n",
        "        loader = testLoader\n",
        "        mean_loss, mean_accuracy = test_loss, test_accuracy\n",
        "    \n",
        "    sys.stdout.write(f\"Evaluation of {data} data:\\n\")\n",
        "    for x_input, y_target in Bar(loader):\n",
        "        x_input, y_target = x_input.to(device=\"cuda:0\"), y_target.to(device=\"cuda:0\")\n",
        "        output = model.eval()(x_input)\n",
        "        loss = loss_criterion(output, y_target)\n",
        "        mean_loss(loss.item(), len(y_target))\n",
        "        mean_accuracy(Utilities.categorical_accuracy(y_true=y_target, output=output), len(y_target))\n",
        "\n",
        "for epoch in range(nepochs + 1):\n",
        "    start = time.time()\n",
        "    reset_metrics()\n",
        "    sys.stdout.write(f\"\\n\\nEpoch {epoch}/{nepochs}\\n\")\n",
        "    if epoch == 0:\n",
        "        # Just evaluate the model once to get the metrics\n",
        "        evaluate_model(data='train')\n",
        "    else:\n",
        "        # Train\n",
        "        sys.stdout.write(f\"Training:\\n\")\n",
        "        for x_input, y_target in Bar(trainLoader):\n",
        "            x_input, y_target = x_input.to(device=\"cuda:0\"), y_target.to(device=\"cuda:0\")\n",
        "            optimizer.zero_grad()  # Zero the gradient buffers\n",
        "            output = model.train()(x_input)\n",
        "            loss = loss_criterion(output, y_target)\n",
        "            loss.backward()  # Backpropagation\n",
        "            if optimizer_name == \"DFW\":\n",
        "              w_dict, w_0_dict = optimizer.step(lambda: float(loss))\n",
        "            else:\n",
        "              optimizer.step()\n",
        "            output = model.train()(x_input)\n",
        "            loss = loss_criterion(output, y_target)\n",
        "            if optimizer_name == \"DFW\":\n",
        "              if optimizer.prox_steps > 1:\n",
        "                optimizer.zero_grad()  # Zero the gradient buffers\n",
        "                output = model.train()(x_input)\n",
        "                loss = loss_criterion(output, y_target)\n",
        "                loss.backward()  # Backpropagation\n",
        "                optimizer._proximal_step(lambda: float(loss), w_dict, w_0_dict) \n",
        "            train_loss(loss.item(), len(y_target))\n",
        "            train_accuracy(Utilities.categorical_accuracy(y_true=y_target, output=output), len(y_target))\n",
        "\n",
        "    evaluate_model(data='test')\n",
        "    sys.stdout.write(f\"\\n Finished epoch {epoch}/{nepochs}: Train Loss {train_loss.result()} | Test Loss {test_loss.result()} | Train Acc {train_accuracy.result()} | Test Acc {test_accuracy.result()}\\n\")\n",
        "\n",
        "    train_losses.append(train_loss.result())\n",
        "    train_accuracies.append(train_accuracy.result())\n",
        "    test_losses.append(test_loss.result())\n",
        "    test_accuracies.append(test_accuracy.result())\n",
        "\n",
        "\n",
        "    elapsed_time = time.time()-start\n",
        "    sys.stdout.write(f\"Time elapsed for the current epoch {elapsed_time}\")\n",
        "    epochs_times.append(elapsed_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq-yV7BYplnM",
        "outputId": "be733c1b-83ca-482d-c842-c8589bd72b66"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 0/10\n",
            "Evaluation of train data:\n",
            "50000/50000: [===============================>] - ETA 0.2s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.1s\n",
            "\n",
            " Finished epoch 0/10: Train Loss 1.978725018043518 | Test Loss 2.0440477750778197 | Train Acc 0.13038 | Test Acc 0.1305\n",
            "Time elapsed for the current epoch 26.92134737968445\n",
            "\n",
            "Epoch 1/10\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.3s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.2s\n",
            "\n",
            " Finished epoch 1/10: Train Loss 1.0872339519882201 | Test Loss 1.0639119579315186 | Train Acc 0.15178 | Test Acc 0.1614\n",
            "Time elapsed for the current epoch 59.98356509208679\n",
            "\n",
            "Epoch 2/10\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.3s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.1s\n",
            "\n",
            " Finished epoch 2/10: Train Loss 1.0051344507217408 | Test Loss 1.180032858657837 | Train Acc 0.16666 | Test Acc 0.1264\n",
            "Time elapsed for the current epoch 60.257572650909424\n",
            "\n",
            "Epoch 3/10\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.3s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.2s\n",
            "\n",
            " Finished epoch 3/10: Train Loss 1.0021776636886597 | Test Loss 1.0421106214523315 | Train Acc 0.19132 | Test Acc 0.1592\n",
            "Time elapsed for the current epoch 60.0362012386322\n",
            "\n",
            "Epoch 4/10\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.3s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.2s\n",
            "\n",
            " Finished epoch 4/10: Train Loss 1.001666866722107 | Test Loss 1.096367946243286 | Train Acc 0.2002 | Test Acc 0.207\n",
            "Time elapsed for the current epoch 60.04589509963989\n",
            "\n",
            "Epoch 5/10\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.3s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.1s\n",
            "\n",
            " Finished epoch 5/10: Train Loss 1.0017040020751953 | Test Loss 1.112831521987915 | Train Acc 0.20322 | Test Acc 0.2202\n",
            "Time elapsed for the current epoch 60.15166735649109\n",
            "\n",
            "Epoch 6/10\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.3s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.2s\n",
            "\n",
            " Finished epoch 6/10: Train Loss 1.0010847493171693 | Test Loss 1.089197536277771 | Train Acc 0.20914 | Test Acc 0.2572\n",
            "Time elapsed for the current epoch 59.910887479782104\n",
            "\n",
            "Epoch 7/10\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.3s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.2s\n",
            "\n",
            " Finished epoch 7/10: Train Loss 1.000616549835205 | Test Loss 1.0984778049468995 | Train Acc 0.21474 | Test Acc 0.2471\n",
            "Time elapsed for the current epoch 59.589694023132324\n",
            "\n",
            "Epoch 8/10\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.3s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.2s\n",
            "\n",
            " Finished epoch 8/10: Train Loss 0.9995449896621704 | Test Loss 1.0305277282714844 | Train Acc 0.22222 | Test Acc 0.2267\n",
            "Time elapsed for the current epoch 60.03062844276428\n",
            "\n",
            "Epoch 9/10\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.3s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.1s\n",
            "\n",
            " Finished epoch 9/10: Train Loss 0.9964185728263855 | Test Loss 1.0821557229995729 | Train Acc 0.2257 | Test Acc 0.2535\n",
            "Time elapsed for the current epoch 60.003119468688965\n",
            "\n",
            "Epoch 10/10\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.3s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.2s\n",
            "\n",
            " Finished epoch 10/10: Train Loss 0.9941247868347168 | Test Loss 1.0657698902130126 | Train Acc 0.22456 | Test Acc 0.2399\n",
            "Time elapsed for the current epoch 60.211220026016235"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save training results and plot\n",
        "\n",
        "if load:\n",
        "    output_folder = os.path.join(os.getcwd(), 'results')\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    fname = output_folder + '/stats_dict_' + model_type + '.pkl'\n",
        "    with open(fname, 'rb') as handle:\n",
        "        stats_dict = pickle.load(handle)\n",
        "\n",
        "results = {'epochs': nepochs, 'train_losses': train_losses, \n",
        "           'train_acc': train_accuracies, 'test_losses': test_losses, \n",
        "           'test_acc': test_accuracies, 'elapsed_time': elapsed_time}\n",
        "stats_dict = {}\n",
        "stats_dict.update({optimizer_name: results})\n",
        "\n",
        "# save everything onto file\n",
        "if save_stats: \n",
        "    output_folder = os.path.join(os.getcwd(), 'results')  # set the folder\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    fname = output_folder + '/stats_dict_' + model_type + '.pkl'\n",
        "    with open(fname, 'wb') as handle:\n",
        "        pickle.dump(stats_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9w2DgFHAExBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters used in the report"
      ],
      "metadata": {
        "id": "ldLaA2by6VYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to reproduce our results, the following set of parameters should be used.\\\n",
        "If not specified, the remaining parameters (e.g. $\\epsilon$ for Adam and Adagrad) are set to their default values.\n",
        "\n",
        "Deep Frank Wolfe:\\\n",
        "$η = 0.1$, $μ = 0.9$, $w_d = 0$\n",
        "\n",
        "Stochastic Gradient Descent:\\\n",
        "$\\gamma = 0.001$, $\\mu = 0.9$, $w_d = 0$\n",
        "\n",
        "Adam:\\\n",
        "$\\gamma = 0.001$, $\\mu = 0.9$, $\\beta_1 = 0.9$, $\\beta_2 = 0.99$\n",
        "\n",
        "AdaGrad:\\\n",
        "$\\gamma = 0.001$, $w_d = 0$\n"
      ],
      "metadata": {
        "id": "U7F6Ai3G6bFd"
      }
    }
  ]
}