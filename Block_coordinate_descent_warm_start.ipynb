{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KDbRlCKv4U8",
        "outputId": "a44169cd-b045-4049-ffa5-f58184e169f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 1.11.0+cu113\n",
            "Torchvision Version: 0.12.0+cu113\n",
            "GPU is available? True\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms, utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "\n",
        "from utilities import *\n",
        "\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "print(\"Torchvision Version:\", torchvision.__version__)\n",
        "print(\"GPU is available?\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "nYF1cnvPwthx"
      },
      "outputs": [],
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqvTMJAbw74Q"
      },
      "source": [
        "# Imported datasets\n",
        "For the testing and comparison of our algorithms we will use the following datasets:\n",
        "\n",
        "1. MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "icE91aqaw110"
      },
      "outputs": [],
      "source": [
        "ts = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0,), (1,))])\n",
        "mnist_trainset = datasets.MNIST('../data', train=True, download=True, transform=ts)\n",
        "mnist_testset = datasets.MNIST(root='../data', train=False, download=True, transform=ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PEt-flsyEKM"
      },
      "source": [
        "# Train - test split\n",
        "\n",
        "Code taken from https://github.com/timlautk/BCD-for-DNNs-PyTorch/blob/master/bcd_dnn_mlp_mnist.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "hVoD2jFeg-69"
      },
      "outputs": [],
      "source": [
        "x_train, y_train, x_test, y_test = train_test_split(mnist_trainset, mnist_testset)\n",
        "\n",
        "# we move to device to use GPU\n",
        "\n",
        "x_train = x_train.to(device = device)\n",
        "x_test = x_test.to(device = device)\n",
        "y_train = y_train.to(device = device)\n",
        "y_test = y_test.to(device = device)\n",
        "\n",
        "# we perform one-hot encoding on the y-> y_one_hot will be a tensor of dimension 10*training_samples, for each sample there\n",
        "# will be a one in the row corresponding to the class.\n",
        "\n",
        "N = y_train.shape[0]\n",
        "K = 10 # number of classes\n",
        "y_one_hot = torch.zeros(N, K).to(device = device).scatter_(1, torch.reshape(y_train,(N,1)), 1)\n",
        "y_one_hot = torch.t(y_one_hot).to(device=device)\n",
        "\n",
        "N_test = y_test.shape[0]\n",
        "y_test_one_hot = torch.zeros(N_test, K).to(device = device).scatter_(1, torch.reshape(y_test,(N_test,1)), 1)\n",
        "y_test_one_hot = torch.t(y_test_one_hot).to(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me_6E9_Hxsxw"
      },
      "source": [
        "# Architecture initialization\n",
        "\n",
        "For the MultiLayerPerceptron we have the parameters **input_size** , **hidden_size**,**output_size** corresponding to the size of the input layer, the hidden layer and the output layer, respectively.\n",
        "\n",
        "The MLP only has 3 layers like https://github.com/timlautk/BCD-for-DNNs-PyTorch/blob/master/bcd_dnn_mlp_mnist.ipynb as a starting point.\n",
        "\n",
        "Also we use ReLU currently for the same reason."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "hlRg4UWiezMt"
      },
      "outputs": [],
      "source": [
        "input_size = 28*28\n",
        "hidden_size = 1500\n",
        "output_size = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "0HQgkqHxr_zv"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(32)\n",
        "d0 = input_size\n",
        "d1 = d2 = 1500\n",
        "d3 = output_size \n",
        "\n",
        "# The layers are: input + 2 hidden + output\n",
        "\n",
        "# we represent the weigths of each layer as matrices with d_{i-1} columns and d_{i} rows\n",
        "\n",
        "# Weight initialization (we replicate pytorch initialization)\n",
        "std_1 = math.sqrt(1/d0)\n",
        "W1 = torch.FloatTensor(d1, d0).uniform_(-std_1, std_1)\n",
        "b1 = torch.FloatTensor(d1, 1).uniform_(-std_1, std_1)\n",
        "\n",
        "# we move them to GPU\n",
        "b1 = b1.to('cuda:0')\n",
        "W1 = W1.to('cuda:0')\n",
        "\n",
        "\n",
        "std_2 = math.sqrt(1/d1)\n",
        "W2 = torch.FloatTensor(d2, d1).uniform_(-std_2, std_2)\n",
        "b2 = torch.FloatTensor(d2, 1).uniform_(-std_2, std_2)\n",
        "\n",
        "# we move them to GPU\n",
        "b2 = b2.to('cuda:0')\n",
        "W2 = W2.to('cuda:0')\n",
        "\n",
        "std_3 = math.sqrt(1/d2)\n",
        "W3 = torch.FloatTensor(d3, d2).uniform_(-std_3, std_3)\n",
        "b3 = torch.FloatTensor(d3, 1).uniform_(-std_3, std_3)\n",
        "\n",
        "# we move them to GPU\n",
        "b3 = b3.to('cuda:0')\n",
        "W3 = W3.to('cuda:0')\n",
        "\n",
        "\n",
        "U1 = torch.addmm(b1.repeat(1, N), W1, x_train) # equivalent to W1@x_train+b1.repeat(1,N)\n",
        "V1 = nn.ReLU()(U1)\n",
        "U2 = torch.addmm(b2.repeat(1, N), W2, V1)\n",
        "V2 = nn.ReLU()(U2)\n",
        "U3 = torch.addmm(b3.repeat(1, N), W3, V2)\n",
        "V3 = U3\n",
        "\n",
        "# constant initializations\n",
        "\n",
        "# gamma is the parameter for regularization (gamma/2)*||V_N-U_N*V_{N}^{k-1}||^2\n",
        "gamma = 0.7\n",
        "gamma1 = gamma2 = gamma3 = gamma4 = gamma  \n",
        "\n",
        "# rho is the parameter for the regularization (rho/2)*||U_N^k-W_N*V_{N-1}^{k-1}||^2\n",
        "rho = gamma\n",
        "rho1 = rho2 = rho3 = rho4 = rho\n",
        "\n",
        "\n",
        "# alphas are the parameters for the regularization---> (alpha/2)*||W_N-W_N^{k-1}||^2 ,\n",
        "#(alpha/2)*||V_N-V_N^{k-1}||^2\n",
        "alpha = 5\n",
        "alpha1 = alpha2 = alpha3 = alpha4 = alpha5 = alpha6 = alpha7 \\\n",
        "= alpha8 = alpha9 = alpha10 = alpha \n",
        "\n",
        "# initialization of the vectors of losses and accuracies\n",
        "niter = 30\n",
        "loss1 = np.empty(niter)\n",
        "loss2 = np.empty(niter)\n",
        "accuracy_train = np.empty(niter)\n",
        "accuracy_test = np.empty(niter)\n",
        "time1 = np.empty(niter)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def update_v_js(U1, U2, W, b, rho, gamma):\n",
        "    \"\"\"\n",
        "    The function updates the V_js parameters during the training phase\n",
        "    \n",
        "    :param U1: The U parameter on the same level of V that we are updating\n",
        "    :param U2: The U parameter which is in the next level of the V that we are updating\n",
        "    :param W: The W parameter which is in the next level of the V that we are updating\n",
        "    :param b: The b parameter which is in the next level of the V that we are updating\n",
        "    :param rho: The constant rho parameter which is in the next level of the V that we are updating\n",
        "    :param gamma: The constant gamma parameter which is in the next level of the V that we are updating\n",
        "    :return: The updated V\n",
        "    \"\"\"\n",
        "    _, d = W.size()\n",
        "    I = torch.eye(d, device=device)\n",
        "    U1 = nn.ReLU()(U1)\n",
        "    _, col_U2 = U2.size()\n",
        "    Vstar = torch.mm(torch.inverse(rho * (torch.mm(torch.t(W), W)) + gamma * I),\n",
        "                     rho * torch.mm(torch.t(W), U2 - b.repeat(1, col_U2)) + gamma * U1)\n",
        "    return Vstar\n",
        "\n",
        "\n",
        "def update_wb_js(U, V, W, b, alpha, rho):\n",
        "    \"\"\"\n",
        "    The function updates the W and b parameters during the training phase\n",
        "    \n",
        "    :param U: The U in the current level of W and b\n",
        "    :param V: The V in the previous level with respect to the W that we are updating\n",
        "    :param W: The current W that we have to update\n",
        "    :param b: The current b that we have to update\n",
        "    :param alpha: The alpha constant of the updates\n",
        "    :param rho: The rho constant of the updates\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    d, N = V.size()\n",
        "    I = torch.eye(d, device=device)\n",
        "    _, col_U = U.size()\n",
        "    Wstar = torch.mm(alpha * W + rho * torch.mm(U - b.repeat(1, col_U), torch.t(V)),\n",
        "                     torch.inverse(alpha * I + rho * (torch.mm(V, torch.t(V)))))\n",
        "    bstar = (alpha * b + rho * torch.sum(U - torch.mm(W, V), dim=1).reshape(b.size())) / (rho * N + alpha)\n",
        "    return Wstar, bstar\n",
        "\n",
        "\n",
        "def relu_prox(a, b, gamma, d, N):\n",
        "    \"\"\"\n",
        "    The function compute the solution to the relu proximal update problem\n",
        "    \n",
        "    :param a: the a in the closed formula of the linearized update\n",
        "    :param b: the b in the closed formula of the linearized update\n",
        "    :param gamma: The constant used in the update\n",
        "    :param d: the dimension of the current layer\n",
        "    :param N: The number of samples\n",
        "    :return: The obtained solution of the prox update\n",
        "    \"\"\"\n",
        "    val = torch.empty(d, N, device=device)\n",
        "    x = (a + gamma * b) / (1 + gamma)\n",
        "    y = torch.min(b, torch.zeros(d, N, device=device))\n",
        "    # torch.zeros(d,N, device=device)\n",
        "    val = torch.where(a + gamma * b < 0, y, torch.zeros(d, N, device=device))\n",
        "    val = torch.where(\n",
        "        ((a + gamma * b >= 0) & (b >= 0)) | ((a * (gamma - np.sqrt(gamma * (gamma + 1))) <= gamma * b) & (b < 0)), x,\n",
        "        val)\n",
        "    val = torch.where((-a <= gamma * b) & (gamma * b <= a * (gamma - np.sqrt(gamma * (gamma + 1)))), b, val)\n",
        "\n",
        "    return val"
      ],
      "metadata": {
        "id": "tpG7KN8JkT83"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_dJSsAAr_z1",
        "outputId": "12b1e8cb-3df7-4408-ca6a-87948f7ad265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1 / 30 \n",
            " - time: 2.3160266876220703 - train_loss: 15883.658452956379 - acc_train: 0.5483666666666667 - acc_test: 0.5625\n",
            "0.7 0.7\n",
            "Epoch 2 / 30 \n",
            " - time: 2.3351428508758545 - train_loss: 11754.880328637173 - acc_train: 0.9021833333333333 - acc_test: 0.9018\n",
            "0.7 0.7\n",
            "Epoch 3 / 30 \n",
            " - time: 2.393686056137085 - train_loss: 8790.470114510692 - acc_train: 0.9384333333333333 - acc_test: 0.9384\n",
            "0.7 0.7\n",
            "Epoch 4 / 30 \n",
            " - time: 2.380854368209839 - train_loss: 6611.96614340581 - acc_train: 0.9463666666666667 - acc_test: 0.945\n",
            "0.7 0.7\n",
            "Epoch 5 / 30 \n",
            " - time: 2.3713650703430176 - train_loss: 4987.047476461156 - acc_train: 0.9496833333333333 - acc_test: 0.9469\n",
            "0.7 0.7\n",
            "Epoch 6 / 30 \n",
            " - time: 2.3583242893218994 - train_loss: 3766.3030269205565 - acc_train: 0.9517 - acc_test: 0.9481\n",
            "0.7 0.7\n",
            "Epoch 7 / 30 \n",
            " - time: 2.4130897521972656 - train_loss: 2846.26274257265 - acc_train: 0.95305 - acc_test: 0.949\n",
            "0.7 0.7\n",
            "Epoch 8 / 30 \n",
            " - time: 2.361016273498535 - train_loss: 2151.948470703512 - acc_train: 0.9540333333333333 - acc_test: 0.9497\n",
            "0.7 0.7\n",
            "Epoch 9 / 30 \n",
            " - time: 2.3951878547668457 - train_loss: 1627.726363893598 - acc_train: 0.95505 - acc_test: 0.9502\n",
            "0.7 0.7\n",
            "Epoch 10 / 30 \n",
            " - time: 2.3675246238708496 - train_loss: 1231.8202805787323 - acc_train: 0.9558166666666666 - acc_test: 0.9504\n",
            "0.7 0.7\n",
            "Epoch 11 / 30 \n",
            " - time: 2.4281673431396484 - train_loss: 932.8512834534048 - acc_train: 0.9565666666666667 - acc_test: 0.9506\n",
            "0.7 0.7\n",
            "Epoch 12 / 30 \n",
            " - time: 2.383122682571411 - train_loss: 707.0595323100684 - acc_train: 0.9573 - acc_test: 0.9512\n",
            "0.7 0.7\n",
            "Epoch 13 / 30 \n",
            " - time: 2.3931446075439453 - train_loss: 536.5171605467797 - acc_train: 0.9579333333333333 - acc_test: 0.9516\n",
            "0.7 0.7\n",
            "Epoch 14 / 30 \n",
            " - time: 2.397125720977783 - train_loss: 407.7050485372543 - acc_train: 0.9585333333333333 - acc_test: 0.9521\n",
            "0.7 0.7\n",
            "Epoch 15 / 30 \n",
            " - time: 2.3896071910858154 - train_loss: 310.3965988278389 - acc_train: 0.9589333333333333 - acc_test: 0.952\n",
            "0.7 0.7\n",
            "Epoch 16 / 30 \n",
            " - time: 2.3860092163085938 - train_loss: 236.8751287937164 - acc_train: 0.9594833333333334 - acc_test: 0.9522\n",
            "0.7 0.7\n",
            "Epoch 17 / 30 \n",
            " - time: 2.4142603874206543 - train_loss: 181.30813365578652 - acc_train: 0.9599166666666666 - acc_test: 0.9525\n",
            "0.7 0.7\n",
            "Epoch 18 / 30 \n",
            " - time: 2.3924319744110107 - train_loss: 139.30859864056112 - acc_train: 0.9605166666666667 - acc_test: 0.9527\n",
            "0.7 0.7\n",
            "Epoch 19 / 30 \n",
            " - time: 2.386859655380249 - train_loss: 107.5529917180538 - acc_train: 0.96105 - acc_test: 0.9529\n",
            "0.7 0.7\n",
            "Epoch 20 / 30 \n",
            " - time: 2.355213165283203 - train_loss: 83.52444989681243 - acc_train: 0.9613333333333334 - acc_test: 0.9529\n",
            "0.7 0.7\n",
            "Epoch 21 / 30 \n",
            " - time: 2.402527093887329 - train_loss: 65.3293818771839 - acc_train: 0.9618 - acc_test: 0.9534\n",
            "0.7 0.7\n",
            "Epoch 22 / 30 \n",
            " - time: 2.4014291763305664 - train_loss: 51.56193209290503 - acc_train: 0.96225 - acc_test: 0.9541\n",
            "0.7 0.7\n",
            "Epoch 23 / 30 \n",
            " - time: 2.3866326808929443 - train_loss: 41.104265171289434 - acc_train: 0.9627666666666667 - acc_test: 0.9543\n",
            "0.7 0.7\n",
            "Epoch 24 / 30 \n",
            " - time: 2.3689396381378174 - train_loss: 33.15557861328125 - acc_train: 0.9632 - acc_test: 0.9544\n",
            "0.7 0.7\n",
            "Epoch 25 / 30 \n",
            " - time: 2.4079203605651855 - train_loss: 27.119344955682752 - acc_train: 0.9635 - acc_test: 0.9543\n",
            "0.7 0.7\n",
            "Epoch 26 / 30 \n",
            " - time: 2.3744497299194336 - train_loss: 22.518975561857225 - acc_train: 0.9639 - acc_test: 0.9547\n",
            "0.7 0.7\n",
            "Epoch 27 / 30 \n",
            " - time: 2.40525221824646 - train_loss: 18.991659307479857 - acc_train: 0.96435 - acc_test: 0.9552\n",
            "0.7 0.7\n",
            "Epoch 28 / 30 \n",
            " - time: 2.380504846572876 - train_loss: 16.28930245935917 - acc_train: 0.9645333333333334 - acc_test: 0.9554\n",
            "0.7 0.7\n",
            "Epoch 29 / 30 \n",
            " - time: 2.3905692100524902 - train_loss: 14.210277238488198 - acc_train: 0.9649833333333333 - acc_test: 0.9552\n",
            "0.7 0.7\n",
            "Epoch 30 / 30 \n",
            " - time: 2.3809125423431396 - train_loss: 12.6034366607666 - acc_train: 0.9653333333333334 - acc_test: 0.9554\n",
            "0.7 0.7\n",
            "The total time spent is: 71.51729726791382 s\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1 / 30 \n",
            " - time: 2.586977005004883 - train_loss: 15758.065768072009 - acc_train: 0.5249333333333334 - acc_test: 0.5223\n",
            "0.77 0.77\n",
            "Epoch 2 / 30 \n",
            " - time: 2.428179979324341 - train_loss: 12897.356897618363 - acc_train: 0.8853833333333333 - acc_test: 0.8822\n",
            "0.8470000000000001 0.8470000000000001\n",
            "Epoch 3 / 30 \n",
            " - time: 2.3749802112579346 - train_loss: 10746.741025347048 - acc_train: 0.9364333333333333 - acc_test: 0.933\n",
            "0.9317000000000002 0.9317000000000002\n",
            "Epoch 4 / 30 \n",
            " - time: 2.4124033451080322 - train_loss: 9078.459287651427 - acc_train: 0.9475666666666667 - acc_test: 0.9429\n",
            "1.0248700000000004 1.0248700000000004\n",
            "Epoch 5 / 30 \n",
            " - time: 2.3568477630615234 - train_loss: 7750.080589102587 - acc_train: 0.9518 - acc_test: 0.9459\n",
            "1.1273570000000006 1.1273570000000006\n",
            "Epoch 6 / 30 \n",
            " - time: 2.4019663333892822 - train_loss: 6675.735223571495 - acc_train: 0.95435 - acc_test: 0.9483\n",
            "1.2400927000000008 1.2400927000000008\n",
            "Epoch 7 / 30 \n",
            " - time: 2.3829941749572754 - train_loss: 5798.746000680534 - acc_train: 0.9563 - acc_test: 0.9491\n",
            "1.364101970000001 1.364101970000001\n",
            "Epoch 8 / 30 \n",
            " - time: 2.4279706478118896 - train_loss: 5078.662023737107 - acc_train: 0.9576333333333333 - acc_test: 0.9495\n",
            "1.5005121670000012 1.5005121670000012\n",
            "Epoch 9 / 30 \n",
            " - time: 2.3751444816589355 - train_loss: 4485.219407997836 - acc_train: 0.9587833333333333 - acc_test: 0.9504\n",
            "1.6505633837000016 1.6505633837000016\n",
            "Epoch 10 / 30 \n",
            " - time: 2.375871419906616 - train_loss: 3994.8774856236655 - acc_train: 0.9596666666666667 - acc_test: 0.9505\n",
            "1.8156197220700019 1.8156197220700019\n",
            "Epoch 11 / 30 \n",
            " - time: 2.3367912769317627 - train_loss: 3588.9417907325915 - acc_train: 0.9605 - acc_test: 0.9515\n",
            "1.9971816942770022 1.9971816942770022\n",
            "Epoch 12 / 30 \n",
            " - time: 2.4057998657226562 - train_loss: 3252.5398419666726 - acc_train: 0.9614333333333334 - acc_test: 0.9524\n",
            "2.1968998637047026 2.1968998637047026\n",
            "Epoch 13 / 30 \n",
            " - time: 2.384960889816284 - train_loss: 2973.9447791924454 - acc_train: 0.9623 - acc_test: 0.9528\n",
            "2.416589850075173 2.416589850075173\n",
            "Epoch 14 / 30 \n",
            " - time: 2.399946689605713 - train_loss: 2743.5652562166206 - acc_train: 0.9630166666666666 - acc_test: 0.9531\n",
            "2.6582488350826905 2.6582488350826905\n",
            "Epoch 15 / 30 \n",
            " - time: 2.3598473072052 - train_loss: 2553.5207963995067 - acc_train: 0.9639 - acc_test: 0.9539\n",
            "2.92407371859096 2.92407371859096\n",
            "Epoch 16 / 30 \n",
            " - time: 2.3808629512786865 - train_loss: 2397.737558323306 - acc_train: 0.9645333333333334 - acc_test: 0.9544\n",
            "3.2164810904500563 3.2164810904500563\n",
            "Epoch 17 / 30 \n",
            " - time: 2.3653297424316406 - train_loss: 2270.9694307039995 - acc_train: 0.9651833333333333 - acc_test: 0.955\n",
            "3.5381291994950623 3.5381291994950623\n",
            "Epoch 18 / 30 \n",
            " - time: 2.4049925804138184 - train_loss: 2169.4965430093457 - acc_train: 0.9658166666666667 - acc_test: 0.9557\n",
            "3.8919421194445687 3.8919421194445687\n",
            "Epoch 19 / 30 \n",
            " - time: 2.3819074630737305 - train_loss: 2090.099142265208 - acc_train: 0.9663333333333334 - acc_test: 0.956\n",
            "4.281136331389026 4.281136331389026\n",
            "Epoch 20 / 30 \n",
            " - time: 2.390129327774048 - train_loss: 2030.4453431956763 - acc_train: 0.9670833333333333 - acc_test: 0.9564\n",
            "4.709249964527928 4.709249964527928\n",
            "Epoch 21 / 30 \n",
            " - time: 2.3896665573120117 - train_loss: 1988.3581256238808 - acc_train: 0.9673 - acc_test: 0.9566\n",
            "5.180174960980722 5.180174960980722\n",
            "Epoch 22 / 30 \n",
            " - time: 2.389249563217163 - train_loss: 1962.4678422986178 - acc_train: 0.96755 - acc_test: 0.9567\n",
            "5.698192457078794 5.698192457078794\n",
            "Epoch 23 / 30 \n",
            " - time: 2.4055495262145996 - train_loss: 1951.4246639636883 - acc_train: 0.9680166666666666 - acc_test: 0.9568\n",
            "6.268011702786674 6.268011702786674\n",
            "Epoch 24 / 30 \n",
            " - time: 2.3803951740264893 - train_loss: 1954.5805814299058 - acc_train: 0.96845 - acc_test: 0.9562\n",
            "6.894812873065342 6.894812873065342\n",
            "Epoch 25 / 30 \n",
            " - time: 2.3545844554901123 - train_loss: 1971.4295562418372 - acc_train: 0.9686833333333333 - acc_test: 0.9558\n",
            "7.5842941603718765 7.5842941603718765\n",
            "Epoch 26 / 30 \n",
            " - time: 2.390423059463501 - train_loss: 2001.5601757416869 - acc_train: 0.9688 - acc_test: 0.9559\n",
            "8.342723576409066 8.342723576409066\n",
            "Epoch 27 / 30 \n",
            " - time: 2.376166820526123 - train_loss: 2044.9564832723693 - acc_train: 0.9689 - acc_test: 0.9561\n",
            "9.176995934049973 9.176995934049973\n",
            "Epoch 28 / 30 \n",
            " - time: 2.3711395263671875 - train_loss: 2101.769956630767 - acc_train: 0.9687833333333333 - acc_test: 0.9561\n",
            "10.094695527454972 10.094695527454972\n",
            "Epoch 29 / 30 \n",
            " - time: 2.3613274097442627 - train_loss: 2172.3595302969593 - acc_train: 0.9684 - acc_test: 0.9556\n",
            "11.10416508020047 11.10416508020047\n",
            "Epoch 30 / 30 \n",
            " - time: 2.401643753051758 - train_loss: 2257.1218579706624 - acc_train: 0.9681333333333333 - acc_test: 0.9552\n",
            "12.214581588220518 12.214581588220518\n",
            "The total time spent is: 71.75404930114746 s\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1 / 30 \n",
            " - time: 2.5848305225372314 - train_loss: 15430.070371008851 - acc_train: 0.5115666666666666 - acc_test: 0.5098\n",
            "0.8049999999999999 0.8049999999999999\n",
            "Epoch 2 / 30 \n",
            " - time: 2.3493492603302 - train_loss: 13237.239477835372 - acc_train: 0.8879166666666667 - acc_test: 0.8911\n",
            "0.9257499999999999 0.9257499999999999\n",
            "Epoch 3 / 30 \n",
            " - time: 2.3610150814056396 - train_loss: 11610.087994476844 - acc_train: 0.9378833333333333 - acc_test: 0.9373\n",
            "1.0646124999999997 1.0646124999999997\n",
            "Epoch 4 / 30 \n",
            " - time: 2.38637375831604 - train_loss: 10371.542757248802 - acc_train: 0.9477666666666666 - acc_test: 0.9473\n",
            "1.2243043749999996 1.2243043749999996\n",
            "Epoch 5 / 30 \n",
            " - time: 2.414891242980957 - train_loss: 9405.754586575404 - acc_train: 0.9515666666666667 - acc_test: 0.9499\n",
            "1.4079500312499993 1.4079500312499993\n",
            "Epoch 6 / 30 \n",
            " - time: 2.368218421936035 - train_loss: 8645.92758833495 - acc_train: 0.9536666666666667 - acc_test: 0.9518\n",
            "1.6191425359374991 1.6191425359374991\n",
            "Epoch 7 / 30 \n",
            " - time: 2.3971316814422607 - train_loss: 8050.749866940514 - acc_train: 0.9551833333333334 - acc_test: 0.9529\n",
            "1.862013916328124 1.862013916328124\n",
            "Epoch 8 / 30 \n",
            " - time: 2.3886666297912598 - train_loss: 7592.647916016775 - acc_train: 0.9565166666666667 - acc_test: 0.9529\n",
            "2.141316003777342 2.141316003777342\n",
            "Epoch 9 / 30 \n",
            " - time: 2.390868663787842 - train_loss: 7252.608126500257 - acc_train: 0.95765 - acc_test: 0.9532\n",
            "2.462513404343943 2.462513404343943\n",
            "Epoch 10 / 30 \n",
            " - time: 2.3887500762939453 - train_loss: 7016.966449263598 - acc_train: 0.95885 - acc_test: 0.9543\n",
            "2.8318904149955344 2.8318904149955344\n",
            "Epoch 11 / 30 \n",
            " - time: 2.391063928604126 - train_loss: 6875.846052989376 - acc_train: 0.96 - acc_test: 0.955\n",
            "3.2566739772448643 3.2566739772448643\n",
            "Epoch 12 / 30 \n",
            " - time: 2.3515210151672363 - train_loss: 6822.598941716912 - acc_train: 0.96095 - acc_test: 0.9563\n",
            "3.7451750738315934 3.7451750738315934\n",
            "Epoch 13 / 30 \n",
            " - time: 2.382978916168213 - train_loss: 6853.440672687659 - acc_train: 0.9620166666666666 - acc_test: 0.9565\n",
            "4.306951334906332 4.306951334906332\n",
            "Epoch 14 / 30 \n",
            " - time: 2.409425735473633 - train_loss: 6966.823922657141 - acc_train: 0.96295 - acc_test: 0.9571\n",
            "4.952994035142281 4.952994035142281\n",
            "Epoch 15 / 30 \n",
            " - time: 2.3858437538146973 - train_loss: 7163.200068390554 - acc_train: 0.9639166666666666 - acc_test: 0.9574\n",
            "5.695943140413624 5.695943140413624\n",
            "Epoch 16 / 30 \n",
            " - time: 2.394146203994751 - train_loss: 7445.401484567832 - acc_train: 0.96475 - acc_test: 0.9579\n",
            "6.550334611475667 6.550334611475667\n",
            "Epoch 17 / 30 \n",
            " - time: 2.428985118865967 - train_loss: 7818.236395190002 - acc_train: 0.9656666666666667 - acc_test: 0.9581\n",
            "7.532884803197017 7.532884803197017\n",
            "Epoch 18 / 30 \n",
            " - time: 2.3654141426086426 - train_loss: 8289.088374971274 - acc_train: 0.9662833333333334 - acc_test: 0.9583\n",
            "8.66281752367657 8.66281752367657\n",
            "Epoch 19 / 30 \n",
            " - time: 2.3907532691955566 - train_loss: 8866.436305735515 - acc_train: 0.9667166666666667 - acc_test: 0.959\n",
            "9.962240152228054 9.962240152228054\n",
            "Epoch 20 / 30 \n",
            " - time: 2.3646960258483887 - train_loss: 9562.600853584056 - acc_train: 0.9672333333333333 - acc_test: 0.9595\n",
            "11.456576175062262 11.456576175062262\n",
            "Epoch 21 / 30 \n",
            " - time: 2.40159010887146 - train_loss: 10391.66644620648 - acc_train: 0.9678 - acc_test: 0.9591\n",
            "13.1750626013216 13.1750626013216\n",
            "Epoch 22 / 30 \n",
            " - time: 2.3952672481536865 - train_loss: 11371.612157599315 - acc_train: 0.9678833333333333 - acc_test: 0.9587\n",
            "15.151321991519838 15.151321991519838\n",
            "Epoch 23 / 30 \n",
            " - time: 2.420835494995117 - train_loss: 12521.107354517137 - acc_train: 0.9681166666666666 - acc_test: 0.9589\n",
            "17.424020290247814 17.424020290247814\n",
            "Epoch 24 / 30 \n",
            " - time: 2.373358964920044 - train_loss: 13863.32586616723 - acc_train: 0.9684666666666667 - acc_test: 0.9586\n",
            "20.037623333784985 20.037623333784985\n",
            "Epoch 25 / 30 \n",
            " - time: 2.380812644958496 - train_loss: 15426.775389711876 - acc_train: 0.9687166666666667 - acc_test: 0.9584\n",
            "23.04326683385273 23.04326683385273\n",
            "Epoch 26 / 30 \n",
            " - time: 2.3470981121063232 - train_loss: 17242.440658671152 - acc_train: 0.9687833333333333 - acc_test: 0.9586\n",
            "26.49975685893064 26.49975685893064\n",
            "Epoch 27 / 30 \n",
            " - time: 2.390381097793579 - train_loss: 19349.125614011482 - acc_train: 0.9688333333333333 - acc_test: 0.9584\n",
            "30.474720387770233 30.474720387770233\n",
            "Epoch 28 / 30 \n",
            " - time: 2.3564791679382324 - train_loss: 21788.639886040477 - acc_train: 0.9685833333333334 - acc_test: 0.9584\n",
            "35.045928445935765 35.045928445935765\n",
            "Epoch 29 / 30 \n",
            " - time: 2.39227294921875 - train_loss: 24613.574900791034 - acc_train: 0.9685166666666667 - acc_test: 0.9576\n",
            "40.302817712826126 40.302817712826126\n",
            "Epoch 30 / 30 \n",
            " - time: 2.3833844661712646 - train_loss: 27875.40740748905 - acc_train: 0.96835 - acc_test: 0.9568\n",
            "46.348240369750044 46.348240369750044\n",
            "The total time spent is: 71.73640370368958 s\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1 / 30 \n",
            " - time: 2.566105365753174 - train_loss: 15608.434165510906 - acc_train: 0.6651 - acc_test: 0.6632\n",
            "0.84 0.84\n",
            "Epoch 2 / 30 \n",
            " - time: 2.397709369659424 - train_loss: 14009.660970705001 - acc_train: 0.9148 - acc_test: 0.9134\n",
            "1.008 1.008\n",
            "Epoch 3 / 30 \n",
            " - time: 2.37686824798584 - train_loss: 12905.809878456563 - acc_train: 0.9431833333333334 - acc_test: 0.9384\n",
            "1.2096 1.2096\n",
            "Epoch 4 / 30 \n",
            " - time: 2.3812994956970215 - train_loss: 12162.317129197527 - acc_train: 0.9502666666666667 - acc_test: 0.9426\n",
            "1.45152 1.45152\n",
            "Epoch 5 / 30 \n",
            " - time: 2.3735780715942383 - train_loss: 11689.071038823566 - acc_train: 0.9532166666666667 - acc_test: 0.9468\n",
            "1.7418239999999998 1.7418239999999998\n",
            "Epoch 6 / 30 \n",
            " - time: 2.4098808765411377 - train_loss: 11439.595007795448 - acc_train: 0.9549666666666666 - acc_test: 0.9487\n",
            "2.0901887999999995 2.0901887999999995\n",
            "Epoch 7 / 30 \n",
            " - time: 2.3752095699310303 - train_loss: 11391.900978853671 - acc_train: 0.9566166666666667 - acc_test: 0.9504\n",
            "2.5082265599999993 2.5082265599999993\n",
            "Epoch 8 / 30 \n",
            " - time: 2.40346360206604 - train_loss: 11538.919573037836 - acc_train: 0.9581833333333334 - acc_test: 0.9515\n",
            "3.0098718719999993 3.0098718719999993\n",
            "Epoch 9 / 30 \n",
            " - time: 2.3741614818573 - train_loss: 11883.911771889278 - acc_train: 0.9594666666666667 - acc_test: 0.9524\n",
            "3.611846246399999 3.611846246399999\n",
            "Epoch 10 / 30 \n",
            " - time: 2.3978893756866455 - train_loss: 12438.496446114175 - acc_train: 0.96075 - acc_test: 0.9534\n",
            "4.334215495679999 4.334215495679999\n",
            "Epoch 11 / 30 \n",
            " - time: 2.371648073196411 - train_loss: 13221.853742328894 - acc_train: 0.9618166666666667 - acc_test: 0.9544\n",
            "5.201058594815998 5.201058594815998\n",
            "Epoch 12 / 30 \n",
            " - time: 2.380694627761841 - train_loss: 14261.405257064696 - acc_train: 0.9627666666666667 - acc_test: 0.955\n",
            "6.241270313779197 6.241270313779197\n",
            "Epoch 13 / 30 \n",
            " - time: 2.365654230117798 - train_loss: 15592.791745641152 - acc_train: 0.9640166666666666 - acc_test: 0.9551\n",
            "7.489524376535036 7.489524376535036\n",
            "Epoch 14 / 30 \n",
            " - time: 2.4186785221099854 - train_loss: 17262.58641996951 - acc_train: 0.9648166666666667 - acc_test: 0.9554\n",
            "8.987429251842043 8.987429251842043\n",
            "Epoch 15 / 30 \n",
            " - time: 2.3884525299072266 - train_loss: 19328.78176927672 - acc_train: 0.9653 - acc_test: 0.956\n",
            "10.784915102210451 10.784915102210451\n",
            "Epoch 16 / 30 \n",
            " - time: 2.4138450622558594 - train_loss: 21861.982541275673 - acc_train: 0.9657166666666667 - acc_test: 0.956\n",
            "12.94189812265254 12.94189812265254\n",
            "Epoch 17 / 30 \n",
            " - time: 2.388256788253784 - train_loss: 24951.541587292228 - acc_train: 0.96605 - acc_test: 0.9564\n",
            "15.530277747183048 15.530277747183048\n",
            "Epoch 18 / 30 \n",
            " - time: 2.405198335647583 - train_loss: 28704.25736792195 - acc_train: 0.96635 - acc_test: 0.9566\n",
            "18.636333296619657 18.636333296619657\n",
            "Epoch 19 / 30 \n",
            " - time: 2.363179922103882 - train_loss: 33249.472320746565 - acc_train: 0.9667333333333333 - acc_test: 0.9564\n",
            "22.363599955943588 22.363599955943588\n",
            "Epoch 20 / 30 \n",
            " - time: 2.396712064743042 - train_loss: 38744.01507687862 - acc_train: 0.9669 - acc_test: 0.9565\n",
            "26.836319947132306 26.836319947132306\n",
            "Epoch 21 / 30 \n",
            " - time: 2.3753621578216553 - train_loss: 45372.58496472699 - acc_train: 0.96725 - acc_test: 0.9563\n",
            "32.203583936558765 32.203583936558765\n",
            "Epoch 22 / 30 \n",
            " - time: 2.4210031032562256 - train_loss: 53363.975632214926 - acc_train: 0.96755 - acc_test: 0.9561\n",
            "38.644300723870515 38.644300723870515\n",
            "Epoch 23 / 30 \n",
            " - time: 2.348325729370117 - train_loss: 62992.89965973713 - acc_train: 0.96745 - acc_test: 0.9559\n",
            "46.37316086864462 46.37316086864462\n",
            "Epoch 24 / 30 \n",
            " - time: 2.379155158996582 - train_loss: 74585.55395277303 - acc_train: 0.96755 - acc_test: 0.9555\n",
            "55.64779304237354 55.64779304237354\n",
            "Epoch 25 / 30 \n",
            " - time: 2.3718817234039307 - train_loss: 88539.78527098575 - acc_train: 0.9675166666666667 - acc_test: 0.9556\n",
            "66.77735165084825 66.77735165084825\n",
            "Epoch 26 / 30 \n",
            " - time: 2.417480945587158 - train_loss: 105331.21980887985 - acc_train: 0.9675166666666667 - acc_test: 0.955\n",
            "80.1328219810179 80.1328219810179\n",
            "Epoch 27 / 30 \n",
            " - time: 2.377265214920044 - train_loss: 125531.97897112221 - acc_train: 0.9674333333333334 - acc_test: 0.9546\n",
            "96.15938637722148 96.15938637722148\n",
            "Epoch 28 / 30 \n",
            " - time: 2.399282217025757 - train_loss: 149830.25810285253 - acc_train: 0.9675333333333334 - acc_test: 0.9545\n",
            "115.39126365266577 115.39126365266577\n",
            "Epoch 29 / 30 \n",
            " - time: 2.371652364730835 - train_loss: 179067.17876363808 - acc_train: 0.9676 - acc_test: 0.9542\n",
            "138.4695163831989 138.4695163831989\n",
            "Epoch 30 / 30 \n",
            " - time: 2.399080514907837 - train_loss: 214206.0793347336 - acc_train: 0.9676333333333333 - acc_test: 0.9541\n",
            "166.1634196598387 166.1634196598387\n",
            "The total time spent is: 71.8089747428894 s\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1 / 30 \n",
            " - time: 2.566831588745117 - train_loss: 15138.456733953579 - acc_train: 0.5321 - acc_test: 0.5335\n",
            "0.875 0.875\n",
            "Epoch 2 / 30 \n",
            " - time: 2.3471972942352295 - train_loss: 14189.84781985241 - acc_train: 0.8923833333333333 - acc_test: 0.8933\n",
            "1.09375 1.09375\n",
            "Epoch 3 / 30 \n",
            " - time: 2.3575403690338135 - train_loss: 13714.482712632744 - acc_train: 0.9398666666666666 - acc_test: 0.9395\n",
            "1.3671875 1.3671875\n",
            "Epoch 4 / 30 \n",
            " - time: 2.3804728984832764 - train_loss: 13630.594279319484 - acc_train: 0.9491333333333334 - acc_test: 0.9465\n",
            "1.708984375 1.708984375\n",
            "Epoch 5 / 30 \n",
            " - time: 2.3819892406463623 - train_loss: 13885.429476720674 - acc_train: 0.9522166666666667 - acc_test: 0.9495\n",
            "2.13623046875 2.13623046875\n",
            "Epoch 6 / 30 \n",
            " - time: 2.3633203506469727 - train_loss: 14470.463099216886 - acc_train: 0.9545333333333333 - acc_test: 0.9512\n",
            "2.6702880859375 2.6702880859375\n",
            "Epoch 7 / 30 \n",
            " - time: 2.4198427200317383 - train_loss: 15407.959820259975 - acc_train: 0.9563333333333334 - acc_test: 0.9521\n",
            "3.337860107421875 3.337860107421875\n",
            "Epoch 8 / 30 \n",
            " - time: 2.3837618827819824 - train_loss: 16745.521779263585 - acc_train: 0.9577166666666667 - acc_test: 0.9532\n",
            "4.172325134277344 4.172325134277344\n",
            "Epoch 9 / 30 \n",
            " - time: 2.3687732219696045 - train_loss: 18552.92279628884 - acc_train: 0.9593333333333334 - acc_test: 0.9533\n",
            "5.21540641784668 5.21540641784668\n",
            "Epoch 10 / 30 \n",
            " - time: 2.393537998199463 - train_loss: 20925.99357270686 - acc_train: 0.9604166666666667 - acc_test: 0.9543\n",
            "6.51925802230835 6.51925802230835\n",
            "Epoch 11 / 30 \n",
            " - time: 2.3895792961120605 - train_loss: 23988.093725224593 - acc_train: 0.9618166666666667 - acc_test: 0.9549\n",
            "8.149072527885437 8.149072527885437\n",
            "Epoch 12 / 30 \n",
            " - time: 2.3693222999572754 - train_loss: 27899.469630179865 - acc_train: 0.9630833333333333 - acc_test: 0.9554\n",
            "10.186340659856796 10.186340659856796\n",
            "Epoch 13 / 30 \n",
            " - time: 2.4058358669281006 - train_loss: 32862.48889183482 - acc_train: 0.96415 - acc_test: 0.9559\n",
            "12.732925824820995 12.732925824820995\n",
            "Epoch 14 / 30 \n",
            " - time: 2.3730034828186035 - train_loss: 39134.70374914937 - acc_train: 0.96505 - acc_test: 0.9566\n",
            "15.916157281026244 15.916157281026244\n",
            "Epoch 15 / 30 \n",
            " - time: 2.3755855560302734 - train_loss: 47037.451583346665 - acc_train: 0.9655666666666667 - acc_test: 0.9573\n",
            "19.895196601282805 19.895196601282805\n",
            "Epoch 16 / 30 \n",
            " - time: 2.394956588745117 - train_loss: 56979.56404063093 - acc_train: 0.9663333333333334 - acc_test: 0.9581\n",
            "24.868995751603507 24.868995751603507\n",
            "Epoch 17 / 30 \n",
            " - time: 2.4219865798950195 - train_loss: 69471.70143710254 - acc_train: 0.9666833333333333 - acc_test: 0.9581\n",
            "31.086244689504383 31.086244689504383\n",
            "Epoch 18 / 30 \n",
            " - time: 2.388451099395752 - train_loss: 85152.02757424365 - acc_train: 0.96705 - acc_test: 0.9586\n",
            "38.85780586188048 38.85780586188048\n",
            "Epoch 19 / 30 \n",
            " - time: 2.4160642623901367 - train_loss: 104824.01841280561 - acc_train: 0.9673833333333334 - acc_test: 0.9586\n",
            "48.5722573273506 48.5722573273506\n",
            "Epoch 20 / 30 \n",
            " - time: 2.3990867137908936 - train_loss: 129505.07652304297 - acc_train: 0.96765 - acc_test: 0.9584\n",
            "60.71532165918825 60.71532165918825\n",
            "Epoch 21 / 30 \n",
            " - time: 2.386078357696533 - train_loss: 160465.8170272867 - acc_train: 0.9678666666666667 - acc_test: 0.9582\n",
            "75.89415207398531 75.89415207398531\n",
            "Epoch 22 / 30 \n",
            " - time: 2.3638007640838623 - train_loss: 199258.8912913701 - acc_train: 0.9681 - acc_test: 0.9574\n",
            "94.86769009248164 94.86769009248164\n",
            "Epoch 23 / 30 \n",
            " - time: 2.3620688915252686 - train_loss: 247872.3821065554 - acc_train: 0.96805 - acc_test: 0.9569\n",
            "118.58461261560205 118.58461261560205\n",
            "Epoch 24 / 30 \n",
            " - time: 2.3732452392578125 - train_loss: 308802.8163366169 - acc_train: 0.9679 - acc_test: 0.957\n",
            "148.23076576950257 148.23076576950257\n",
            "Epoch 25 / 30 \n",
            " - time: 2.409090518951416 - train_loss: 385141.68836531095 - acc_train: 0.9678 - acc_test: 0.9565\n",
            "185.2884572118782 185.2884572118782\n",
            "Epoch 26 / 30 \n",
            " - time: 2.3652584552764893 - train_loss: 480803.17153225554 - acc_train: 0.9676333333333333 - acc_test: 0.9565\n",
            "231.61057151484778 231.61057151484778\n",
            "Epoch 27 / 30 \n",
            " - time: 2.4112026691436768 - train_loss: 600624.4452674261 - acc_train: 0.9675833333333334 - acc_test: 0.9562\n",
            "289.51321439355974 289.51321439355974\n",
            "Epoch 28 / 30 \n",
            " - time: 2.4265809059143066 - train_loss: 750668.3520807512 - acc_train: 0.9674166666666667 - acc_test: 0.9559\n",
            "361.8915179919497 361.8915179919497\n",
            "Epoch 29 / 30 \n",
            " - time: 2.392430305480957 - train_loss: 938631.6030446029 - acc_train: 0.9672666666666667 - acc_test: 0.9553\n",
            "452.3643974899371 452.3643974899371\n",
            "Epoch 30 / 30 \n",
            " - time: 2.3717269897460938 - train_loss: 1173944.0638675333 - acc_train: 0.9670666666666666 - acc_test: 0.9548\n",
            "565.4554968624213 565.4554968624213\n",
            "The total time spent is: 71.75862240791321 s\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1 / 30 \n",
            " - time: 2.5506484508514404 - train_loss: 15788.794707167335 - acc_train: 0.4441333333333333 - acc_test: 0.4427\n",
            "0.9099999999999999 0.9099999999999999\n",
            "Epoch 2 / 30 \n",
            " - time: 2.382289171218872 - train_loss: 15432.154379149099 - acc_train: 0.8824 - acc_test: 0.8827\n",
            "1.1829999999999998 1.1829999999999998\n",
            "Epoch 3 / 30 \n",
            " - time: 2.3819894790649414 - train_loss: 15606.985301891515 - acc_train: 0.93785 - acc_test: 0.9325\n",
            "1.5378999999999998 1.5378999999999998\n",
            "Epoch 4 / 30 \n",
            " - time: 2.414691925048828 - train_loss: 16300.676129061965 - acc_train: 0.9490666666666666 - acc_test: 0.9433\n",
            "1.9992699999999999 1.9992699999999999\n",
            "Epoch 5 / 30 \n",
            " - time: 2.378753185272217 - train_loss: 17529.14812716233 - acc_train: 0.9535333333333333 - acc_test: 0.9466\n",
            "2.599051 2.599051\n",
            "Epoch 6 / 30 \n",
            " - time: 2.386911392211914 - train_loss: 19364.90703911661 - acc_train: 0.9561333333333333 - acc_test: 0.9484\n",
            "3.3787662999999997 3.3787662999999997\n",
            "Epoch 7 / 30 \n",
            " - time: 2.371458053588867 - train_loss: 21933.104343109015 - acc_train: 0.9577666666666667 - acc_test: 0.9502\n",
            "4.3923961899999995 4.3923961899999995\n",
            "Epoch 8 / 30 \n",
            " - time: 2.390761375427246 - train_loss: 25416.566532307428 - acc_train: 0.9591833333333334 - acc_test: 0.9503\n",
            "5.7101150469999995 5.7101150469999995\n",
            "Epoch 9 / 30 \n",
            " - time: 2.37727952003479 - train_loss: 30064.65540881044 - acc_train: 0.9605333333333334 - acc_test: 0.9513\n",
            "7.4231495611 7.4231495611\n",
            "Epoch 10 / 30 \n",
            " - time: 2.392298460006714 - train_loss: 36210.10816141866 - acc_train: 0.9618333333333333 - acc_test: 0.9524\n",
            "9.65009442943 9.65009442943\n",
            "Epoch 11 / 30 \n",
            " - time: 2.3432958126068115 - train_loss: 44291.456780068205 - acc_train: 0.9626166666666667 - acc_test: 0.953\n",
            "12.545122758259001 12.545122758259001\n",
            "Epoch 12 / 30 \n",
            " - time: 2.4059641361236572 - train_loss: 54882.53411116884 - acc_train: 0.9635333333333334 - acc_test: 0.9538\n",
            "16.308659585736702 16.308659585736702\n",
            "Epoch 13 / 30 \n",
            " - time: 2.364323377609253 - train_loss: 68732.80523862901 - acc_train: 0.9645666666666667 - acc_test: 0.9544\n",
            "21.201257461457715 21.201257461457715\n",
            "Epoch 14 / 30 \n",
            " - time: 2.3830621242523193 - train_loss: 86827.33979519522 - acc_train: 0.9652166666666666 - acc_test: 0.9541\n",
            "27.56163469989503 27.56163469989503\n",
            "Epoch 15 / 30 \n",
            " - time: 2.3934249877929688 - train_loss: 110439.93144009307 - acc_train: 0.9660166666666666 - acc_test: 0.9541\n",
            "35.83012510986354 35.83012510986354\n",
            "Epoch 16 / 30 \n",
            " - time: 2.3955888748168945 - train_loss: 141243.32604804472 - acc_train: 0.9660666666666666 - acc_test: 0.9541\n",
            "46.5791626428226 46.5791626428226\n",
            "Epoch 17 / 30 \n",
            " - time: 2.400890827178955 - train_loss: 181405.83071261138 - acc_train: 0.96605 - acc_test: 0.9541\n",
            "60.55291143566939 60.55291143566939\n",
            "Epoch 18 / 30 \n",
            " - time: 2.4318206310272217 - train_loss: 233758.19666170204 - acc_train: 0.9661333333333333 - acc_test: 0.9542\n",
            "78.7187848663702 78.7187848663702\n",
            "Epoch 19 / 30 \n",
            " - time: 2.4047164916992188 - train_loss: 301965.3508326903 - acc_train: 0.9660166666666666 - acc_test: 0.9536\n",
            "102.33442032628126 102.33442032628126\n",
            "Epoch 20 / 30 \n",
            " - time: 2.4214704036712646 - train_loss: 390864.82858592866 - acc_train: 0.9658666666666667 - acc_test: 0.953\n",
            "133.03474642416563 133.03474642416563\n",
            "Epoch 21 / 30 \n",
            " - time: 2.3869266510009766 - train_loss: 506612.7976316416 - acc_train: 0.96555 - acc_test: 0.9526\n",
            "172.94517035141533 172.94517035141533\n",
            "Epoch 22 / 30 \n",
            " - time: 2.4071056842803955 - train_loss: 657431.9290912801 - acc_train: 0.9652666666666667 - acc_test: 0.9522\n",
            "224.82872145683993 224.82872145683993\n",
            "Epoch 23 / 30 \n",
            " - time: 2.352250814437866 - train_loss: 853742.8724214807 - acc_train: 0.9648166666666667 - acc_test: 0.952\n",
            "292.2773378938919 292.2773378938919\n",
            "Epoch 24 / 30 \n",
            " - time: 2.392106771469116 - train_loss: 1109426.9141939867 - acc_train: 0.9645333333333334 - acc_test: 0.9513\n",
            "379.9605392620595 379.9605392620595\n",
            "Epoch 25 / 30 \n",
            " - time: 2.3659119606018066 - train_loss: 1442272.5702814932 - acc_train: 0.9644333333333334 - acc_test: 0.951\n",
            "493.9487010406774 493.9487010406774\n",
            "Epoch 26 / 30 \n",
            " - time: 2.431872606277466 - train_loss: 1875672.783203137 - acc_train: 0.96405 - acc_test: 0.9508\n",
            "642.1333113528807 642.1333113528807\n",
            "Epoch 27 / 30 \n",
            " - time: 2.419801950454712 - train_loss: 2439779.107152436 - acc_train: 0.9638666666666666 - acc_test: 0.9501\n",
            "834.773304758745 834.773304758745\n",
            "Epoch 28 / 30 \n",
            " - time: 2.380568027496338 - train_loss: 3173941.342613838 - acc_train: 0.9636666666666667 - acc_test: 0.9503\n",
            "1085.2052961863685 1085.2052961863685\n",
            "Epoch 29 / 30 \n",
            " - time: 2.354212522506714 - train_loss: 4129446.4186168513 - acc_train: 0.9634833333333334 - acc_test: 0.9498\n",
            "1410.7668850422792 1410.7668850422792\n",
            "Epoch 30 / 30 \n",
            " - time: 2.3981945514678955 - train_loss: 5372895.86156232 - acc_train: 0.9633166666666667 - acc_test: 0.9498\n",
            "1833.996950554963 1833.996950554963\n",
            "The total time spent is: 71.86059021949768 s\n"
          ]
        }
      ],
      "source": [
        "warm_start = True\n",
        "multipliers = [1,1.1,1.15,1.2,1.25,1.3]\n",
        "best_accuracy = []\n",
        "torch.cuda.empty_cache()\n",
        "for mult in multipliers:\n",
        "  # Iterations\n",
        "  print('Train on', N, 'samples, validate on', N_test, 'samples')\n",
        "  for k in range(niter):\n",
        "\n",
        "      start = time.time()\n",
        "\n",
        "      # update V3\n",
        "      V3 = (y_one_hot + gamma3*U3 + alpha1*V3)/(1+ gamma3 + alpha1)\n",
        "    \n",
        "      # update U3 \n",
        "      U3 = (gamma3*V3 + rho3*(torch.mm(W3,V2) + b3.repeat(1,N)))/(gamma3 + rho3)\n",
        "\n",
        "      # update W3 and b3\n",
        "      W3, b3 = update_wb_js(U3,V2,W3,b3,alpha1,rho3)\n",
        "    \n",
        "      # update V2\n",
        "      V2 = update_v_js(U2,U3,W3,b3,rho3,gamma2)\n",
        "    \n",
        "      # update U2\n",
        "      U2 = relu_prox(V2,(rho2*torch.addmm(b2.repeat(1,N), W2, V1) + alpha2*U2)/(rho2 + alpha2),(rho2 + alpha2)/gamma2,d2,N)\n",
        "    \n",
        "      # update W2 and b2\n",
        "      W2, b2 = update_wb_js(U2,V1,W2,b2,alpha3,rho2)\n",
        "    \n",
        "      # update V1\n",
        "      V1 = update_v_js(U1,U2,W2,b2,rho2,gamma1)\n",
        "    \n",
        "      # update U1\n",
        "      U1 = relu_prox(V1,(rho1*torch.addmm(b1.repeat(1,N), W1, x_train) + alpha7*U1)/(rho1 + alpha7),(rho1 + alpha7)/gamma1,d1,N)\n",
        "    \n",
        "      # update W1 and b1\n",
        "      W1, b1 = update_wb_js(U1,x_train,W1,b1,alpha8,rho1)\n",
        "\n",
        "      a1_train = nn.ReLU()(torch.addmm(b1.repeat(1, N), W1, x_train))\n",
        "      a2_train = nn.ReLU()(torch.addmm(b2.repeat(1, N), W2, a1_train))\n",
        "      pred = torch.argmax(torch.addmm(b3.repeat(1, N), W3, a2_train), dim=0)\n",
        "\n",
        "      a1_test = nn.ReLU()(torch.addmm(b1.repeat(1, N_test), W1, x_test))\n",
        "      a2_test = nn.ReLU()(torch.addmm(b2.repeat(1, N_test), W2, a1_test))\n",
        "      pred_test = torch.argmax(torch.addmm(b3.repeat(1, N_test), W3, a2_test), dim=0)\n",
        "    \n",
        "      loss1[k] = gamma3/2*torch.pow(torch.dist(V3,y_one_hot,2),2).cpu().numpy()\n",
        "      loss2[k] = loss1[k] + rho1/2*torch.pow(torch.dist(torch.addmm(b1.repeat(1,N), W1, x_train),U1,2),2).cpu().numpy() \\\n",
        "      +rho2/2*torch.pow(torch.dist(torch.addmm(b2.repeat(1,N), W2, V1),U2,2),2).cpu().numpy() \\\n",
        "      +rho3/2*torch.pow(torch.dist(torch.addmm(b3.repeat(1,N), W3, V2),U3,2),2).cpu().numpy()\n",
        "    \n",
        "      # compute training accuracy\n",
        "      correct_train = pred == y_train\n",
        "      accuracy_train[k] = np.mean(correct_train.cpu().numpy())\n",
        "    \n",
        "      # compute validation accuracy\n",
        "      correct_test = pred_test == y_test\n",
        "      accuracy_test[k] = np.mean(correct_test.cpu().numpy())\n",
        "    \n",
        "      # compute training time\n",
        "      stop = time.time()\n",
        "      duration = stop - start\n",
        "      time1[k] = duration\n",
        "    \n",
        "      # print results\n",
        "      print('Epoch', k + 1, '/', niter, '\\n', \n",
        "          '-', 'time:', time1[k], '-', 'train_loss:', loss2[k], \n",
        "          '-', 'acc_train:', accuracy_train[k], '-', 'acc_test:', accuracy_test[k])\n",
        "    \n",
        "      if (warm_start == True):\n",
        "        rho1 = rho2 = rho3 = rho4 = rho1*mult\n",
        "        gamma1 = gamma2 = gamma3 = gamma4 = gamma1*mult\n",
        "        print(gamma1, rho1)\n",
        "\n",
        "  best_accuracy.append(np.max(accuracy_test))\n",
        "  print('The total time spent is:', np.sum(time1), 's')\n",
        "  # Weight initialization (we replicate pytorch initialization)\n",
        "  std_1 = math.sqrt(1/d0)\n",
        "  W1 = torch.FloatTensor(d1, d0).uniform_(-std_1, std_1)\n",
        "  b1 = torch.FloatTensor(d1, 1).uniform_(-std_1, std_1)\n",
        "\n",
        "  # we move them to GPU\n",
        "  b1 = b1.to('cuda:0')\n",
        "  W1 = W1.to('cuda:0')\n",
        "\n",
        "\n",
        "  std_2 = math.sqrt(1/d1 )\n",
        "  W2 = torch.FloatTensor(d2, d1).uniform_(-std_2, std_2)\n",
        "  b2 = torch.FloatTensor(d2, 1).uniform_(-std_2, std_2)\n",
        "\n",
        "  # we move them to GPU\n",
        "  b2 = b2.to('cuda:0')\n",
        "  W2 = W2.to('cuda:0')\n",
        "\n",
        "  std_3 = math.sqrt(1/d2)\n",
        "  W3 = torch.FloatTensor(d3, d2).uniform_(-std_3, std_3)\n",
        "  b3 = torch.FloatTensor(d3, 1).uniform_(-std_3, std_3)\n",
        "\n",
        "  # we move them to GPU\n",
        "  b3 = b3.to('cuda:0')\n",
        "  W3 = W3.to('cuda:0')\n",
        "\n",
        "\n",
        "  U1 = torch.addmm(b1.repeat(1, N), W1, x_train) # equivalent to W1@x_train+b1.repeat(1,N)\n",
        "  V1 = nn.ReLU()(U1)\n",
        "  U2 = torch.addmm(b2.repeat(1, N), W2, V1)\n",
        "  V2 = nn.ReLU()(U2)\n",
        "  U3 = torch.addmm(b3.repeat(1, N), W3, V2)\n",
        "  V3 = U3\n",
        "\n",
        "  # constant initializations\n",
        "\n",
        "  # gamma is the parameter for regularization (gamma/2)*||V_N-U_N*V_{N}^{k-1}||^2\n",
        "  gamma = 0.7\n",
        "  gamma1 = gamma2 = gamma3 = gamma4 = gamma  \n",
        "\n",
        "  # rho is the parameter for the regularization (rho/2)*||U_N^k-W_N*V_{N-1}^{k-1}||^2\n",
        "  rho = gamma\n",
        "  rho1 = rho2 = rho3 = rho4 = rho\n",
        "\n",
        "\n",
        "  # alphas are the parameters for the regularization---> (alpha/2)*||W_N-W_N^{k-1}||^2 ,\n",
        "  #(alpha/2)*||V_N-V_N^{k-1}||^2\n",
        "  alpha = 5\n",
        "  alpha1 = alpha2 = alpha3 = alpha4 = alpha5 = alpha6 = alpha7 \\\n",
        "  = alpha8 = alpha9 = alpha10 = alpha \n",
        "\n",
        "  # initialization of the vectors of losses and accuracies\n",
        "  niter = 30\n",
        "  loss1 = np.empty(niter)\n",
        "  loss2 = np.empty(niter)\n",
        "  accuracy_train = np.empty(niter)\n",
        "  accuracy_test = np.empty(niter)\n",
        "  time1 = np.empty(niter)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMo_cj9O3dpm",
        "outputId": "0a9b9fa9-3bff-499a-9876-7106da12f5da"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9554, 0.9568, 0.9595, 0.9566, 0.9586, 0.9544]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrFu1jgT3kLz"
      },
      "source": [
        "# Training\n",
        "\n",
        "Note: Fix it so that it moves everything to device in the following function and that it does the label sample split here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "eFw-MbOat5p6",
        "outputId": "07c4018a-13be-4ccd-fee0-4e0f607b2f9d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "## We plot the train losses\n",
        "\n",
        "plot_train_losses(loss1.shape[0], loss1, 'Coordinate_descent')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "2gF9-R3-uB3c",
        "outputId": "e3937a86-2a20-44b4-de5b-c3af9e27355e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "## We plot the test accuracy\n",
        "\n",
        "plot_test_accuracy(accuracy_test.shape[0], accuracy_test, 'Coordinate_descent')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "TKYWNxdibHks"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Block_coordinate_descent.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}