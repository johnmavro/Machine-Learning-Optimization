{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BulR-Tbhu_a"
   },
   "source": [
    "# Setup instructions\n",
    "\n",
    "We provide a user-ready interface to reproduce our results concerning the Block Coordinate Descent algorithm. Therefore, we strongly recommend using Google Colab to perform training. \n",
    "To run this notebook on Google Colab, please import from the **Block_Coordinate_Descent** directory the following files:\n",
    "\n",
    "1. CD_utilities.py\n",
    "2. Torch_architectures.py\n",
    "3. Train_functions.py\n",
    "4. layers.py\n",
    "5. utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KDbRlCKv4U8"
   },
   "outputs": [],
   "source": [
    "#@title Import and Utilities\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "from utilities import *\n",
    "from Torch_architectures import *\n",
    "from Train_functions import *\n",
    "from CD_utilities import *\n",
    "from layers import *\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"Torchvision Version:\", torchvision.__version__)\n",
    "print(\"GPU is available?\", torch.cuda.is_available())\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVkAgTGRo1is"
   },
   "outputs": [],
   "source": [
    "#@title  Choose dataset name and optimizer \n",
    "\n",
    "ts = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0,), (1,))])\n",
    "\n",
    "# change the flag to choose the dataset to work with\n",
    "dataset_flag = \"MNIST\" #@param ['MNIST','FMNIST','CIFAR10']\n",
    "batch_size = 256 #@param {type:\"integer\"}\n",
    "if dataset_flag =='MNIST':\n",
    "  trainset = datasets.MNIST('../data', train=True, download=True, transform=ts)\n",
    "  testset = datasets.MNIST(root='../data', train=False, download=True, transform=ts)\n",
    "  dataset_train = torch.utils.data.DataLoader(testset,batch_size = 128, shuffle = True)\n",
    "  dataset_test = torch.utils.data.DataLoader(trainset,batch_size = batch_size,shuffle = True)\n",
    "elif dataset_flag =='FMNIST':\n",
    "  trainset = datasets.FashionMNIST('../data', train=True, download=True, transform=ts)\n",
    "  testset = datasets.FashionMNIST(root='../data', train=False, download=True, transform=ts)\n",
    "  dataset_train = torch.utils.data.DataLoader(testset,batch_size = 128, shuffle = True)\n",
    "  dataset_test = torch.utils.data.DataLoader(trainset,batch_size = batch_size,shuffle = True)\n",
    "elif dataset_flag=='CIFAR10':\n",
    "  trainset = datasets.CIFAR10('../data', train=True, download=True, transform=ts)\n",
    "  testset = datasets.CIFAR10(root='../data', train=False, download=True, transform=ts)\n",
    "  dataset_train = torch.utils.data.DataLoader(testset,batch_size = 128, shuffle = True)\n",
    "  dataset_test = torch.utils.data.DataLoader(trainset,batch_size = batch_size,shuffle = True)\n",
    "\n",
    "x_train, y_train, x_test, y_test,y_train_one_hot, y_test_one_hot, I1, I2 = load_dataset(trainset, testset,10)\n",
    "\n",
    "# we move to device to use GPU\n",
    "\n",
    "x_train = x_train.to(device = device)\n",
    "x_test = x_test.to(device = device)\n",
    "y_train = y_train.to(device = device)\n",
    "y_test = y_test.to(device = device)\n",
    "y_train_one_hot = y_train_one_hot.to(device)\n",
    "y_test_one_hot = y_test_one_hot.to(device)\n",
    "input_size = x_train.shape[0]\n",
    "hidden_size = int(1.5*input_size)\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lX28NWho-N4"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "#@title Model Selection\n",
    "model_name = 'Multilayer-Perceptron' #@param ['Multilayer-Perceptron']\n",
    "optimizer_name = \"Coordinate-Descent\" #@param ['SGD','Adam','Coordinate-Descent','Coordinate-Descent+SGD','Coordinate-Descent+Adam']\n",
    "momentum = 0.9 #@param {type:\"number\"}\n",
    "lr = 0.001 #@param {type:\"number\"}\n",
    "weight_decay = 0.00 #@param {type:\"number\"}\n",
    "beta_1 = 0.9 #@param {type:\"number\"}\n",
    "beta_2 = 0.999 #@param {type:\"number\"}\n",
    "gamma = 0.1 #@param {type:\"number\"}\n",
    "alpha = 4 #@param {type:\"number\"}\n",
    "epochs =  10#@param {type:\"integer\"}\n",
    "#the ratio of the epochs for coordinate descent for mixed classifiers\n",
    "ratio =  0.6#@param {type:\"number\"}\n",
    "GD_Update = False #@param {type:\"boolean\"}\n",
    "linear_extension = True #@param {type:\"boolean\"}\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "if(model_name =='Multilayer-Perceptron'):\n",
    "  model = MultiLayerPerceptron(input_size,hidden_size,output_size) \n",
    "\n",
    "\n",
    "if (optimizer_name == \"SGD\" or optimizer_name == \"Coordinate-Descent+SGD\"):\n",
    "  optimizer = torch.optim.SGD(params=model.parameters(), lr=lr,\n",
    "                              momentum=momentum, weight_decay=weight_decay)\n",
    "  assert lr > 0\n",
    "  assert 0 <= momentum <= 1\n",
    "\n",
    "elif (optimizer_name == \"Adam\" or optimizer_name == \"Coordinate-Descent+Adam\"):\n",
    "  optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, \n",
    "                               betas=(beta_1, beta_2), weight_decay=weight_decay)\n",
    "\n",
    "if(optimizer_name != 'Coordinate-Descent'):\n",
    "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_O44lBu4TC_"
   },
   "outputs": [],
   "source": [
    "#@title Train the network\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "accuracy_train = []\n",
    "accuracy_test = []\n",
    "epochs_times = []\n",
    "start = time.time()\n",
    "if(optimizer_name == 'Coordinate-Descent' or optimizer_name == 'Coordinate-Descent+SGD' or optimizer_name== 'Coordinate-Descent+Adam'):\n",
    "  print('training BCD')\n",
    "  if(optimizer_name != 'Coordinate-Descent'):\n",
    "    total_epochs = epochs\n",
    "    epochs = int(total_epochs * ratio)\n",
    "  train_losses, test_losses , accuracy_train, accuracy_test,epochs_times,Ws,bs = execute_training([[\"Perceptron\",hidden_size,1],[\"Perceptron\",hidden_size,1]], input_size, hidden_size, output_size, x_train, x_test, y_train, y_test, y_train_one_hot, y_test_one_hot,\n",
    "                                         GD_Update, linear_extension, I1 = hidden_size,I2=1, niter = epochs, gamma = gamma, alpha = alpha)\n",
    "  #Train using BCD\n",
    "  train_losses = list(train_losses)\n",
    "  test_losses = list(test_losses)\n",
    "  accuracy_train = list(accuracy_train)\n",
    "  accuracy_test = list(accuracy_test)\n",
    "  if(optimizer_name != 'Coordinate-Descent'):\n",
    "    epochs = total_epochs-epochs\n",
    "if(optimizer_name != 'Coordinate-Descent'):\n",
    "  model = model.to(device)\n",
    "  #train using sgd or adam\n",
    "  if(optimizer_name == 'Coordinate-Descent+SGD' or optimizer_name == 'Coordinate-Descent+Adam'):\n",
    "    i=0\n",
    "    for param in model.parameters():\n",
    "      if i%2 == 0:\n",
    "        param.data = Ws[int(i/2)]\n",
    "        #temp_W.pop()\n",
    "      else:\n",
    "        param.data = torch.flatten(bs[int(i/2)])\n",
    "        #temp_b.pop()\n",
    "      i+=1\n",
    "  train_loss, test_loss, acc_train, acc_test, times = train_model(model, dataset_train, dataset_test, optimizer, cross_entropy, epochs,scheduler,optimizer_name)\n",
    "  train_losses = list(train_losses) + train_loss\n",
    "  test_losses = list(test_losses) + test_loss\n",
    "  accuracy_train = list(accuracy_train) + acc_train\n",
    "  accuracy_test = list(accuracy_test) + acc_test\n",
    "  epochs_times = list(epochs_times) + times\n",
    "elapsed_time = time.time() - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "op31_mG1kcON"
   },
   "outputs": [],
   "source": [
    "#@title Save training statistics from the previous cell\n",
    "\n",
    "results = {'epochs': epochs_times, 'train_losses': train_losses, \n",
    "           'train_acc': accuracy_train, 'test_losses': test_losses, \n",
    "           'test_acc': accuracy_test, 'elapsed_time': elapsed_time}\n",
    "\n",
    "stats_dict = {}\n",
    "stats_dict.update({optimizer_name: results})\n",
    "\n",
    "save_stats = True\n",
    "if(GD_Update):\n",
    "  suffix = '-Entropy.pkl'\n",
    "elif(linear_extension):\n",
    "  suffix = '-linear_prox.pkl'\n",
    "else:\n",
    "  suffix = '.pkl'\n",
    "\n",
    "# save everything onto file\n",
    "if save_stats: \n",
    "    output_folder = os.path.join(os.getcwd(), dataset_flag)  # set the folder\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    fname = output_folder + '/stats_dict_' + model_name + '_' + optimizer_name + suffix\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pickle.dump(stats_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFdulP3nz6ES"
   },
   "outputs": [],
   "source": [
    "#@title Plot only latest training trends obtained in the previous cell\n",
    "\n",
    "# Test accuracy and training loss plots\n",
    "full_name = optimizer_name\n",
    "if(GD_Update):\n",
    "  suffix = '-Entropy'\n",
    "  full_name = full_name + suffix\n",
    "elif(linear_extension):\n",
    "  suffix = '-linear_prox'\n",
    "  full_name = full_name + suffix\n",
    "plot_stats(dataset_flag, [full_name], [optimizer_name], \"MLP\",epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FhdhvgvWeCx"
   },
   "source": [
    "# Produce complete training plots\n",
    "\n",
    "### NOTE: here, you can select the training results you collected above to produce plots similar to the ones shown in the report. Please, select the dataset and the optimizers of your liking. For the proper working of the plotting function called below, make sure that the dictionaries are in the corresponding folder. \n",
    "\n",
    "1. In the following cell select the dataset for which you want to plot the results.\n",
    "2. Select the optimizers for which you have produced the results:\n",
    "  - For `Block Coordinate Descent` select `plot_BCD`\n",
    "  - For `Block Coordinate Descent + GD update` select `plot_BCD_GD_update`\n",
    "  - For `Block Coordinate Descent prox linear` select `plot_BCD_linear_prox`\n",
    "  - For `SGD` select `plot_SGD`\n",
    "  - For `Adam` select `plot_Adam`\n",
    "  - For `Block Coordinate Descent + Adam` select `plot_BCD_Adam`\n",
    "  - For `Block Coordinate Descent + SGD` select `plot_BCD_SGD`\n",
    "\n",
    "Of course you can select as many optimizers as you want for the same plot.\n",
    "\n",
    "### NOTE: we remark that `Block Coordinate Descent + GD update` measures a different loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9Px3TKWWZu-"
   },
   "outputs": [],
   "source": [
    "#@title Produce complete plots of training trends\n",
    "\n",
    "dataset_name = \"MNIST\" #@param [\"MNIST\",\"FMNIST\",\"CIFAR10\"]\n",
    "n_epochs =  10#@param {type:\"integer\"}\n",
    "plot_BCD = True #@param {type:\"boolean\"}\n",
    "plot_BCD_GD_update = False #@param {type:\"boolean\"}\n",
    "plot_BCD_linear_prox = True #@param {type:\"boolean\"}\n",
    "plot_SGD = False #@param {type:\"boolean\"}\n",
    "plot_Adam = False #@param {type:\"boolean\"}\n",
    "plot_BCD_Adam = False #@param {type:\"boolean\"}\n",
    "plot_BCD_SGD = False #@param {type:\"boolean\"}\n",
    "\n",
    "full_names = []\n",
    "opt_names = []\n",
    "\n",
    "if(plot_BCD):\n",
    "  full_names = full_names + [\"Coordinate-Descent\"]\n",
    "  opt_names = opt_names + [\"Coordinate-Descent\"]\n",
    "if(plot_BCD_GD_update):\n",
    "  full_names = full_names + [\"Coordinate-Descent-Entropy\"]\n",
    "  opt_names = opt_names + [\"Coordinate-Descent\"]\n",
    "if(plot_BCD_linear_prox):\n",
    "  full_names = full_names + [\"Coordinate-Descent-linear_prox\"]\n",
    "  opt_names = opt_names + [\"Coordinate-Descent\"]\n",
    "if(plot_SGD):\n",
    "  full_names = full_names + [\"SGD\"]\n",
    "  opt_names = opt_names + [\"SGD\"]\n",
    "if(plot_Adam):\n",
    "  full_names = full_names + [\"Adam\"]\n",
    "  opt_names = opt_names + [\"Adam\"]\n",
    "if(plot_BCD_Adam):\n",
    "  full_names = full_names + [\"Coordinate-Descent+Adam\"]\n",
    "  opt_names = opt_names + [\"Coordinate-Descent+Adam\"]\n",
    "if(plot_BCD_SGD):\n",
    "  full_names = full_names + [\"Coordinate-Descent+SGD\"]\n",
    "  opt_names = opt_names + [\"Coordinate-Descent+SGD\"]\n",
    "    \n",
    "plot_stats(dataset_name, full_names, opt_names, \"MLP\",n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rfX1FgBq7CN"
   },
   "source": [
    "# Hyper-parameters used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywSQ1UakrCL0"
   },
   "source": [
    "In order to reproduce our results (i.e. the training trends shown in the report), the following set of parameters should be used.\\ If not specified otherwise, other parameters (e.g. for numerical stability) are set to their default values.\n",
    "\n",
    "$\\text{Block Coordinate Descent}$:\\\n",
    "$\\gamma = 0.1$, $\\alpha = 4$\n",
    "\n",
    "$\\text{Stochastic Gradient Descent (with scheduler)}:$\\\n",
    "$\\gamma = 0.01$, $\\mu = 0.9$\\\n",
    "$\\text{Scheduler:}$\\\n",
    "$\\text{Step size = 15,}\\gamma = 0.2$\n",
    "\n",
    "Adam:\\\n",
    "$\\gamma = 0.001$, $\\mu = 0.9$, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$\n",
    "\n",
    "For hybrid variants, we took $ratio = 0.6$\n",
    "\n",
    "For the mixed optimizer of SGD decrease the learning rate to $\\gamma = 0.001$\n",
    "\n",
    "$\\text{Block Coordinate Descent + GD update for V_N}:$\\\n",
    "$GD\\_update = True$\n",
    "\n",
    "$\\text{Prox Linear update for V_N}:$\\\n",
    "$prox\\_linear = True$"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BCD.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
