{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## If using colab run the following cell, otherwise do not run it\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Machine-Learning-Optimization_working"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puJO6BZlNmuf",
        "outputId": "b5ee5d14-b2bf-43a1-f30a-7eef503ccd93"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Machine-Learning-Optimization_working\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC7BWlWnoFBa",
        "outputId": "4931b158-cf1a-45fc-a2f5-4f4924307073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: barbar in /usr/local/lib/python3.7/dist-packages (0.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install barbar\n",
        "\n",
        "#@title Import and utilities \n",
        "\n",
        "from Frank_Wolfe.utils.utils import *\n",
        "from Frank_Wolfe.DFW import *\n",
        "from Frank_Wolfe.architectures import *\n",
        "from Frank_Wolfe.MultiClassHingeLoss import *\n",
        "from Frank_Wolfe.MultiClassHingeLoss import set_smoothing_enabled\n",
        "from barbar import Bar\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import sys\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "device = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gMj3YVkWoM9x"
      },
      "outputs": [],
      "source": [
        "# The following are flag useful for saving or loading figures/stats\n",
        "\n",
        "save_stats = True\n",
        "save_figs = True\n",
        "load = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf6SJcqjoUSA",
        "outputId": "7fa95e16-f026-45fc-d35a-c7b32925fd42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "#@title Choose dataset name and architecture of the network \n",
        "\n",
        "# Select the dataset and the architecture\n",
        "\n",
        "dataset_name = 'CIFAR100' #@param ['CIFAR10', 'CIFAR100']\n",
        "model_type = 'DenseNet' #@param ['DenseNet', 'WideResNet', 'GoogLeNet']\n",
        "\n",
        "# load the model\n",
        "if model_type == 'GoogLeNet':\n",
        "    model = GoogleNet(num_class=10 if dataset_name == 'CIFAR10' else 100)\n",
        "elif model_type == 'DenseNet':\n",
        "    model = torchvision.models.densenet121(pretrained=False)\n",
        "elif model_type == 'WideResNet':\n",
        "    model =  WideResNet(num_classes=10 if dataset_name == 'CIFAR10' else 100)\n",
        "else:\n",
        "    raise ValueError(\"Please, select an available architecture\")\n",
        "\n",
        "\n",
        "datasetDict = setDatasetAttributes(dataset_name) # dictionary useful to normalize the images\n",
        "trainTransformDict, testTransformDict = setTrainAndTest(dataset_name) # dict useful for the transform operation\n",
        "root = f\"{dataset_name}-dataset\"\n",
        "\n",
        "# prepare train and test datasets \n",
        "trainData = datasetDict['datasetDict'](root=root, train=True, download=True,\n",
        "                                            transform=trainTransformDict[dataset_name])\n",
        "testData = datasetDict['datasetDict'](root=root, train=False,\n",
        "                                        transform=testTransformDict[dataset_name])\n",
        "# move the model to GPU\n",
        "model = model.to(device=\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C6IgJdqGoWkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0611f98a-0a1a-4c1e-893c-fc9658f4f4f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am in\n"
          ]
        }
      ],
      "source": [
        "#@title Choose optimizer and parameters \n",
        "\n",
        "# Choice of the optimizer and the parameters, the parameters used in our experiments can be found\n",
        "# both in the report and at the end of the notebook\n",
        "\n",
        "optimizer_name = \"DFW multistep\" #@param  ['DFW', 'Adam', 'SGD with scheduler', 'DFW multistep']\n",
        "momentum = 0.9 #@param {type:\"number\"}\n",
        "lr = 0.001 #@param {type:\"number\"}\n",
        "eta =   0.1 #@param {type:\"number\"}\n",
        "beta_1 = 0.9 #@param {type:\"number\"}\n",
        "beta_2 = 0.999 #@param {type:\"number\"}\n",
        "weight_decay = 0.00 #@param {type:\"number\"}\n",
        "asymptotic_prox_steps_num = 2 #@param {type: \"number\"}\n",
        "\n",
        "if optimizer_name != \"DFW multistep\":\n",
        "    asymptotic_prox_steps_num = 1\n",
        "\n",
        "# define the optimizer\n",
        "\n",
        "if optimizer_name == \"DFW\" or optimizer_name == 'DFW multistep':\n",
        "    optimizer = DFW(params=model.parameters(), eta=eta, momentum=momentum,\n",
        "                  weight_decay = weight_decay, prox_steps=asymptotic_prox_steps_num)\n",
        "    \n",
        "    assert asymptotic_prox_steps_num >0\n",
        "    assert eta > 0\n",
        "    assert 0 <= momentum <= 1\n",
        "elif optimizer_name == \"SGD with scheduler\":\n",
        "    optimizer = torch.optim.SGD(params=model.parameters(), lr=lr,\n",
        "                              momentum=momentum, weight_decay=weight_decay)\n",
        "    scheduler = StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "    assert 0 <= momentum <= 1\n",
        "elif optimizer_name == \"Adam\":\n",
        "    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, \n",
        "                               betas=(beta_1, beta_2), weight_decay=weight_decay)\n",
        "    \n",
        "if optimizer_name == \"DFW\" or optimizer_name == \"DFW multistep\":\n",
        "    loss_criterion = MultiClassHingeLoss().to(device=\"cuda:0\")\n",
        "\n",
        "else: \n",
        "    loss_criterion = nn.CrossEntropyLoss().to(device=\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Cq-yV7BYplnM",
        "outputId": "4c21b8bc-9d98-48c3-a9aa-5d39ac59571b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in\n",
            "\n",
            "\n",
            "Epoch 0/50\n",
            "Evaluation of train data:\n",
            "50000/50000: [===============================>] - ETA 0.2s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.1s\n",
            "\n",
            " Finished epoch 0/50: Train Loss 1.9938795345306397 | Test Loss 1.8983999757766723 | Train Acc 0.0 | Test Acc 0.0\n",
            "Time elapsed for the current epoch 16.54431962966919\n",
            "\n",
            "Epoch 1/50\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.4s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.1s\n",
            "\n",
            " Finished epoch 1/50: Train Loss 0.7150843365287781 | Test Loss 1.4758526119232178 | Train Acc 0.01106 | Test Acc 0.01\n",
            "Time elapsed for the current epoch 115.50006246566772\n",
            "\n",
            "Epoch 2/50\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.4s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.1s\n",
            "\n",
            " Finished epoch 2/50: Train Loss 0.6907602533721924 | Test Loss 1.2718890270233154 | Train Acc 0.01048 | Test Acc 0.01\n",
            "Time elapsed for the current epoch 116.50871229171753\n",
            "\n",
            "Epoch 3/50\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.4s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.1s\n",
            "\n",
            " Finished epoch 3/50: Train Loss 0.7046723214912415 | Test Loss 1.1122491161346435 | Train Acc 0.00968 | Test Acc 0.01\n",
            "Time elapsed for the current epoch 116.02266931533813\n",
            "\n",
            "Epoch 4/50\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.4s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.1s\n",
            "\n",
            " Finished epoch 4/50: Train Loss 0.7106945220756531 | Test Loss 1.2922365169525147 | Train Acc 0.01042 | Test Acc 0.01\n",
            "Time elapsed for the current epoch 115.9386842250824\n",
            "\n",
            "Epoch 5/50\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.4s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.1s\n",
            "\n",
            " Finished epoch 5/50: Train Loss 0.7089122166633606 | Test Loss 1.0959537616729735 | Train Acc 0.01082 | Test Acc 0.01\n",
            "Time elapsed for the current epoch 116.23536491394043\n",
            "\n",
            "Epoch 6/50\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.4s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.1s\n",
            "\n",
            " Finished epoch 6/50: Train Loss 0.7151945082473755 | Test Loss 1.3004033163070678 | Train Acc 0.01046 | Test Acc 0.01\n",
            "Time elapsed for the current epoch 116.59774279594421\n",
            "\n",
            "Epoch 7/50\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.4s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.1s\n",
            "\n",
            " Finished epoch 7/50: Train Loss 0.6898914098930359 | Test Loss 1.1705061260223388 | Train Acc 0.00916 | Test Acc 0.01\n",
            "Time elapsed for the current epoch 116.18181729316711\n",
            "\n",
            "Epoch 8/50\n",
            "Training:\n",
            "50000/50000: [===============================>] - ETA 0.4s\n",
            "Evaluation of test data:\n",
            "10000/10000: [===============================>] - ETA 0.1s\n",
            "\n",
            " Finished epoch 8/50: Train Loss 0.7056865490722656 | Test Loss 1.3153724517822265 | Train Acc 0.01016 | Test Acc 0.01\n",
            "Time elapsed for the current epoch 115.59533667564392\n",
            "\n",
            "Epoch 9/50\n",
            "Training:\n",
            " 5888/50000: [===>............................] - ETA 135.3s"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6f97aa4f7b9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moptimizer_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"DFW\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0moptimizer_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DFW multistep'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title Train the network  \n",
        "\n",
        "# we will append our results on these lists\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "epochs_times = []\n",
        "\n",
        "# parameters for the training phase\n",
        "nepochs = 50 #@param {type:\"integer\"}\n",
        "batch_size = 128  #@param {type:\"integer\"}\n",
        "verbose = 0 #@param [0, 1]\n",
        "smooth = False\n",
        "\n",
        "if(dataset_name == 'CIFAR100' and (optimizer_name == 'DFW' or optimizer_name=='DFW multistep')):\n",
        "    smooth = True # for the smoothing of the loss in case we use CIFAR100\n",
        "\n",
        "# Loaders\n",
        "trainLoader = torch.utils.data.DataLoader(trainData, batch_size=batch_size, shuffle=True,\n",
        "                                      pin_memory=torch.cuda.is_available(), num_workers=2)\n",
        "testLoader = torch.utils.data.DataLoader(testData, batch_size=batch_size, shuffle=False,\n",
        "                                      pin_memory=torch.cuda.is_available(), num_workers=2)\n",
        "\n",
        "# initialize necessary metrics objects\n",
        "train_loss, train_accuracy = AverageMeter(), AverageMeter()\n",
        "test_loss, test_accuracy = AverageMeter(), AverageMeter()\n",
        "\n",
        "# function to reset metrics\n",
        "def reset_metrics():\n",
        "    train_loss.reset()\n",
        "    train_accuracy.reset()\n",
        "    test_loss.reset()\n",
        "    test_accuracy.reset()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(data=\"train\"):\n",
        "    if data == \"train\":\n",
        "        loader = trainLoader\n",
        "        mean_loss, mean_accuracy = train_loss, train_accuracy\n",
        "    elif data == \"test\":\n",
        "        loader = testLoader\n",
        "        mean_loss, mean_accuracy = test_loss, test_accuracy\n",
        "    \n",
        "    sys.stdout.write(f\"Evaluation of {data} data:\\n\")\n",
        "    \n",
        "    # iteration over the dataset\n",
        "    for x_input, y_target in Bar(loader):\n",
        "        x_input, y_target = x_input.to(device=\"cuda:0\"), y_target.to(device=\"cuda:0\") # we move to GPU\n",
        "        output = model.eval()(x_input)\n",
        "        loss = loss_criterion(output, y_target)\n",
        "        \n",
        "        # update metrics\n",
        "        mean_loss(loss.item(), len(y_target)) \n",
        "        mean_accuracy(Utilities.categorical_accuracy(y_true=y_target, output=output), len(y_target))\n",
        "\n",
        "    \n",
        "# Training\n",
        "for epoch in range(nepochs + 1):\n",
        "    \n",
        "    start = time.time() # start to time\n",
        "    reset_metrics() # reset the metrics from the previous epoch\n",
        "    sys.stdout.write(f\"\\n\\nEpoch {epoch}/{nepochs}\\n\")\n",
        "    \n",
        "    if epoch == 0:\n",
        "        # First pass through the network to evaluate the model once to get the metrics\n",
        "        evaluate_model(data='train')\n",
        "    else:\n",
        "        if epoch > int(0.2 * nepochs) and optimizer_name == \"DFW multistep\" and asymptotic_prox_steps_num >1:\n",
        "            \n",
        "            # if we already finished the first 20% of the epochs we continue with single steps\n",
        "            optimizer.prox_steps = 1\n",
        "            \n",
        "        sys.stdout.write(f\"Training:\\n\")\n",
        "        for x_input, y_target in Bar(trainLoader):\n",
        "            x_input, y_target = x_input.to(device=\"cuda:0\"), y_target.to(device=\"cuda:0\")\n",
        "            optimizer.zero_grad()  # Zero the gradient buffers\n",
        "            output = model.train()(x_input) # compute the output\n",
        "            if smooth == True:\n",
        "                with set_smoothing_enabled(True):\n",
        "                    loss = loss_criterion(output, y_target)\n",
        "            else:\n",
        "                loss = loss_criterion(output, y_target) # compute the loss\n",
        "\n",
        "            loss.backward()  # Backpropagation\n",
        "            if optimizer_name == \"DFW\" or optimizer_name == 'DFW multistep':\n",
        "                optimizer.step(lambda: float(loss), model, x_input, y_target, smooth)\n",
        "            else:\n",
        "                optimizer.step() \n",
        "            train_loss(loss.item(), len(y_target))\n",
        "            train_accuracy(Utilities.categorical_accuracy(y_true=y_target, output=output), len(y_target))\n",
        "\n",
        "    if optimizer_name == \"SGD with scheduler\":\n",
        "        scheduler.step()\n",
        "    evaluate_model(data='test')\n",
        "    sys.stdout.write(f\"\\n Finished epoch {epoch}/{nepochs}: Train Loss {train_loss.result()} | Test Loss {test_loss.result()} | Train Acc {train_accuracy.result()} | Test Acc {test_accuracy.result()}\\n\")\n",
        "    \n",
        "    train_losses.append(train_loss.result())\n",
        "    train_accuracies.append(train_accuracy.result())\n",
        "    test_losses.append(test_loss.result())\n",
        "    test_accuracies.append(test_accuracy.result())\n",
        "\n",
        "\n",
        "    elapsed_time = time.time()-start\n",
        "    sys.stdout.write(f\"Time elapsed for the current epoch {elapsed_time}\")\n",
        "    epochs_times.append(elapsed_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9w2DgFHAExBE"
      },
      "outputs": [],
      "source": [
        "#@title Save training results and plot\n",
        "\n",
        "if load:\n",
        "    output_folder = os.path.join(os.getcwd(), 'results')\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    fname = output_folder + '/stats_dict_' + model_type + '.pkl'\n",
        "    with open(fname, 'rb') as handle:\n",
        "        stats_dict = pickle.load(handle)\n",
        "\n",
        "# define dictionary of the results\n",
        "results = {'epochs': nepochs, 'train_losses': train_losses, \n",
        "           'train_acc': train_accuracies, 'test_losses': test_losses, \n",
        "           'test_acc': test_accuracies, 'elapsed_time': epochs_times}\n",
        "stats_dict = {}\n",
        "stats_dict.update({optimizer_name: results})\n",
        "\n",
        "# save everything onto file\n",
        "if save_stats: \n",
        "    output_folder = os.path.join(os.getcwd(), 'results')  # set the folder\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    fname = output_folder + '/stats_dict_' + model_type + '.pkl'\n",
        "    with open(fname, 'wb') as handle:\n",
        "        pickle.dump(stats_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldLaA2by6VYV"
      },
      "source": [
        "# Parameters used in the report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7F6Ai3G6bFd"
      },
      "source": [
        "In order to reproduce our results, the following set of parameters should be used.\\\n",
        "If not specified, the remaining parameters (e.g. $\\epsilon$ for Adam and Adagrad) are set to their default values.\n",
        "\n",
        "Deep Frank Wolfe:\\\n",
        "$η = 0.1$, $μ = 0.9$, $w_d = 0$\n",
        "\n",
        "Stochastic Gradient Descent with scheduler:\\\n",
        "$\\gamma = 0.01$, $\\mu = 0.9$, $w_d = 0$\n",
        "\n",
        "Adam:\\\n",
        "$\\gamma = 0.001$, $\\mu = 0.9$, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$\n",
        "\n",
        "Deep Frank Wolfe multistep:\\\n",
        "$η = 0.1$, $μ = 0.9$, $w_d = 0$,  n_steps $= 2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEEgyDFNNeqL"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DFW.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}